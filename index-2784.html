<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="This is a snapshot site for StackOverflow">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>StackOverflow Snapshot (old posts, page 2784) | StackOverflow Snapshot</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/index-2784.html">
<link rel="prev" href="index-2785.html" type="text/html">
<link rel="next" href="index-2783.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="archive.html">Archive</a>
                </li>
<li>
<a href="categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            

    


    
<div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/refused-to-display-in-a-frame-because-it-set-x-frame-options-to-sameorigin/" class="u-url">Refused to display in a frame because it set 'X-Frame-Options' to 'SAMEORIGIN'</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/refused-to-display-in-a-frame-because-it-set-x-frame-options-to-sameorigin/" rel="bookmark">
            <time class="published dt-published" datetime="2023-03-05T17:42:31+08:00" itemprop="datePublished" title="2023-03-05 17:42">2023-03-05 17:42</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>I am developing a website that is supposed to be responsive so that people can
access it from their phones. The site has got some secured parts that can be
logged into using Google, Facebook, ...etc (OAuth).</p>
<p>The server backend is developed using ASP.Net Web API 2 and the front end is
mainly AngularJS with some Razor.</p>
<p>For the authentication part, everything is working fine in all browsers
including Android but the Google authentication is not working on iPhone and
it gives me this error message</p>
<div class="code"><pre class="code literal-block">Refused to display 'https://accounts.google.com/o/openid2/auth
?openid.ns=http://specs.openid.ne…tp://axschema.org/namePerson
/last&amp;openid.ax.required=email,name,first,last'
in a frame because it set 'X-Frame-Options' to 'SAMEORIGIN'.
</pre></div>

<p>Now as far I am concerned I do not use any iframe in my HTML files.</p>
<p>I googled around, but no answer got me to fix the issue.</p>
<p><br><br></p>
<h2>Answer</h2>
<p>O.K. after spending more time on this with the help of this SO post</p>
<p>Overcoming "Display forbidden by X-Frame-Options"</p>
<p>I managed to solve the issue by adding <code>&amp;output=embed</code> to the end of the url
before posting to the google URL:</p>
<div class="code"><pre class="code literal-block"><span class="k">var</span><span class="w"> </span><span class="n">url</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="o">.</span><span class="n">url</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="s2">"&amp;output=embed"</span><span class="p">;</span>
<span class="n">window</span><span class="o">.</span><span class="n">location</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">url</span><span class="p">);</span>
</pre></div>

<p><br></p>
<h3>Suggest</h3>
<p>O.K. after spending more time on this with the help of this SO post</p>
<p>Overcoming "Display forbidden by X-Frame-Options"</p>
<p>I managed to solve the issue by adding <code>&amp;output=embed</code> to the end of the url
before posting to the google URL:</p>
<div class="code"><pre class="code literal-block"><span class="k">var</span><span class="w"> </span><span class="n">url</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="o">.</span><span class="n">url</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="s2">"&amp;output=embed"</span><span class="p">;</span>
<span class="n">window</span><span class="o">.</span><span class="n">location</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">url</span><span class="p">);</span>
</pre></div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/why-do-you-need-dot-slash-before-executable-or-script-name-to-run-it-in-bash/" class="u-url">Why do you need ./ (dot-slash) before executable or script name to run it in bash?</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/why-do-you-need-dot-slash-before-executable-or-script-name-to-run-it-in-bash/" rel="bookmark">
            <time class="published dt-published" datetime="2023-03-05T17:40:58+08:00" itemprop="datePublished" title="2023-03-05 17:40">2023-03-05 17:40</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>When running scripts in bash, I have to write <code>./</code> in the beginning:</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>./manage.py<span class="w"> </span>syncdb
</pre></div>

<p>If I don't, I get an error message:</p>
<div class="code"><pre class="code literal-block">$<span class="w"> </span>manage.py<span class="w"> </span>syncdb
-bash:<span class="w"> </span>manage.py:<span class="w"> </span><span class="nb">command</span><span class="w"> </span>not<span class="w"> </span>found
</pre></div>

<p>What is the reason for this? I thought <code>.</code> is an alias for current folder, and
therefore these two calls should be equivalent.</p>
<p>I also don't understand why I don't need <code>./</code> when running applications, such
as:</p>
<div class="code"><pre class="code literal-block">user:/home/user$ cd /usr/bin
user:/usr/bin$ git
</pre></div>

<p>(which runs without <code>./</code>)</p>
<p><br><br></p>
<h2>Answer</h2>
<p>Because on Unix, usually, the current directory is not in <code>$PATH</code>.</p>
<p>When you type a command the shell looks up a list of directories, as specified
by the <code>PATH</code> variable. The current directory is not in that list.</p>
<p>The reason for not having the current directory on that list is security.</p>
<p>Let's say you're root and go into another user's directory and type <code>sl</code>
instead of <code>ls</code>. If the current directory is in <code>PATH</code>, the shell will try to
execute the <code>sl</code> program in that directory (since there is no other <code>sl</code>
program). That <code>sl</code> program might be malicious.</p>
<p>It works with <code>./</code> because POSIX specifies that a command name that contain a
<code>/</code> will be used as a filename directly, suppressing a search in <code>$PATH</code>. You
could have used full path for the exact same effect, but <code>./</code> is shorter and
easier to write.</p>
<p><strong>EDIT</strong></p>
<p>That <code>sl</code> part was just an example. The directories in <code>PATH</code> are searched
sequentially and when a match is made that program is executed. So, depending
on how <code>PATH</code> looks, typing a normal command may or may not be enough to run
the program in the current directory.</p>
<p><br></p>
<h3>Suggest</h3>
<p>When bash interprets the command line, it looks for commands in locations
described in the environment variable <code>$PATH</code>. To see it type:</p>
<div class="code"><pre class="code literal-block">echo $PATH
</pre></div>

<p>You will have some paths separated by colons. As you will see the current path
<code>.</code> is usually not in <code>$PATH</code>. So Bash cannot find your command if it is in
the current directory. You can change it by having:</p>
<div class="code"><pre class="code literal-block">PATH=$PATH:.
</pre></div>

<p>This line adds the current directory in <code>$PATH</code> so you can do:</p>
<div class="code"><pre class="code literal-block">manage.py syncdb
</pre></div>

<p>It is <strong>not</strong> recommended as it has security issue, plus you can have weird
behaviours, as <code>.</code> varies upon the directory you are in :)</p>
<p>Avoid:</p>
<div class="code"><pre class="code literal-block">PATH=.:$PATH
</pre></div>

<p>As you can “mask” some standard command and open the door to security breach
:)</p>
<p>Just my two cents.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/deoptimizing-a-program-for-the-pipeline-in-intel-sandybridge-family-cpus/" class="u-url">Deoptimizing a program for the pipeline in Intel Sandybridge-family CPUs</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/deoptimizing-a-program-for-the-pipeline-in-intel-sandybridge-family-cpus/" rel="bookmark">
            <time class="published dt-published" datetime="2023-03-05T17:39:28+08:00" itemprop="datePublished" title="2023-03-05 17:39">2023-03-05 17:39</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>I've been racking my brain for a week trying to complete this assignment and
I'm hoping someone here can lead me toward the right path. Let me start with
the instructor's instructions:</p>
<blockquote>
<p>Your assignment is the opposite of our first lab assignment, which was to
optimize a prime number program. Your purpose in this assignment is to
pessimize the program, i.e. make it run slower. Both of these are CPU-
intensive programs. They take a few seconds to run on our lab PCs. You may
not change the algorithm.</p>
<p>To deoptimize the program, use your knowledge of how the Intel i7 pipeline
operates. Imagine ways to re-order instruction paths to introduce WAR, RAW,
and other hazards. Think of ways to minimize the effectiveness of the cache.
Be diabolically incompetent.</p>
</blockquote>
<p>The assignment gave a choice of Whetstone or Monte-Carlo programs. The cache-
effectiveness comments are mostly only applicable to Whetstone, but I chose
the Monte-Carlo simulation program:</p>
<div class="code"><pre class="code literal-block"><span class="c1">// Un-modified baseline for pessimization, as given in the assignment</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;algorithm&gt;</span><span class="c1">    // Needed for the "max" function</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cmath&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;iostream&gt;</span>

<span class="c1">// A simple implementation of the Box-Muller algorithm, used to generate</span>
<span class="c1">// gaussian random numbers - necessary for the Monte Carlo method below</span>
<span class="c1">// Note that C++11 actually provides std::normal_distribution&lt;&gt; in </span>
<span class="c1">// the &lt;random&gt; library, which can be used instead of this function</span>
<span class="kr">double</span><span class="w"> </span><span class="nf">gaussian_box_muller</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="kr">double</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span><span class="p">;</span>
<span class="w">  </span><span class="kr">double</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span><span class="p">;</span>
<span class="w">  </span><span class="kr">double</span><span class="w"> </span><span class="n">euclid_sq</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Continue generating two uniform random variables</span>
<span class="w">  </span><span class="c1">// until the square of their "euclidean distance" </span>
<span class="w">  </span><span class="c1">// is less than unity</span>
<span class="w">  </span><span class="k">do</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">2.0</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">rand</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="kr">static_cast</span><span class="o">&lt;</span><span class="kr">double</span><span class="o">&gt;</span><span class="p">(</span><span class="n">RAND_MAX</span><span class="p">)</span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">2.0</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">rand</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="kr">static_cast</span><span class="o">&lt;</span><span class="kr">double</span><span class="o">&gt;</span><span class="p">(</span><span class="n">RAND_MAX</span><span class="p">)</span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="n">euclid_sq</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="o">*</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">y</span><span class="o">*</span><span class="n">y</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span><span class="w"> </span><span class="k">while</span><span class="w"> </span><span class="p">(</span><span class="n">euclid_sq</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mf">1.0</span><span class="p">);</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">x</span><span class="o">*</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">-2</span><span class="o">*</span><span class="n">log</span><span class="p">(</span><span class="n">euclid_sq</span><span class="p">)</span><span class="o">/</span><span class="n">euclid_sq</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// Pricing a European vanilla call option with a Monte Carlo method</span>
<span class="kr">double</span><span class="w"> </span><span class="nf">monte_carlo_call_price</span><span class="p">(</span><span class="kr">const</span><span class="w"> </span><span class="kr">int</span><span class="o">&amp;</span><span class="w"> </span><span class="n">num_sims</span><span class="p">,</span><span class="w"> </span><span class="kr">const</span><span class="w"> </span><span class="kr">double</span><span class="o">&amp;</span><span class="w"> </span><span class="n">S</span><span class="p">,</span><span class="w"> </span><span class="kr">const</span><span class="w"> </span><span class="kr">double</span><span class="o">&amp;</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="kr">const</span><span class="w"> </span><span class="kr">double</span><span class="o">&amp;</span><span class="w"> </span><span class="n">r</span><span class="p">,</span><span class="w"> </span><span class="kr">const</span><span class="w"> </span><span class="kr">double</span><span class="o">&amp;</span><span class="w"> </span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="kr">const</span><span class="w"> </span><span class="kr">double</span><span class="o">&amp;</span><span class="w"> </span><span class="n">T</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="kr">double</span><span class="w"> </span><span class="n">S_adjust</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">S</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">exp</span><span class="p">(</span><span class="n">T</span><span class="o">*</span><span class="p">(</span><span class="n">r</span><span class="mf">-0.5</span><span class="o">*</span><span class="n">v</span><span class="o">*</span><span class="n">v</span><span class="p">));</span>
<span class="w">  </span><span class="kr">double</span><span class="w"> </span><span class="n">S_cur</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span><span class="p">;</span>
<span class="w">  </span><span class="kr">double</span><span class="w"> </span><span class="n">payoff_sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span><span class="p">;</span>

<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kr">int</span><span class="w"> </span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">&lt;</span><span class="n">num_sims</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kr">double</span><span class="w"> </span><span class="n">gauss_bm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gaussian_box_muller</span><span class="p">();</span>
<span class="w">    </span><span class="n">S_cur</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">S_adjust</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">exp</span><span class="p">(</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">v</span><span class="o">*</span><span class="n">v</span><span class="o">*</span><span class="n">T</span><span class="p">)</span><span class="o">*</span><span class="n">gauss_bm</span><span class="p">);</span>
<span class="w">    </span><span class="n">payoff_sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="nf">max</span><span class="p">(</span><span class="n">S_cur</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="n">payoff_sum</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="kr">static_cast</span><span class="o">&lt;</span><span class="kr">double</span><span class="o">&gt;</span><span class="p">(</span><span class="n">num_sims</span><span class="p">))</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">r</span><span class="o">*</span><span class="n">T</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// Pricing a European vanilla put option with a Monte Carlo method</span>
<span class="kr">double</span><span class="w"> </span><span class="nf">monte_carlo_put_price</span><span class="p">(</span><span class="kr">const</span><span class="w"> </span><span class="kr">int</span><span class="o">&amp;</span><span class="w"> </span><span class="n">num_sims</span><span class="p">,</span><span class="w"> </span><span class="kr">const</span><span class="w"> </span><span class="kr">double</span><span class="o">&amp;</span><span class="w"> </span><span class="n">S</span><span class="p">,</span><span class="w"> </span><span class="kr">const</span><span class="w"> </span><span class="kr">double</span><span class="o">&amp;</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="kr">const</span><span class="w"> </span><span class="kr">double</span><span class="o">&amp;</span><span class="w"> </span><span class="n">r</span><span class="p">,</span><span class="w"> </span><span class="kr">const</span><span class="w"> </span><span class="kr">double</span><span class="o">&amp;</span><span class="w"> </span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="kr">const</span><span class="w"> </span><span class="kr">double</span><span class="o">&amp;</span><span class="w"> </span><span class="n">T</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="kr">double</span><span class="w"> </span><span class="n">S_adjust</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">S</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">exp</span><span class="p">(</span><span class="n">T</span><span class="o">*</span><span class="p">(</span><span class="n">r</span><span class="mf">-0.5</span><span class="o">*</span><span class="n">v</span><span class="o">*</span><span class="n">v</span><span class="p">));</span>
<span class="w">  </span><span class="kr">double</span><span class="w"> </span><span class="n">S_cur</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span><span class="p">;</span>
<span class="w">  </span><span class="kr">double</span><span class="w"> </span><span class="n">payoff_sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span><span class="p">;</span>

<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kr">int</span><span class="w"> </span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">&lt;</span><span class="n">num_sims</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kr">double</span><span class="w"> </span><span class="n">gauss_bm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gaussian_box_muller</span><span class="p">();</span>
<span class="w">    </span><span class="n">S_cur</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">S_adjust</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">exp</span><span class="p">(</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">v</span><span class="o">*</span><span class="n">v</span><span class="o">*</span><span class="n">T</span><span class="p">)</span><span class="o">*</span><span class="n">gauss_bm</span><span class="p">);</span>
<span class="w">    </span><span class="n">payoff_sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="nf">max</span><span class="p">(</span><span class="n">K</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">S_cur</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="n">payoff_sum</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="kr">static_cast</span><span class="o">&lt;</span><span class="kr">double</span><span class="o">&gt;</span><span class="p">(</span><span class="n">num_sims</span><span class="p">))</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">r</span><span class="o">*</span><span class="n">T</span><span class="p">);</span>
<span class="p">}</span>

<span class="kr">int</span><span class="w"> </span><span class="nf">main</span><span class="p">(</span><span class="kr">int</span><span class="w"> </span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="kr">char</span><span class="w"> </span><span class="o">**</span><span class="n">argv</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// First we create the parameter list                                                                               </span>
<span class="w">  </span><span class="kr">int</span><span class="w"> </span><span class="n">num_sims</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">10000000</span><span class="p">;</span><span class="w">   </span><span class="c1">// Number of simulated asset paths                                                       </span>
<span class="w">  </span><span class="kr">double</span><span class="w"> </span><span class="n">S</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">100.0</span><span class="p">;</span><span class="w">  </span><span class="c1">// Option price                                                                                  </span>
<span class="w">  </span><span class="kr">double</span><span class="w"> </span><span class="n">K</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">100.0</span><span class="p">;</span><span class="w">  </span><span class="c1">// Strike price                                                                                  </span>
<span class="w">  </span><span class="kr">double</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.05</span><span class="p">;</span><span class="w">   </span><span class="c1">// Risk-free rate (5%)                                                                           </span>
<span class="w">  </span><span class="kr">double</span><span class="w"> </span><span class="n">v</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.2</span><span class="p">;</span><span class="w">    </span><span class="c1">// Volatility of the underlying (20%)                                                            </span>
<span class="w">  </span><span class="kr">double</span><span class="w"> </span><span class="n">T</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0</span><span class="p">;</span><span class="w">    </span><span class="c1">// One year until expiry</span>

<span class="w">  </span><span class="c1">// Then we calculate the call/put values via Monte Carlo                                                                          </span>
<span class="w">  </span><span class="kr">double</span><span class="w"> </span><span class="n">call</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">monte_carlo_call_price</span><span class="p">(</span><span class="n">num_sims</span><span class="p">,</span><span class="w"> </span><span class="n">S</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="n">r</span><span class="p">,</span><span class="w"> </span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="n">T</span><span class="p">);</span>
<span class="w">  </span><span class="kr">double</span><span class="w"> </span><span class="nf">put</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">monte_carlo_put_price</span><span class="p">(</span><span class="n">num_sims</span><span class="p">,</span><span class="w"> </span><span class="n">S</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="n">r</span><span class="p">,</span><span class="w"> </span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="n">T</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Finally we output the parameters and prices                                                                      </span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">"Number of Paths: "</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">num_sims</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">"Underlying:      "</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">S</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">"Strike:          "</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">K</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">"Risk-Free Rate:  "</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">"Volatility:      "</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">v</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">"Maturity:        "</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">T</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">"Call Price:      "</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">call</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">"Put Price:       "</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="nf">put</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>

<p>The changes I have made seemed to increase the code running time by a second
but I'm not entirely sure what I can change to stall the pipeline without
adding code. A point to the right direction would be awesome, I appreciate any
responses.</p>
<hr>
<h3>Update: the professor who gave this assignment posted some details</h3>
<p>The highlights are:</p>
<ul>
<li>It's a second semester architecture class at a community college (using the Hennessy and Patterson textbook).</li>
<li>the lab computers have Haswell CPUs</li>
<li>The students have been exposed to the <code>CPUID</code> instruction and how to determine cache size, as well as intrinsics and the <code>CLFLUSH</code> instruction.</li>
<li>any compiler options are allowed, and so is inline asm.</li>
<li>Writing your own square root algorithm was announced as being outside the pale</li>
</ul>
<p>Cowmoogun's comments on the meta thread indicate that it wasn't clear compiler
optimizations could be part of this, and assumed <code>-O0</code>, and that a 17%
increase in run-time was reasonable.</p>
<p>So it sounds like the goal of the assignment was to get students to re-order
the existing work to reduce instruction-level parallelism or things like that,
but it's not a bad thing that people have delved deeper and learned more.</p>
<hr>
<p>Keep in mind that this is a computer-architecture question, not a question
about how to make C++ slow in general.</p>
<p><br><br></p>
<h2>Answer</h2>
<p>Important background reading: <strong>Agner Fog's microarch pdf</strong> , and probably
also Ulrich Drepper's What Every Programmer Should Know About Memory. See also
the other links in the x86 tag wiki, especially Intel's optimization manuals,
and David Kanter's analysis of the Haswell microarchitecture, with diagrams.</p>
<p>Very cool assignment; much better than the ones I've seen where students were
asked to optimize some code for <code>gcc -O0</code>, learning a bunch of tricks that
don't matter in real code. In this case, you're being asked to learn about the
CPU pipeline and use that to guide your de-optimization efforts, not just
blind guessing. <strong>The most fun part of this one is justifying each
pessimization with "diabolical incompetence", not intentional malice.</strong></p>
<hr>
<p><strong>Problems with the assignment wording and code</strong> :</p>
<p>The uarch-specific options for this code are limited. It doesn't use any
arrays, and much of the cost is calls to <code>exp</code>/<code>log</code> library functions. There
isn't an obvious way to have more or less instruction-level parallelism, and
the loop-carried dependency chain is very short.</p>
<p>It would be hard to get a slowdown just from re-arranging the expressions to
change the dependencies, to reduce ILP from hazards.</p>
<p>Intel Sandybridge-family CPUs are aggressive out-of-order designs that spend
lots of transistors and power to find parallelism and avoid hazards
(dependencies) that would trouble a classic RISC in-order pipeline. Usually
the only traditional hazards that slow it down are RAW "true" dependencies
that cause throughput to be limited by latency.</p>
<p><strong>WAR and WAW hazards for registers are pretty much not an issue, thanks to
register renaming</strong>. (except for <code>popcnt</code>/<code>lzcnt</code>/<code>tzcnt</code>, which have a false
dependency their destination on Intel CPUs, even though it should be write-
only).</p>
<p>For memory ordering, modern CPUs use a store buffer to delay commit into cache
until retirement, also avoiding WAR and WAW hazards. See also this answer
about what a store buffer is, and being essential essential for OoO exec to
decouple execution from things other cores can see.</p>
<p>Why does mulss take only 3 cycles on Haswell, different from Agner's
instruction tables? (Unrolling FP loops with multiple accumulators) has more
about register renaming and hiding FMA latency in an FP dot product loop.</p>
<hr>
<p><strong>The "i7" brand-name was introduced with Nehalem (successor to Core2)</strong> , and
some Intel manuals even say Core i7 when they seem to mean Nehalem, but they
kept the "i7" branding for Sandybridge and later microarchitectures. SnB is
when the P6-family evolved into a new species, the SnB-family. In many ways,
Nehalem has more in common with Pentium III than with Sandybridge (e.g.
register read stalls aka ROB-read stalls don't happen on SnB, because it
changed to using a physical register file. Also a uop cache and a different
internal uop format). <strong>The term "i7 architecture" is not useful</strong> , because
it makes little sense to group the SnB-family with Nehalem but not Core2.
(Nehalem did introduce the shared inclusive L3 cache architecture for
connecting multiple cores together, though. And also integrated GPUs. So chip-
level, the naming makes more sense.)</p>
<hr>
<h2>Summary of the good ideas that diabolical incompetence can justify</h2>
<p>Even the diabolically incompetent are unlikely to add obviously useless work
or an infinite loop, and making a mess with C++/Boost classes is beyond the
scope of the assignment.</p>
<ul>
<li>Multi-thread with a single <em>shared</em> <code>std::atomic&lt;uint64_t&gt;</code> loop counter, so the right total number of iterations happen. Atomic uint64_t is especially bad with <code>-m32 -march=i586</code>. For bonus points, arrange for it to be misaligned, and crossing a page boundary with an uneven split (not 4:4).</li>
<li>
<strong>False sharing</strong> for some other non-atomic variable -&gt; memory-order mis-speculation pipeline clears, as well as extra cache misses.</li>
<li>Instead of using <code>-</code> on FP variables, XOR the high byte with 0x80 to flip the sign bit, causing <strong>store-forwarding stalls</strong>.</li>
<li>Time each iteration independently, with something even heavier than <code>RDTSC</code>. e.g. <code>CPUID</code> / <code>RDTSC</code> or a time function that makes a system call. Serializing instructions are inherently pipeline-unfriendly.</li>
<li>Change multiplies by constants to divides by their reciprocal ("for ease of reading"). <strong>div is slow and not fully pipelined.</strong>
</li>
<li>Vectorize the multiply/sqrt with AVX (SIMD), but fail to use <code>vzeroupper</code> before calls to scalar math-library <code>exp()</code> and <code>log()</code> functions, causing <strong>AVX &lt;-&gt;SSE transition stalls</strong>.</li>
<li>Store the RNG output in a linked list, or in arrays which you traverse out of order. Same for the result of each iteration, and sum at the end.</li>
</ul>
<p>Also covered in this answer but excluded from the summary: suggestions that
would be just as slow on a non-pipelined CPU, or that don't seem to be
justifiable even with diabolical incompetence. e.g. many gimp-the-compiler
ideas that produce obviously different / worse asm.</p>
<hr>
<h3>Multi-thread badly</h3>
<p>Maybe use OpenMP to multi-thread loops with very few iterations, with way more
overhead than speed gain. Your monte-carlo code has enough parallelism to
actually get a speedup, though, esp. if we succeed at making each iteration
slow. (Each thread computes a partial <code>payoff_sum</code>, added at the end). <code>#omp
parallel</code> on that loop would probably be an optimization, not a pessimization.</p>
<p><strong>Multi-thread but force both threads to share the same loop counter
(with<code>atomic</code> increments so the total number of iterations is correct).</strong> This
seems diabolically logical. This means using a <code>static</code> variable as a loop
counter. This justifies use of <code>atomic</code> for loop counters, and creates actual
cache-line ping-ponging (as long as the threads don't run on the same physical
core with hyperthreading; that might not be <em>as</em> slow). Anyway, this is <em>much</em>
slower than the un-contended case for <code>lock xadd</code> or <code>lock dec</code>. And <code>lock
cmpxchg8b</code> to atomically increment a contended <code>uint64_t</code> on a 32bit system
will have to retry in a loop instead of having the hardware arbitrate an
atomic <code>inc</code>.</p>
<p>Also create <strong>false sharing</strong> , where multiple threads keep their private data
(e.g. RNG state) in different bytes of the same cache line. (Intel tutorial
about it, including perf counters to look at). <strong>There's a microarchitecture-
specific aspect to this</strong> : Intel CPUs speculate on memory mis-ordering <em>not</em>
happening, and there's a memory-order machine-clear perf event to detect this,
at least on P4. The penalty might not be as large on Haswell. As that link
points out, a <code>lock</code>ed instruction assumes this will happen, avoiding mis-
speculation. A normal load speculates that other cores won't invalidate a
cache line between when the load executes and when it retires in program-order
(unless you use <code>pause</code>). True sharing without <code>lock</code>ed instructions is
usually a bug. It would be interesting to compare a non-atomic shared loop
counter with the atomic case. To really pessimize, keep the shared atomic loop
counter, and cause false sharing in the same or a different cache line for
some other variable.</p>
<hr>
<h2>Random uarch-specific ideas:</h2>
<p>If you can introduce <strong>any unpredictable branches</strong> , that will pessimize the
code substantially. Modern x86 CPUs have quite long pipelines, so a mispredict
costs ~15 cycles (when running from the uop cache).</p>
<hr>
<h4>Dependency chains:</h4>
<p>I think this was one of the intended parts of the assignment.</p>
<p>Defeat the CPU's ability to exploit instruction-level parallelism by choosing
an order of operations that has one long dependency chain instead of multiple
short dependency chains. Compilers aren't allowed to change the order of
operations for FP calculations unless you use <code>-ffast-math</code>, because that can
change the results (as discussed below).</p>
<p>To really make this effective, increase the length of a loop-carried
dependency chain. Nothing leaps out as obvious, though: The loops as written
have very short loop-carried dependency chains: just an FP add. (3 cycles).
Multiple iterations can have their calculations in-flight at once, because
they can start well before the <code>payoff_sum +=</code> at the end of the previous
iteration. (<code>log()</code> and <code>exp</code> take many instructions, but not a lot more than
Haswell's out-of-order window for finding parallelism: ROB size=192 fused-
domain uops, and scheduler size=60 unfused-domain uops. As soon as execution
of the current iteration progresses far enough to make room for instructions
from the next iteration to issue, any parts of it that have their inputs ready
(i.e. independent/separate dep chain) can start executing when older
instructions leave the execution units free (e.g. because they're bottlenecked
on latency, not throughput.).</p>
<p>The RNG state will almost certainly be a longer loop-carried dependency chain
than the <code>addps</code>.</p>
<hr>
<h4>Use slower/more FP operations (esp. more division):</h4>
<p>Divide by 2.0 instead of multiplying by 0.5, and so on. FP multiply is heavily
pipelined in Intel designs, and has one per 0.5c throughput on Haswell and
later. <strong>FP<code>divsd</code>/<code>divpd</code> is only partially pipelined</strong>. (Although Skylake
has an impressive one per 4c throughput for <code>divpd xmm</code>, with 13-14c latency,
vs not pipelined at all on Nehalem (7-22c)).</p>
<p>The <code>do { ...; euclid_sq = x*x + y*y; } while (euclid_sq &gt;= 1.0);</code> is clearly
testing for a distance, so clearly it would be proper to <code>sqrt()</code> it. :P
(<code>sqrt</code> is even slower than <code>div</code>).</p>
<p>As @Paul Clayton suggests, rewriting expressions with associative/distributive
equivalents can introduce more work (as long as you don't use <code>-ffast-math</code> to
allow the compiler to re-optimize). <code>(exp(T*(r-0.5*v*v))</code> could become
<code>exp(T*r - T*v*v/2.0)</code>. Note that while math on real numbers is associative,
floating point math is <em>not</em> , even without considering overflow/NaN (which is
why <code>-ffast-math</code> isn't on by default). See Paul's comment for a very hairy
nested <code>pow()</code> suggestion.</p>
<p>If you can scale the calculations down to very small numbers, then FP math ops
take <strong>~120 extra cycles to trap to microcode when an operation on two normal
numbers produces a denormal</strong>. See Agner Fog's microarch pdf for the exact
numbers and details. This is unlikely since you have a lot of multiplies, so
the scale factor would be squared and underflow all the way to 0.0. I don't
see any way to justify the necessary scaling with incompetence (even
diabolical), only intentional malice.</p>
<hr>
<h4>If you can use intrinsics (<code>&lt;immintrin.h&gt;</code>)</h4>
<p>Use <code>movnti</code> to evict your data from cache. Diabolical: it's new and weakly-
ordered, so that should let the CPU run it faster, right? Or see that linked
question for a case where someone was in danger of doing exactly this (for
scattered writes where only some of the locations were hot). <code>clflush</code> is
probably impossible without malice.</p>
<p>Use integer shuffles between FP math operations to cause bypass delays.</p>
<p><strong>Mixing SSE and AVX instructions without proper use of<code>vzeroupper</code> causes
large stalls in pre-Skylake</strong> (and a different penalty in Skylake). Even
without that, vectorizing badly can be worse than scalar (more cycles spent
shuffling data into/out of vectors than saved by doing the
add/sub/mul/div/sqrt operations for 4 Monte-Carlo iterations at once, with
256b vectors). add/sub/mul execution units are fully pipelined and full-width,
but div and sqrt on 256b vectors aren't as fast as on 128b vectors (or
scalars), so the speedup isn't dramatic for <code>double</code>.</p>
<p><code>exp()</code> and <code>log()</code> don't have hardware support, so that part would require
extracting vector elements back to scalar and calling the library function
separately, then shuffling the results back into a vector. libm is typically
compiled to only use SSE2, so will use the legacy-SSE encodings of scalar math
instructions. If your code uses 256b vectors and calls <code>exp</code> without doing a
<code>vzeroupper</code> first, then you stall. After returning, an AVX-128 instruction
like <code>vmovsd</code> to set up the next vector element as an arg for <code>exp</code> will also
stall. And then <code>exp()</code> will stall again when it runs an SSE instruction.
<strong>This is exactly what happened in this question, causing a 10x slowdown.</strong>
(Thanks @ZBoson).</p>
<p>See also Nathan Kurz's experiments with Intel's math lib vs. glibc for this
code. Future glibc will come with vectorized implementations of <code>exp()</code> and so
on.</p>
<hr>
<p>If targeting pre-IvB, or esp. Nehalem, try to get gcc to cause partial-
register stalls with 16bit or 8bit operations followed by 32bit or 64bit
operations. In most cases, gcc will use <code>movzx</code> after an 8 or 16bit operation,
but here's a case where gcc modifies <code>ah</code> and then reads <code>ax</code></p>
<hr>
<h3>With (inline) asm:</h3>
<p>With (inline) asm, you could break the uop cache: A 32B chunk of code that
doesn't fit in three 6uop cache lines forces a switch from the uop cache to
the decoders. An incompetent <code>ALIGN</code> (like NASM's default) using many single-
byte <code>nop</code>s instead of a couple long <code>nop</code>s on a branch target inside the
inner loop might do the trick. Or put the alignment padding after the label,
instead of before. :P This only matters if the frontend is a bottleneck, which
it won't be if we succeeded at pessimizing the rest of the code.</p>
<p>Use self-modifying code to trigger pipeline clears (aka machine-nukes).</p>
<p>LCP stalls from 16bit instructions with immediates too large to fit in 8 bits
are unlikely to be useful. The uop cache on SnB and later means you only pay
the decode penalty once. On Nehalem (the first i7), it might work for a loop
that doesn't fit in the 28 uop loop buffer. gcc will sometimes generate such
instructions, even with <code>-mtune=intel</code> and when it could have used a 32bit
instruction.</p>
<hr>
<p>A common idiom for timing is <code>CPUID</code>(to serialize) then <code>RDTSC</code>. Time every
iteration separately with a <code>CPUID</code>/<code>RDTSC</code> to make sure the <code>RDTSC</code> isn't
reordered with earlier instructions, which will slow things down a <em>lot</em>. (In
real life, the smart way to time is to time all the iterations together,
instead of timing each separately and adding them up).</p>
<hr>
<h2>Cause lots of cache misses and other memory slowdowns</h2>
<p>Use a <code>union { double d; char a[8]; }</code> for some of your variables. <strong>Cause a
store-forwarding stall</strong> by doing a narrow store (or Read-Modify-Write) to
just one of the bytes. (That wiki article also covers a lot of other
microarchitectural stuff for load/store queues). e.g. <strong>flip the sign of
a<code>double</code> using XOR 0x80 on just the high byte</strong>, instead of a <code>-</code> operator.
The diabolically incompetent developer may have heard that FP is slower than
integer, and thus try to do as much as possible using integer ops. (A compiler
could theoretically still compile this to an <code>xorps</code> with a constant like <code>-</code>,
but for x87 the compiler would have to realize that it's negating the value
and <code>fchs</code> or replace the next add with a subtract.)</p>
<hr>
<p>Use <code>volatile</code> if you're compiling with <code>-O3</code> and not using <code>std::atomic</code>, to
force the compiler to actually store/reload all over the place. Global
variables (instead of locals) will also force some stores/reloads, but the C++
memory model's weak ordering doesn't require the compiler to spill/reload to
memory all the time.</p>
<p><strong>Replace local vars with members of a big struct, so you can control the
memory layout.</strong></p>
<p>Use arrays in the struct for padding (and storing random numbers, to justify
their existence).</p>
<p>Choose your memory layout so everything goes into a different line in the same
"set" in the L1 cache. It's only 8-way associative, i.e. each set has 8
"ways". Cache lines are 64B.</p>
<p>Even better, <strong>put things exactly 4096B apart, since loads have a false
dependency on stores to different pages but with the same offset within a
page</strong>. Aggressive out-of-order CPUs use Memory Disambiguation to figure out
when loads and stores can be reordered without changing the results, and
Intel's implementation has false-positives that prevent loads from starting
early. Probably they only check bits below the page offset so it can start
before the TLB has translated the high bits from a virtual page to a physical
page. As well as Agner's guide, see this answer, and a section near the end of
@Krazy Glew's answer on the same question. (Andy Glew was an architect of
Intel's PPro - P6 microarchitecture.) (Also related:
https://stackoverflow.com/a/53330296 and https://github.com/travisdowns/uarch-
bench/wiki/Memory-Disambiguation-on-Skylake)</p>
<p>Use <code>__attribute__((packed))</code> to let you mis-align variables so they span
cache-line or even page boundaries. (So a load of one <code>double</code> needs data from
two cache-lines). Misaligned loads have no penalty in any Intel i7 uarch,
except when crossing cache lines and page lines. Cache-line splits still take
extra cycles. Skylake dramatically reduces the penalty for page split loads,
from 100 to 5 cycles. (Section 2.1.3). (And can do two page walks in
parallel).</p>
<p><strong>A page-split on an<code>atomic&lt;uint64_t&gt;</code> should be just about the worst case</strong>,
esp. if it's 5 bytes in one page and 3 bytes in the other page, or anything
other than 4:4. Even splits down the middle are more efficient for cache-line
splits with 16B vectors on some uarches, IIRC. Put everything in a
<code>alignas(4096) struct __attribute((packed))</code> (to save space, of course),
including an array for storage for the RNG results. Achieve the misalignment
by using <code>uint8_t</code> or <code>uint16_t</code> for something before the counter.</p>
<p>If you can get the compiler to use indexed addressing modes, that will defeat
uop micro-fusion. Maybe by using <code>#define</code>s to replace simple scalar variables
with <code>my_data[constant]</code>.</p>
<p>If you can introduce an extra level of indirection, so load/store addresses
aren't known early, that can pessimize further.</p>
<hr>
<h4>Traverse arrays in non-contiguous order</h4>
<p>I think we can come up with incompetent justification for introducing an array
in the first place: It lets us separate the random number generation from the
random number use. Results of each iteration could also be stored in an array,
to be summed later (with more diabolical incompetence).</p>
<p>For "maximum randomness", we could have a thread looping over the random array
writing new random numbers into it. The thread consuming the random numbers
could generate a random index to load a random number from. (There's some
make-work here, but microarchitecturally it helps for load-addresses to be
known early so any possible load latency can be resolved before the loaded
data is needed.) Having a reader and writer on different cores will cause
memory-ordering mis-speculation pipeline clears (as discussed earlier for the
false-sharing case).</p>
<p>For maximum pessimization, loop over your array with a stride of 4096 bytes
(i.e. 512 doubles). e.g.</p>
<div class="code"><pre class="code literal-block"><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="nc">int</span><span class="w"> </span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">&lt;</span><span class="mi">512</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="nc">int</span><span class="w"> </span><span class="n">j</span><span class="o">=</span><span class="n">i</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">&lt;</span><span class="n">UPPER_BOUND</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">+=</span><span class="mi">512</span><span class="p">)</span>
<span class="w">        </span><span class="n">monte_carlo_step</span><span class="p">(</span><span class="n">rng_array</span><span class="o">[</span><span class="n">j</span><span class="o">]</span><span class="p">);</span>
</pre></div>

<p>So the access pattern is 0, 4096, 8192, ...,<br>
8, 4104, 8200, ...<br>
16, 4112, 8208, ...</p>
<p>This is what you'd get for accessing a 2D array like <code>double
rng_array[MAX_ROWS][512]</code> in the wrong order (looping over rows, instead of
columns within a row in the inner loop, as suggested by @JesperJuhl). If
diabolical incompetence can justify a 2D array with dimensions like that,
garden variety real-world incompetence easily justifies looping with the wrong
access pattern. This happens in real code in real life.</p>
<p>Adjust the loop bounds if necessary to use many different pages instead of
reusing the same few pages, if the array isn't that big. Hardware prefetching
doesn't work (as well/at all) across pages. The prefetcher can track one
forward and one backward stream within each page (which is what happens here),
but will only act on it if the memory bandwidth isn't already saturated with
non-prefetch.</p>
<p>This will also generate lots of TLB misses, unless the pages get merged into a
hugepage (Linux does this opportunistically for anonymous (not file-backed)
allocations like <code>malloc</code>/<code>new</code> that use <code>mmap(MAP_ANONYMOUS)</code>).</p>
<p>Instead of an array to store the list of results, you could use a <strong>linked
list</strong>. Every iteration would require a pointer-chasing load (a RAW true
dependency hazard for the load-address of the next load). With a bad
allocator, you might manage to scatter the list nodes around in memory,
defeating cache. With a bad toy allocator, it could put every node at the
beginning of its own page. (e.g. allocate with <code>mmap(MAP_ANONYMOUS)</code> directly,
without breaking up pages or tracking object sizes to properly support
<code>free</code>).</p>
<hr>
<p>These aren't really microarchitecture-specific, and have little to do with the
pipeline (most of these would also be a slowdown on a non-pipelined CPU).</p>
<h4>Somewhat off-topic: make the compiler generate worse code / do more work:</h4>
<p>Use C++11 <code>std::atomic&lt;int&gt;</code> and <code>std::atomic&lt;double&gt;</code> for the most pessimal
code. The MFENCEs and <code>lock</code>ed instructions are quite slow even without
contention from another thread.</p>
<p><code>-m32</code> will make slower code, because x87 code will be worse than SSE2 code.
The stack-based 32bit calling convention takes more instructions, and passes
even FP args on the stack to functions like <code>exp()</code>.
<code>atomic&lt;uint64_t&gt;::operator++</code> on <code>-m32</code> requires a <code>lock cmpxchg8B</code> loop
(i586). (So use that for loop counters! [Evil laugh]).</p>
<p><code>-march=i386</code> will also pessimize (thanks @Jesper). FP compares with <code>fcom</code>
are slower than 686 <code>fcomi</code>. Pre-586 doesn't provide an atomic 64bit store,
(let alone a cmpxchg), so all 64bit <code>atomic</code> ops compile to libgcc function
calls (which is probably compiled for i686, rather than actually using a
lock). Try it on the Godbolt Compiler Explorer link in the last paragraph.</p>
<p>Use <code>long double</code> / <code>sqrtl</code> / <code>expl</code> for extra precision and extra slowness in
ABIs where sizeof(<code>long double</code>) is 10 or 16 (with padding for alignment).
(IIRC, 64bit Windows uses 8byte <code>long double</code> equivalent to <code>double</code>. (Anyway,
load/store of 10byte (80bit) FP operands is 4 / 7 uops, vs. <code>float</code> or
<code>double</code> only taking 1 uop each for <code>fld m64/m32</code>/<code>fst</code>). Forcing x87 with
<code>long double</code> defeats auto-vectorization even for gcc <code>-m64 -march=haswell
-O3</code>.</p>
<p>If not using <code>atomic&lt;uint64_t&gt;</code> loop counters, use <code>long double</code> for
everything, including loop counters.</p>
<p><code>atomic&lt;double&gt;</code> compiles, but read-modify-write operations like <code>+=</code> aren't
supported for it (even on 64bit). <code>atomic&lt;long double&gt;</code> has to call a library
function just for atomic loads/stores. It's probably really inefficient,
because the x86 ISA doesn't naturally support atomic 10byte loads/stores, and
the only way I can think of without locking (<code>cmpxchg16b</code>) requires 64bit
mode.</p>
<hr>
<p>At <code>-O0</code>, breaking up a big expression by assigning parts to temporary vars
will cause more store/reloads. Without <code>volatile</code> or something, this won't
matter with optimization settings that a real build of real code would use.</p>
<p>C aliasing rules allow a <code>char</code> to alias anything, so storing through a
<code>char*</code> forces the compiler to store/reload everything before/after the byte-
store, even at <code>-O3</code>. (This is a problem for auto-vectorizing code that
operates on an array of <code>uint8_t</code>, for example.)</p>
<p>Try <code>uint16_t</code> loop counters, to force truncation to 16bit, probably by using
16bit operand-size (potential stalls) and/or extra <code>movzx</code> instructions
(safe). Signed overflow is undefined behaviour, so unless you use <code>-fwrapv</code> or
at least <code>-fno-strict-overflow</code>, signed loop counters don't have to be re-
sign-extended every iteration, even if used as offsets to 64bit pointers.</p>
<hr>
<p>Force conversion from integer to <code>float</code> and back again. And/or
<code>double</code>&lt;=&gt;<code>float</code> conversions. The instructions have latency &gt; 1, and scalar
int-&gt;float (<code>cvtsi2ss</code>) is badly designed to not zero the rest of the xmm
register. (gcc inserts an extra <code>pxor</code> to break dependencies, for this
reason.)</p>
<hr>
<p>Frequently <strong>set your CPU affinity to a different CPU</strong> (suggested by @Egwor).
diabolical reasoning: You don't want one core to get overheated from running
your thread for a long time, do you? Maybe swapping to another core will let
that core turbo to a higher clock speed. (In reality: they're so thermally
close to each other that this is highly unlikely except in a multi-socket
system). Now just get the tuning wrong and do it way too often. Besides the
time spent in the OS saving/restoring thread state, the new core has cold
L2/L1 caches, uop cache, and branch predictors.</p>
<p>Introducing frequent unnecessary system calls can slow you down no matter what
they are. Although some important but simple ones like <code>gettimeofday</code> may be
implemented in user-space with, with no transition to kernel mode. (glibc on
Linux does this with the kernel's help: the kernel exports code+data in the
VDSO).</p>
<p>For more on system call overhead (including cache/TLB misses after returning
to user-space, not just the context switch itself), the FlexSC paper has some
great perf-counter analysis of the current situation, as well as a proposal
for batching system calls from massively multi-threaded server processes.</p>
<p><br></p>
<h3>Suggest</h3>
<p>A few things that you can do to make things perform as bad as possible:</p>
<ul>
<li>
<p>compile the code for the i386 architecture. This will prevent the use of SSE and newer instructions and force the use of the x87 FPU.</p>
</li>
<li>
<p>use <code>std::atomic</code> variables everywhere. This will make them very expensive due to the compiler being forced to insert memory barriers all over the place. And this is something an incompetent person might plausibly do to "ensure thread safety".</p>
</li>
<li>
<p>make sure to access memory in the worst possible way for the prefetcher to predict (column major vs row major).</p>
</li>
<li>
<p>to make your variables extra expensive you could make sure they all have 'dynamic storage duration' (heap allocated) by allocating them with <code>new</code> rather than letting them have 'automatic storage duration' (stack allocated).</p>
</li>
<li>
<p>make sure that all memory you allocate is very oddly aligned and by all means avoid allocating huge pages, since doing so would be much too TLB efficient.</p>
</li>
<li>
<p>whatever you do, don't build your code with the compilers optimizer enabled. And make sure to enable the most expressive debug symbols you can (won't make the code <em>run</em> slower, but it'll waste some extra disk space).</p>
</li>
</ul>
<p>Note: This answer basically just summarizes my comments that @Peter Cordes
already incorporated into his very good answer. Suggest he get's your upvote
if you only have one to spare :)</p>
    </div>
    </article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="index-2785.html" rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-2783.html" rel="next">Older posts</a>
            </li>
        </ul></nav>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow中文网</a>  
            
        </footer>
</div>
</div>


            <script src="assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="assets/js/search.js"></script>
</body>
</html>
