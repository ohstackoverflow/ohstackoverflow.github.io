<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="This is a snapshot site for StackOverflow">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>StackOverflow Snapshot (old posts, page 1780) | StackOverflow Snapshot</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/index-1780.html">
<link rel="prev" href="index-1781.html" type="text/html">
<link rel="next" href="index-1779.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="archive.html">Archive</a>
                </li>
<li>
<a href="categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            

    


    
<div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/what-are-logits-what-is-the-difference-between-softmax-and-softmax-cross-entropy-with-logits/" class="u-url">What are logits? What is the difference between softmax and softmax_cross_entropy_with_logits?</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/what-are-logits-what-is-the-difference-between-softmax-and-softmax-cross-entropy-with-logits/" rel="bookmark">
            <time class="published dt-published" datetime="2023-03-03T09:11:54+08:00" itemprop="datePublished" title="2023-03-03 09:11">2023-03-03 09:11</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>In the tensorflow API docs they use a keyword called <code>logits</code>. What is it? A
lot of methods are written like:</p>
<div class="code"><pre class="code literal-block">tf.nn.softmax(logits, name=None)
</pre></div>

<p>If <code>logits</code> is just a generic <code>Tensor</code> input, why is it named <code>logits</code>?</p>
<hr>
<p>Secondly, what is the difference between the following two methods?</p>
<div class="code"><pre class="code literal-block">tf.nn.softmax(logits, name=None)
tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=None)
</pre></div>

<p>I know what <code>tf.nn.softmax</code> does, but not the other. An example would be
really helpful.</p>
<p><br><br></p>
<h2>Answer</h2>
<p>The softmax+logits simply means that the function operates on the unscaled
output of earlier layers and that the relative scale to understand the units
is linear. It means, in particular, the sum of the inputs may not equal 1,
that the values are <em>not</em> probabilities (you might have an input of 5).
Internally, it first applies softmax to the unscaled output, and then and then
computes the cross entropy of those values vs. what they "should" be as
defined by the labels.</p>
<p><code>tf.nn.softmax</code> produces the result of applying the softmax function to an
input tensor. The softmax "squishes" the inputs so that <code>sum(input) = 1</code>, and
it does the mapping by interpreting the inputs as log-probabilities (logits)
and then converting them back into raw probabilities between 0 and 1. The
shape of output of a softmax is the same as the input:</p>
<div class="code"><pre class="code literal-block"><span class="n">a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="o">.</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="o">.</span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="o">.</span><span class="mi">9</span><span class="p">]]))</span>
<span class="nb">print</span><span class="w"> </span><span class="n">s</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
<span class="p">[[</span><span class="w"> </span><span class="mf">0.16838508</span><span class="w">  </span><span class="mf">0.205666</span><span class="w">    </span><span class="mf">0.25120102</span><span class="w">  </span><span class="mf">0.37474789</span><span class="p">]]</span>
</pre></div>

<p>See this answer for more about why softmax is used extensively in DNNs.</p>
<p><code>tf.nn.softmax_cross_entropy_with_logits</code> combines the softmax step with the
calculation of the cross-entropy loss after applying the softmax function, but
it does it all together in a more mathematically careful way. It's similar to
the result of:</p>
<div class="code"><pre class="code literal-block">sm = tf.nn.softmax(x)
ce = cross_entropy(sm)
</pre></div>

<p>The cross entropy is a summary metric: it sums across the elements. The output
of <code>tf.nn.softmax_cross_entropy_with_logits</code> on a shape <code>[2,5]</code> tensor is of
shape <code>[2,1]</code> (the first dimension is treated as the batch).</p>
<p>If you want to do optimization to minimize the cross entropy <strong>AND</strong> you're
softmaxing after your last layer, you should use
<code>tf.nn.softmax_cross_entropy_with_logits</code> instead of doing it yourself,
because it covers numerically unstable corner cases in the mathematically
right way. Otherwise, you'll end up hacking it by adding little epsilons here
and there.</p>
<p><strong>Edited 2016-02-07:</strong> If you have single-class labels, where an object can
only belong to one class, you might now consider using
<code>tf.nn.sparse_softmax_cross_entropy_with_logits</code> so that you don't have to
convert your labels to a dense one-hot array. This function was added after
release 0.6.0.</p>
<p><br></p>
<h3>Suggest</h3>
<p><strong>Short version:</strong></p>
<p>Suppose you have two tensors, where <code>y_hat</code> contains computed scores for each
class (for example, from y = W*x +b) and <code>y_true</code> contains one-hot encoded
true labels.</p>
<div class="code"><pre class="code literal-block">y_hat  = ... # Predicted label, e.g. y = tf.matmul(X, W) + b
y_true = ... # True label, one-hot encoded
</pre></div>

<p>If you interpret the scores in <code>y_hat</code> as unnormalized log probabilities, then
they are <strong>logits</strong>.</p>
<p>Additionally, the total cross-entropy loss computed in this manner:</p>
<div class="code"><pre class="code literal-block">y_hat_softmax = tf.nn.softmax(y_hat)
total_loss = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(y_hat_softmax), [1]))
</pre></div>

<p>is essentially equivalent to the total cross-entropy loss computed with the
function <code>softmax_cross_entropy_with_logits()</code>:</p>
<div class="code"><pre class="code literal-block">total_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_hat, y_true))
</pre></div>

<p><strong>Long version:</strong></p>
<p>In the output layer of your neural network, you will probably compute an array
that contains the class scores for each of your training instances, such as
from a computation <code>y_hat = W*x + b</code>. To serve as an example, below I've
created a <code>y_hat</code> as a 2 x 3 array, where the rows correspond to the training
instances and the columns correspond to classes. So here there are 2 training
instances and 3 classes.</p>
<div class="code"><pre class="code literal-block"><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>

<span class="c1"># Create example y_hat.</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="kp">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],[</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.7</span><span class="p">]]))</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span>
<span class="c1"># array([[ 0.5,  1.5,  0.1],</span>
<span class="c1">#        [ 2.2,  1.3,  1.7]])</span>
</pre></div>

<p>Note that the values are not normalized (i.e. the rows don't add up to 1). In
order to normalize them, we can apply the softmax function, which interprets
the input as unnormalized log probabilities (aka <strong>logits</strong> ) and outputs
normalized linear probabilities.</p>
<div class="code"><pre class="code literal-block">y_hat_softmax = tf.nn.softmax(y_hat)
sess.run(y_hat_softmax)
# array([[ 0.227863  ,  0.61939586,  0.15274114],
#        [ 0.49674623,  0.20196195,  0.30129182]])
</pre></div>

<p>It's important to fully understand what the softmax output is saying. Below
I've shown a table that more clearly represents the output above. It can be
seen that, for example, the probability of training instance 1 being "Class 2"
is 0.619. The class probabilities for each training instance are normalized,
so the sum of each row is 1.0.</p>
<div class="code"><pre class="code literal-block">                      Pr(Class 1)  Pr(Class 2)  Pr(Class 3)
                    ,--------------------------------------
Training instance 1 | 0.227863   | 0.61939586 | 0.15274114
Training instance 2 | 0.49674623 | 0.20196195 | 0.30129182
</pre></div>

<p>So now we have class probabilities for each training instance, where we can
take the argmax() of each row to generate a final classification. From above,
we may generate that training instance 1 belongs to "Class 2" and training
instance 2 belongs to "Class 1".</p>
<p>Are these classifications correct? We need to measure against the true labels
from the training set. You will need a one-hot encoded <code>y_true</code> array, where
again the rows are training instances and columns are classes. Below I've
created an example <code>y_true</code> one-hot array where the true label for training
instance 1 is "Class 2" and the true label for training instance 2 is "Class
3".</p>
<div class="code"><pre class="code literal-block">y_true = tf.convert_to_tensor(np.array([[0.0, 1.0, 0.0],[0.0, 0.0, 1.0]]))
sess.run(y_true)
# array([[ 0.,  1.,  0.],
#        [ 0.,  0.,  1.]])
</pre></div>

<p>Is the probability distribution in <code>y_hat_softmax</code> close to the probability
distribution in <code>y_true</code>? We can use cross-entropy loss to measure the error.</p>
<p><img alt="Formula for cross-entropy loss" src="images/rODko.png"></p>
<p>We can compute the cross-entropy loss on a row-wise basis and see the results.
Below we can see that training instance 1 has a loss of 0.479, while training
instance 2 has a higher loss of 1.200. This result makes sense because in our
example above, <code>y_hat_softmax</code> showed that training instance 1's highest
probability was for "Class 2", which matches training instance 1 in <code>y_true</code>;
however, the prediction for training instance 2 showed a highest probability
for "Class 1", which does not match the true class "Class 3".</p>
<div class="code"><pre class="code literal-block">loss_per_instance_1 = -tf.reduce_sum(y_true * tf.log(y_hat_softmax), reduction_indices=[1])
sess.run(loss_per_instance_1)
# array([ 0.4790107 ,  1.19967598])
</pre></div>

<p>What we really want is the total loss over all the training instances. So we
can compute:</p>
<div class="code"><pre class="code literal-block">total_loss_1 = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(y_hat_softmax), reduction_indices=[1]))
sess.run(total_loss_1)
# 0.83934333897877944
</pre></div>

<p><strong>Using softmax_cross_entropy_with_logits()</strong></p>
<p>We can instead compute the total cross entropy loss using the
<code>tf.nn.softmax_cross_entropy_with_logits()</code> function, as shown below.</p>
<div class="code"><pre class="code literal-block">loss_per_instance_2 = tf.nn.softmax_cross_entropy_with_logits(y_hat, y_true)
sess.run(loss_per_instance_2)
# array([ 0.4790107 ,  1.19967598])

total_loss_2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_hat, y_true))
sess.run(total_loss_2)
# 0.83934333897877922
</pre></div>

<p>Note that <code>total_loss_1</code> and <code>total_loss_2</code> produce essentially equivalent
results with some small differences in the very final digits. However, you
might as well use the second approach: it takes one less line of code and
accumulates less numerical error because the softmax is done for you inside of
<code>softmax_cross_entropy_with_logits()</code>.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/disable-validation-of-html-form-elements/" class="u-url">Disable validation of HTML form elements</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/disable-validation-of-html-form-elements/" rel="bookmark">
            <time class="published dt-published" datetime="2023-03-03T09:11:29+08:00" itemprop="datePublished" title="2023-03-03 09:11">2023-03-03 09:11</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>In my forms, I'd like to use the new HTML form types, for example <code>&lt;input
type="url" /&gt;</code> (more info about the types here).</p>
<p>The problem is that Chrome wants to be super helpful and validate these
elements for me, except that it sucks at it. If it fails the built-in
validation, there's no message or indication other than the element getting
focus. I prefill URL elements with <code>"http://"</code>, and so my own custom
validation just treats those values as empty strings, however Chrome rejects
that. If I could change its validation rules, that would work too.</p>
<p>I know I could just revert back to using <code>type="text"</code> but I want the nice
enhancements using these new types offers (eg: it automatically switches to a
custom keyboard layout on mobile devices):</p>
<p><img alt="" src="images/uiPln.png"></p>
<p>Is there a way to switch off or customise the automatic validation?</p>
<p><br><br></p>
<h2>Answer</h2>
<p>If you want to disable client side validation for a form in HTML, add the
<code>novalidate</code> attribute to the form element. Ex:</p>
<div class="code"><pre class="code literal-block"><span class="nt">&lt;form</span><span class="w"> </span><span class="na">method=</span><span class="s">"post"</span><span class="w"> </span><span class="na">action=</span><span class="s">"/foo"</span><span class="w"> </span><span class="err">novalidate</span><span class="nt">&gt;</span>...<span class="nt">&lt;/form&gt;</span>
</pre></div>

<p>See https://www.w3.org/TR/html5/sec-forms.html#element-attrdef-form-novalidate</p>
<p><br></p>
<h3>Suggest</h3>
<p>I had a read of the spec and did some testing in Chrome, and if you catch the
"invalid" event and return false that seems to allow form submission.</p>
<p>I am using jquery, with this HTML.</p>
<div class="code"><pre class="code literal-block">//<span class="w"> </span>suppress<span class="w"> </span>"invalid"<span class="w"> </span>events<span class="w"> </span>on<span class="w"> </span>URL<span class="w"> </span>inputs
$('input[type="url"]').bind('invalid',<span class="w"> </span>function()<span class="w"> </span>{
<span class="w">  </span>alert('invalid');
<span class="w">  </span>return<span class="w"> </span>false;
});

document.forms[0].onsubmit<span class="w"> </span>=<span class="w"> </span>function<span class="w"> </span>()<span class="w"> </span>{
<span class="w">  </span>alert('form<span class="w"> </span>submitted');
};


<span class="nt">&lt;script</span><span class="w"> </span><span class="na">src=</span><span class="s">"https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"</span><span class="nt">&gt;&lt;/script&gt;</span>
<span class="nt">&lt;form&gt;</span>
<span class="w">  </span><span class="nt">&lt;input</span><span class="w"> </span><span class="na">type=</span><span class="s">"url"</span><span class="w"> </span><span class="na">value=</span><span class="s">"http://"</span><span class="w"> </span><span class="nt">/&gt;</span>
<span class="w">  </span><span class="nt">&lt;button</span><span class="w"> </span><span class="na">type=</span><span class="s">"submit"</span><span class="nt">&gt;</span>Submit<span class="nt">&lt;/button&gt;</span>
<span class="nt">&lt;/form&gt;</span>
</pre></div>

<p>I haven't tested this in any other browsers.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/how-to-use-chrome-s-network-debugger-with-redirects/" class="u-url">How to use Chrome's network debugger with redirects</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/how-to-use-chrome-s-network-debugger-with-redirects/" rel="bookmark">
            <time class="published dt-published" datetime="2023-03-03T09:11:08+08:00" itemprop="datePublished" title="2023-03-03 09:11">2023-03-03 09:11</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>The Chrome network debugger gives me a great view of all the HTTP resources
loaded for a page. But it clears the list whenever a new top-level HTML page
is loaded. This makes it very difficult to debug pages that automatically
reload for one reason or another (running script or 300 responses).</p>
<p>Can I tell Chrome not to clear the network debugger when a new top-level page
is loaded? Or can I go back and look at the previous page's network resources?</p>
<p>Or can I somehow force Chrome to pause before loading a new page when I don't
control the page I'm trying to debug that's doing the redirecting? It's part
of an OpenID dance that's going awry, so the combination of SSL and
credentials makes it extremely difficult to debug with command-line tools.</p>
<p><br><br></p>
<h2>Answer</h2>
<p><em>This has been changed since v32, thanks to @Daniel Alexiuc &amp; @Thanatos for
their comments.</em></p>
<hr>
<p><strong>Current</strong> (≥ v32)</p>
<p>At the top of the "Network" tab of DevTools, there's a checkbox to switch on
the "Preserve log" functionality. If it is checked, the network log is
preserved on page load.</p>
<p><img alt="Chrome v33 DevTools Network Tab: Preserve Log" src="images/cFuNP.png"></p>
<p>The little red dot on the left now has the purpose to switch network logging
on and off completely.</p>
<hr>
<p><strong>Older versions</strong></p>
<p>In older versions of Chrome (v21 here), there's a little, clickable red dot in
the footer of the "Network" tab.</p>
<p><img alt="Chrome v22 DevTools Network Tab: Preserve Log Upon
Navigation" src="images/tSUP0.png"></p>
<p>If you hover over it, it will tell you, that it will "Preserve Log Upon
Navigation" when it is activated. It holds the promise.</p>
<p><br></p>
<h3>Suggest</h3>
<p>I don't know of a way to force Chrome to not clear the Network debugger, but
this might accomplish what you're looking for:</p>
<ol>
<li>Open the js console</li>
<li><code>window.addEventListener("beforeunload", function() { debugger; }, false)</code></li>
</ol>
<p>This will pause chrome before loading the new page by hitting a breakpoint.</p>
    </div>
    </article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="index-1781.html" rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-1779.html" rel="next">Older posts</a>
            </li>
        </ul></nav>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow中文网</a>  
            
        </footer>
</div>
</div>


            <script src="assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="assets/js/search.js"></script>
</body>
</html>
