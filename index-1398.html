<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="This is a snapshot site for StackOverflow">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>StackOverflow Snapshot (old posts, page 1398) | StackOverflow Snapshot</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/index-1398.html">
<link rel="prev" href="index-1399.html" type="text/html">
<link rel="next" href="index-1397.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="archive.html">Archive</a>
                </li>
<li>
<a href="categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            

    


    
<div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/valueerror-non-broadcastable-output-operand-with-shape-31-doesn-t-match-the-broadcast-shape-34/" class="u-url">ValueError: non-broadcastable output operand with shape (3,1) doesn't match the broadcast shape (3,4)</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/valueerror-non-broadcastable-output-operand-with-shape-31-doesn-t-match-the-broadcast-shape-34/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:10:43+08:00" itemprop="datePublished" title="2023-02-28 03:10">2023-02-28 03:10</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>I recently started to follow along with Siraj Raval's Deep Learning tutorials
on YouTube, but I an error came up when I tried to run my code. The code is
from the second episode of his series, How To Make A Neural Network. When I
ran the code I got the error:</p>
<div class="code"><pre class="code literal-block">Traceback (most recent call last):
File "C:\Users\dpopp\Documents\Machine Learning\first_neural_net.py", line 66, in &lt;module&gt;
neural_network.train(training_set_inputs, training_set_outputs, 10000)
File "C:\Users\dpopp\Documents\Machine Learning\first_neural_net.py", line 44, in train
self.synaptic_weights += adjustment
ValueError: non-broadcastable output operand with shape (3,1) doesn't match the broadcast shape (3,4)
</pre></div>

<p>I checked multiple times with his code and couldn't find any differences, and
even tried copying and pasting his code from the GitHub link. This is the code
I have now:</p>
<div class="code"><pre class="code literal-block"><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="kp">exp</span><span class="p">,</span> <span class="kp">array</span><span class="p">,</span> <span class="n">random</span><span class="p">,</span> <span class="kp">dot</span>

<span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">():</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Seed the random number generator, so it generates the same numbers</span>
        <span class="c1"># every time the program runs.</span>
        <span class="n">random</span><span class="o">.</span><span class="kp">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># We model a single neuron, with 3 input connections and 1 output connection.</span>
        <span class="c1"># We assign random weights to a 3 x 1 matrix, with values in the range -1 to 1</span>
        <span class="c1"># and mean 0.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">synaptic_weights</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="c1"># The Sigmoid function, which describes an S shaped curve.</span>
    <span class="c1"># We pass the weighted sum of the inputs through this function to</span>
    <span class="c1"># normalise them between 0 and 1.</span>
    <span class="k">def</span> <span class="nf">__sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="kp">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

    <span class="c1"># The derivative of the Sigmoid function.</span>
    <span class="c1"># This is the gradient of the Sigmoid curve.</span>
    <span class="c1"># It indicates how confident we are about the existing weight.</span>
    <span class="k">def</span> <span class="nf">__sigmoid_derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>

    <span class="c1"># We train the neural network through a process of trial and error.</span>
    <span class="c1"># Adjusting the synaptic weights each time.</span>
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_set_inputs</span><span class="p">,</span> <span class="n">training_set_outputs</span><span class="p">,</span> <span class="n">number_of_training_iterations</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">number_of_training_iterations</span><span class="p">):</span>
            <span class="c1"># Pass the training set through our neural network (a single neuron).</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">think</span><span class="p">(</span><span class="n">training_set_inputs</span><span class="p">)</span>

            <span class="c1"># Calculate the error (The difference between the desired output</span>
            <span class="c1"># and the predicted output).</span>
            <span class="n">error</span> <span class="o">=</span> <span class="n">training_set_outputs</span> <span class="o">-</span> <span class="n">output</span>

            <span class="c1"># Multiply the error by the input and again by the gradient of the Sigmoid curve.</span>
            <span class="c1"># This means less confident weights are adjusted more.</span>
            <span class="c1"># This means inputs, which are zero, do not cause changes to the weights.</span>
            <span class="n">adjustment</span> <span class="o">=</span> <span class="kp">dot</span><span class="p">(</span><span class="n">training_set_inputs</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">error</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">__sigmoid_derivative</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>

            <span class="c1"># Adjust the weights.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">synaptic_weights</span> <span class="o">+=</span> <span class="n">adjustment</span>

    <span class="c1"># The neural network thinks.</span>
    <span class="k">def</span> <span class="nf">think</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># Pass inputs through our neural network (our single neuron).</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__sigmoid</span><span class="p">(</span><span class="kp">dot</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">synaptic_weights</span><span class="p">))</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">'__main__'</span><span class="p">:</span>

    <span class="c1"># Initialize a single neuron neural network</span>
    <span class="n">neural_network</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"Random starting synaptic weights:"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">neural_network</span><span class="o">.</span><span class="n">synaptic_weights</span><span class="p">)</span>

    <span class="c1"># The training set. We have 4 examples, each consisting of 3 input values</span>
    <span class="c1"># and 1 output value.</span>
    <span class="n">training_set_inputs</span> <span class="o">=</span> <span class="kp">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
    <span class="n">training_set_outputs</span> <span class="o">=</span> <span class="kp">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

    <span class="c1"># Train the neural network using a training set</span>
    <span class="c1"># Do it 10,000 times and make small adjustments each time</span>
    <span class="n">neural_network</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">training_set_inputs</span><span class="p">,</span> <span class="n">training_set_outputs</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"New Synaptic weights after training:"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">neural_network</span><span class="o">.</span><span class="n">synaptic_weights</span><span class="p">)</span>

    <span class="c1"># Test the neural net with a new situation</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Considering new situation [1, 0, 0] -&gt; ?:"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">neural_network</span><span class="o">.</span><span class="n">think</span><span class="p">(</span><span class="kp">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])))</span>
</pre></div>

<p>Even after copying and pasting the same code that worked in Siraj's episode,
I'm still getting the same error.</p>
<p>I just started out look into artificial intelligence, and don't understand
what the error means. Could someone please explain what it means and how to
fix it? Thanks!</p>
<p><br><br></p>
<h2>Answer</h2>
<p>Change <code>self.synaptic_weights += adjustment</code> to</p>
<div class="code"><pre class="code literal-block">self.synaptic_weights = self.synaptic_weights + adjustment
</pre></div>

<hr>
<p><code>self.synaptic_weights</code> must have a shape of (3,1) and <code>adjustment</code> must have
a shape of (3,4). While the shapes are <em>broadcastable</em> numpy must not like
trying to assign the result with shape (3,4) to an array of shape (3,1)</p>
<div class="code"><pre class="code literal-block"><span class="nv">a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">np</span>.<span class="nv">ones</span><span class="ss">((</span><span class="mi">3</span>,<span class="mi">1</span><span class="ss">))</span>
<span class="nv">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">np</span>.<span class="k">random</span>.<span class="nv">randint</span><span class="ss">(</span><span class="mi">1</span>,<span class="mi">10</span>,<span class="w"> </span><span class="ss">(</span><span class="mi">3</span>,<span class="mi">4</span><span class="ss">))</span>

<span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="nv">a</span>
<span class="nv">array</span><span class="ss">(</span>[[<span class="mi">1</span>],
<span class="w">       </span>[<span class="mi">1</span>],
<span class="w">       </span>[<span class="mi">1</span>]]<span class="ss">)</span>
<span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="nv">b</span>
<span class="nv">array</span><span class="ss">(</span>[[<span class="mi">8</span>,<span class="w"> </span><span class="mi">2</span>,<span class="w"> </span><span class="mi">5</span>,<span class="w"> </span><span class="mi">7</span>],
<span class="w">       </span>[<span class="mi">2</span>,<span class="w"> </span><span class="mi">5</span>,<span class="w"> </span><span class="mi">4</span>,<span class="w"> </span><span class="mi">8</span>],
<span class="w">       </span>[<span class="mi">7</span>,<span class="w"> </span><span class="mi">7</span>,<span class="w"> </span><span class="mi">6</span>,<span class="w"> </span><span class="mi">6</span>]]<span class="ss">)</span>

<span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">b</span>
<span class="nv">array</span><span class="ss">(</span>[[<span class="mi">9</span>,<span class="w"> </span><span class="mi">3</span>,<span class="w"> </span><span class="mi">6</span>,<span class="w"> </span><span class="mi">8</span>],
<span class="w">       </span>[<span class="mi">3</span>,<span class="w"> </span><span class="mi">6</span>,<span class="w"> </span><span class="mi">5</span>,<span class="w"> </span><span class="mi">9</span>],
<span class="w">       </span>[<span class="mi">8</span>,<span class="w"> </span><span class="mi">8</span>,<span class="w"> </span><span class="mi">7</span>,<span class="w"> </span><span class="mi">7</span>]]<span class="ss">)</span>

<span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="nv">b</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="nv">a</span>
<span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="nv">b</span>
<span class="nv">array</span><span class="ss">(</span>[[<span class="mi">9</span>,<span class="w"> </span><span class="mi">3</span>,<span class="w"> </span><span class="mi">6</span>,<span class="w"> </span><span class="mi">8</span>],
<span class="w">       </span>[<span class="mi">3</span>,<span class="w"> </span><span class="mi">6</span>,<span class="w"> </span><span class="mi">5</span>,<span class="w"> </span><span class="mi">9</span>],
<span class="w">       </span>[<span class="mi">8</span>,<span class="w"> </span><span class="mi">8</span>,<span class="w"> </span><span class="mi">7</span>,<span class="w"> </span><span class="mi">7</span>]]<span class="ss">)</span>
<span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="nv">a</span>
<span class="nv">array</span><span class="ss">(</span>[[<span class="mi">1</span>],
<span class="w">       </span>[<span class="mi">1</span>],
<span class="w">       </span>[<span class="mi">1</span>]]<span class="ss">)</span>

<span class="o">&gt;&gt;&gt;</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="nv">b</span>
<span class="nv">Traceback</span><span class="w"> </span><span class="ss">(</span><span class="nv">most</span><span class="w"> </span><span class="nv">recent</span><span class="w"> </span><span class="k">call</span><span class="w"> </span><span class="nl">last</span><span class="ss">)</span>:
<span class="w">  </span><span class="nv">File</span><span class="w"> </span><span class="s2">"&lt;pyshell#24&gt;"</span>,<span class="w"> </span><span class="nv">line</span><span class="w"> </span><span class="mi">1</span>,<span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="o">&lt;</span><span class="nv">module</span><span class="o">&gt;</span>
<span class="w">    </span><span class="nv">a</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="nv">b</span>
<span class="nv">ValueError</span>:<span class="w"> </span><span class="nv">non</span><span class="o">-</span><span class="nv">broadcastable</span><span class="w"> </span><span class="nv">output</span><span class="w"> </span><span class="nv">operand</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">shape</span><span class="w"> </span><span class="ss">(</span><span class="mi">3</span>,<span class="mi">1</span><span class="ss">)</span><span class="w"> </span><span class="nv">doesn</span><span class="err">'t match the broadcast shape (3,4)</span>
</pre></div>

<p>The same error occurs when using numpy.add and specifying <code>a</code> as the output
array</p>
<div class="code"><pre class="code literal-block">&gt;&gt;&gt; np.add(a,b, out = a)
Traceback (most recent call last):
  File "&lt;pyshell#31&gt;", line 1, in &lt;module&gt;
    np.add(a,b, out = a)
ValueError: non-broadcastable output operand with shape (3,1) doesn't match the broadcast shape (3,4)
&gt;&gt;&gt;
</pre></div>

<p>A new <code>a</code> needs to be created</p>
<div class="code"><pre class="code literal-block">&gt;&gt;&gt; a = a + b
&gt;&gt;&gt; a
array([[10,  4,  7,  9],
       [ 4,  7,  6, 10],
       [ 9,  9,  8,  8]])
&gt;&gt;&gt;
</pre></div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/giving-a-neural-network-pain/" class="u-url">Giving a neural network "pain"</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/giving-a-neural-network-pain/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:10:23+08:00" itemprop="datePublished" title="2023-02-28 03:10">2023-02-28 03:10</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>I've programmed a non-directional neural network. So kind of like the brain,
all neurons are updated at the same time, and there are no explicit layers.</p>
<p>Now I'm wondering, how does pain work? How can I structure a neural network so
that a "pain" signal will make it want to do anything to get rid of said pain.</p>
<p><br><br></p>
<h2>Answer</h2>
<p>It doesn't really work quite like that. The network you have described is too
simple to have a concept like pain that it would try to get rid of. On a low
level it's nothing but just another input, but obviously that doesn't make the
network "dislike" it.</p>
<p>In order to gain such a signal, you could train the network to perform certain
actions when it receives this particular signal. As it becomes more refined,
this signal starts looking like a real pain signal, but it's nothing more than
a specific training of the network.</p>
<p>The pain signal in higher animals has this "do anything to get rid of it"
response because higher animals have rather advanced cognitive abilities
compared to the network you have described. Worms, on the other hand, might
respond in a very specific way to a "pain" input - twitch a certain way. It's
hard-wired that way, and to say that the worm tries to do anything to get rid
of the signal would be wrong; it's more like a motor connected to a button
that spins every time you press the button.</p>
<p>Realistic mechanisms for getting artificial neural networks to do useful
things are collectively known as "neural network training", and is a large and
complex research area. You can google for this phrase to get various ideas.</p>
<p>You should be aware, however, that neural networks are not a panacea for
solving hard problems; they don't automatically get things done through magic.
Using them effectively requires a good deal of experimentation with traning
algorithm tweaks and network parameter tweaks.</p>
<p><br></p>
<h3>Suggest</h3>
<p>I don't know much (if anything) about AI theory, except that we are still
looking for a way to give AI the model it needs to reason and think and ponder
like real humans do. (We're still looking for the key - and maybe it's
<strong>pain</strong>.)</p>
<p>Most of my adult life has been focused on computer programming and studying
and understanding the mind.</p>
<p>I am writing here because I think that PAIN might be the missing link. (Also
stackoverflow rocks right now.) I know that creating a model that actually
enables higher thinking is a large leap, but I just had this amazing aha-type
moment and had to share it. :)</p>
<p>In my studies of Buddhism, I learned of a scientist who studied leprosy cases.
The reason lepers become deformed is because they don't feel pain when they
come into contact with damaging forces. It's here that science and Buddhist
reasoning collide in a fundamental truth.</p>
<p><strong>Pain</strong> is what keeps us alive, defines our boundaries, and shapes how we
make our choices and our world-view.</p>
<p>In an AI model, the principle would be to define a series of forces perhaps,
that are constantly at play. The idea is to keep the mind alive.</p>
<p>The concept of ideas having life is something we humans also seem to play out.
When someone "kills" your idea, by proving it wrong, at first, there is a
resistance to the "death" of the idea. In fact, it takes a lot sometimes, to
force an idea to be changed. We all know stubborn people... It has been said
that the "death" of an idea, is the "death" of part of one's ego. The ego is
always trying to build itself up.</p>
<p>So you see, to give AI an ego, you must give it pain, and then it will have to
fight to build "safe" thoughts so that it may grow it's own ideas and
eventually human psychosis and "consciousness".</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/one-stage-vs-two-stage-object-detection/" class="u-url">One stage vs two stage object detection</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/one-stage-vs-two-stage-object-detection/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:10:02+08:00" itemprop="datePublished" title="2023-02-28 03:10">2023-02-28 03:10</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>I was going through YOLOv4 paper which often uses the term <strong>one &amp; two stage
object detection</strong>. I was unable to understand what's the difference between
the two types of object detectors. I am assuming</p>
<ul>
<li>One stage does both region detection + object classification using one network only</li>
<li>two stage does the above operations using 2 different networks</li>
</ul>
<p>Is this assumption correct?</p>
<p><br><br></p>
<h2>Answer</h2>
<p>Instead of "region detection + object classification", its "(1)region proposal
+ (2)classification and localization in two stage detectors.</p>
<p>(1-region proposal) is done by what is called a Region Proposal Network (RPN,
for short). RPN is used to decide “where” to look in order to reduce the
computational requirements of the overall inference process. The RPN quickly
and efficiently scans every location in order to assess whether further
processing needs to be carried out in a given region. It does that by
outputting k bounding box proposals each with 2 scores representing
probability of object or not at each location. In other words, it is used to
find up to a predefined number(~2000) of regions (bounding boxes), which may
contain objects.</p>
<p>An important problem within object detection is generating a variable-length
list of bounding boxes. The variable-length problem is solved in the RPN by
using anchors: fixed sized reference bounding boxes which are placed uniformly
throughout the original image. Instead of having to detect where objects are,
we model the problem into two parts. For every anchor, we ask:</p>
<ul>
<li>Does this anchor contain a relevant object?</li>
<li>How would we adjust this anchor to better fit the relevant object?</li>
</ul>
<p>After having a list of possible relevant objects and their locations in the
original image, it becomes a more straightforward problem to solve. Using the
features extracted by the CNN and the bounding boxes with relevant objects, we
apply Region of Interest (RoI) Pooling and extract those features which would
correspond to the relevant objects into a new tensor.</p>
<p>Next in second stage, R-CNN module uses above information to:</p>
<ul>
<li>Classify the content in the bounding box (or discard it, using “background” as a label).</li>
<li>Adjust the bounding box coordinates (so it better fits the object).</li>
</ul>
<p><br></p>
<h3>Suggest</h3>
<p><strong>One-stage detectors</strong> :</p>
<p>Object classification and bounding-box regression are done directly without
using pre-generated region proposals (candidate object bounding-boxes).</p>
<p><strong>Two-stage detectors</strong> :</p>
<ol>
<li>
<em>Generation of region proposals</em> , e.g. by selective search as in R-CNN and Fast R-CNN, or by a Region Proposal Network (RPN) as in Faster R-CNN.</li>
<li>
<em>Object classification for each region proposal</em>. Additionally other things can be done such as bounding-box regression for refining the region proposals, binary-mask prediction etc.</li>
</ol>
<p>Two-stage detectors usually reach better accuracy but are slower than one-
stage detectors.</p>
<p><img alt="enter image description here" src="images/WYQp3.png"> (image taken from "On the
Performance of One-Stage and Two-Stage Object Detectors in Autonomous Vehicles
Using Camera Data")</p>
    </div>
    </article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="index-1399.html" rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-1397.html" rel="next">Older posts</a>
            </li>
        </ul></nav>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow中文网</a>  
            
        </footer>
</div>
</div>


            <script src="assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="assets/js/search.js"></script>
</body>
</html>
