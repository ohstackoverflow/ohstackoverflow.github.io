<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="This is a snapshot site for StackOverflow">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>StackOverflow Snapshot (old posts, page 1381) | StackOverflow Snapshot</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/index-1381.html">
<link rel="prev" href="index-1382.html" type="text/html">
<link rel="next" href="index-1380.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="archive.html">Archive</a>
                </li>
<li>
<a href="categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            

    


    
<div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/algorithms-for-realtime-strategy-wargame-ai/" class="u-url">Algorithms for realtime strategy wargame AI</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/algorithms-for-realtime-strategy-wargame-ai/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T02:49:17+08:00" itemprop="datePublished" title="2023-02-28 02:49">2023-02-28 02:49</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>I'm designing a realtime strategy wargame where the AI will be responsible for
controlling a large number of units (possibly 1000+) on a large hexagonal map.</p>
<p>A unit has a number of action points which can be expended on movement,
attacking enemy units or various special actions (e.g. building new units).
For example, a tank with 5 action points could spend 3 on movement then 2 in
firing on an enemy within range. Different units have different costs for
different actions etc.</p>
<p>Some additional notes:</p>
<ul>
<li>The output of the AI is a "command" to any given unit</li>
<li>Action points are allocated at the beginning of a time period, but may be spent at any point within the time period (this is to allow for realtime multiplayer games). Hence "do nothing and save action points for later" is a potentially valid tactic (e.g. a gun turret that cannot move waiting for an enemy to come within firing range)</li>
<li>The game is updating in realtime, but the AI can get a consistent snapshot of the game state at any time (thanks to the game state being one of Clojure's persistent data structures)</li>
<li>I'm not expecting "optimal" behaviour, just something that is not obviously stupid and provides reasonable fun/challenge to play against</li>
</ul>
<p>What can you recommend in terms of specific algorithms/approaches that would
allow for the right balance between efficiency and reasonably intelligent
behaviour?</p>
<p><br><br></p>
<h2>Answer</h2>
<p>First you should aim to make your game turn based at some level for the AI
(i.e. you can somehow model it turn based even if it may not be entirely turn
based, in RTS you may be able to break discrete intervals of time into turns.)
Second, you should determine how much information the AI should work with.
That is, if the AI is allowed to cheat and know every move of its opponent
(thereby making it stronger) or if it should know less or more. Third, you
should define a cost function of a state. The idea being that a higher cost
means a worse state for the computer to be in. Fourth you need a move
generator, generating all valid states the AI can transition to from a given
state (this may be homogeneous [state-independent] or heterogeneous [state-
dependent].)</p>
<p>The thing is, the cost function will be greatly influenced by what exactly you
define the state to be. The more information you encode in the state the
better balanced your AI will be but the more difficult it will be for it to
perform, as it will have to search exponentially more for every additional
state variable you include (in an exhaustive search.)</p>
<p>If you provide a definition of a state and a cost function your problem
transforms to a general problem in AI that can be tackled with any algorithm
of your choice.</p>
<p>Here is a summary of what I think would work well:</p>
<ol>
<li>
<p>Evolutionary algorithms may work well if you put enough effort into them, but they will add a layer of complexity that will create room for bugs amongst other things that can go wrong. They will also require extreme amounts of tweaking of the fitness function etc. I don't have much experience working with these but if they are anything like neural networks (which I believe they are since both are heuristics inspired by biological models) you will quickly find they are fickle and far from consistent. Most importantly, I doubt they add any benefits over the option I describe in 3.</p>
</li>
<li>
<p>With the cost function and state defined it would technically be possible for you to apply gradient decent (with the assumption that the state function is differentiable and the domain of the state variables are continuous) however this would probably yield inferior results, since the biggest weakness of gradient descent is getting stuck in local minima. To give an example, this method would be prone to something like attacking the enemy always as soon as possible because there is a non-zero chance of annihilating them. Clearly, this may not be desirable behaviour for a game, however, gradient decent is a greedy method and doesn't know better.</p>
</li>
<li>
<p>This option would be my most highest recommended one: simulated annealing. Simulated annealing would (IMHO) have all the benefits of 1. without the added complexity while being much more robust than 2. In essence SA is just a random walk amongst the states. So in addition to the cost and states you will have to define a way to randomly transition between states. SA is also not prone to be stuck in local minima, while producing very good results quite consistently. The only tweaking required with SA would be the cooling schedule--which decides how fast SA will converge. The greatest advantage of SA I find is that it is conceptually simple and produces superior results empirically to most other methods I have tried. Information on SA can be found here with a long list of generic implementations at the bottom.</p>
</li>
</ol>
<p>3b. ( <em>Edit Added much later</em> ) SA and the techniques I listed above are
general AI techniques and not really specialized to AI for games. In general,
the more specialized the algorithm the more chance it has at performing
better. See No Free Lunch Theorem 2. Another extension of 3 is something
called parallel tempering which dramatically improves the performance of SA by
helping it avoid local optima. Some of the original papers on parallel
tempering are quite dated 3, but others have been updated4.</p>
<p>Regardless of what method you choose in the end, its going to be very
important to break your problem down into states and a cost function as I said
earlier. As a rule of thumb I would start with 20-50 state variables as your
state search space is exponential in the number of these variables.</p>
<p><br></p>
<h3>Suggest</h3>
<p>First you should aim to make your game turn based at some level for the AI
(i.e. you can somehow model it turn based even if it may not be entirely turn
based, in RTS you may be able to break discrete intervals of time into turns.)
Second, you should determine how much information the AI should work with.
That is, if the AI is allowed to cheat and know every move of its opponent
(thereby making it stronger) or if it should know less or more. Third, you
should define a cost function of a state. The idea being that a higher cost
means a worse state for the computer to be in. Fourth you need a move
generator, generating all valid states the AI can transition to from a given
state (this may be homogeneous [state-independent] or heterogeneous [state-
dependent].)</p>
<p>The thing is, the cost function will be greatly influenced by what exactly you
define the state to be. The more information you encode in the state the
better balanced your AI will be but the more difficult it will be for it to
perform, as it will have to search exponentially more for every additional
state variable you include (in an exhaustive search.)</p>
<p>If you provide a definition of a state and a cost function your problem
transforms to a general problem in AI that can be tackled with any algorithm
of your choice.</p>
<p>Here is a summary of what I think would work well:</p>
<ol>
<li>
<p>Evolutionary algorithms may work well if you put enough effort into them, but they will add a layer of complexity that will create room for bugs amongst other things that can go wrong. They will also require extreme amounts of tweaking of the fitness function etc. I don't have much experience working with these but if they are anything like neural networks (which I believe they are since both are heuristics inspired by biological models) you will quickly find they are fickle and far from consistent. Most importantly, I doubt they add any benefits over the option I describe in 3.</p>
</li>
<li>
<p>With the cost function and state defined it would technically be possible for you to apply gradient decent (with the assumption that the state function is differentiable and the domain of the state variables are continuous) however this would probably yield inferior results, since the biggest weakness of gradient descent is getting stuck in local minima. To give an example, this method would be prone to something like attacking the enemy always as soon as possible because there is a non-zero chance of annihilating them. Clearly, this may not be desirable behaviour for a game, however, gradient decent is a greedy method and doesn't know better.</p>
</li>
<li>
<p>This option would be my most highest recommended one: simulated annealing. Simulated annealing would (IMHO) have all the benefits of 1. without the added complexity while being much more robust than 2. In essence SA is just a random walk amongst the states. So in addition to the cost and states you will have to define a way to randomly transition between states. SA is also not prone to be stuck in local minima, while producing very good results quite consistently. The only tweaking required with SA would be the cooling schedule--which decides how fast SA will converge. The greatest advantage of SA I find is that it is conceptually simple and produces superior results empirically to most other methods I have tried. Information on SA can be found here with a long list of generic implementations at the bottom.</p>
</li>
</ol>
<p>3b. ( <em>Edit Added much later</em> ) SA and the techniques I listed above are
general AI techniques and not really specialized to AI for games. In general,
the more specialized the algorithm the more chance it has at performing
better. See No Free Lunch Theorem 2. Another extension of 3 is something
called parallel tempering which dramatically improves the performance of SA by
helping it avoid local optima. Some of the original papers on parallel
tempering are quite dated 3, but others have been updated4.</p>
<p>Regardless of what method you choose in the end, its going to be very
important to break your problem down into states and a cost function as I said
earlier. As a rule of thumb I would start with 20-50 state variables as your
state search space is exponential in the number of these variables.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/what-are-the-differences-between-simulated-annealing-and-genetic-algorithms/" class="u-url">What are the differences between simulated annealing and genetic algorithms?</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/what-are-the-differences-between-simulated-annealing-and-genetic-algorithms/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T02:48:52+08:00" itemprop="datePublished" title="2023-02-28 02:48">2023-02-28 02:48</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>What are the relevant differences, in terms of performance and use cases,
between simulated annealing (with bean search) and genetic algorithms?</p>
<p>I know that SA can be thought as GA where the population size is only one, but
I don't know the key difference between the two.</p>
<p>Also, I am trying to think of a situation where SA will outperform GA or GA
will outperform SA. Just one simple example which will help me understand will
be enough.</p>
<p><br><br></p>
<h2>Answer</h2>
<p>Well strictly speaking, these two things-- <strong>simulated annealing</strong> (SA) and
<strong>genetic algorithms</strong> are neither algorithms nor is their purpose 'data
mining'.</p>
<p>Both are <em>meta-heuristics</em> --a couple of levels above 'algorithm' on the
abstraction scale. In other words, both terms refer to high-level metaphors--
one borrowed from metallurgy and the other from evolutionary biology. In the
meta-heuristic taxonomy, SA is a <em>single-state method</em> and GA is a <em>population
method</em> (in a sub-class along with PSO, ACO, et al, usually referred to as
<em>biologically-inspired meta-heuristics</em> ).</p>
<p>These two meta-heuristics are used to solve optimization problems,
particularly (though not exclusively) in <em>combinatorial optimization</em> (aka
<em>constraint-satisfaction programming</em> ). Combinatorial optimization refers to
optimization by selecting from among a set of discrete items--in other words,
there is no continuous function to minimize. The knapsack problem, traveling
salesman problem, cutting stock problem--are all combinatorial optimization
problems.</p>
<p>The connection to data mining is that the core of many (most?) supervised
Machine Learning (ML) algorithms is the solution of an optimization
problem--(Multi-Layer Perceptron and Support Vector Machines for instance).</p>
<p>Any solution technique to solve cap problems, regardless of the algorithm,
will consist essentially of these steps (which are typically coded as a single
block within a recursive loop):</p>
<ol>
<li>
<p>encode the domain-specific details in a cost function (it's the step-wise minimization of the value returned from this function that constitutes a 'solution' to the c/o problem);</p>
</li>
<li>
<p>evaluate the cost function passing in an initial 'guess' (to begin iteration);</p>
</li>
<li>
<p>based on the value returned from the cost function, generate a subsequent candidate solution (or more than one, depending on the meta-heuristic) to the cost function;</p>
</li>
<li>
<p>evaluate each candidate solution by passing it in an argument set, to the cost function;</p>
</li>
<li>
<p>repeat steps (iii) and (iv) until either some convergence criterion is satisfied or a maximum number of iterations is reached.</p>
</li>
</ol>
<p>Meta-heuristics are directed to step (iii) above; hence, SA and GA differ in
how they generate candidate solutions for evaluation by the cost function. In
other words, that's the place to look to understand how these two meta-
heuristics differ.</p>
<p>Informally, the essence of an algorithm directed to solution of combinatorial
optimization is how it handles a candidate solution whose value returned from
the cost function is <strong><em>worse</em></strong> than the current best candidate solution (the
one that returns the lowest value from the cost function). The simplest way
for an optimization algorithm to handle such a candidate solution is to reject
it outright--that's what the hill climbing algorithm does. But by doing this,
simple hill climbing will always miss a better solution separated from the
current solution by a hill. Put another way, a sophisticated optimization
algorithm has to include a technique for (temporarily) accepting a candidate
solution worse than (i.e., uphill from) the current best solution because an
even better solution than the current one might lie along a path through that
worse solution.</p>
<hr>
<p>So how do SA and GA generate candidate solutions?</p>
<p>The essence of SA is usually expressed in terms of the probability that a
higher-cost candidate solution will be accepted (the entire expression inside
the double parenthesis is an exponent:</p>
<div class="code"><pre class="code literal-block">p = e((-highCost - lowCost)/temperature)
</pre></div>

<p>Or in python:</p>
<div class="code"><pre class="code literal-block">p = pow(math.e, (-hiCost - loCost) / T)
</pre></div>

<p>The 'temperature' term is a variable whose value decays during progress of the
optimization--and therefore, the probability that SA will accept a worse
solution decreases as iteration number increases.</p>
<p>Put another way, when the algorithm begins iterating, T is very large, which
as you can see, causes the algorithm to move to every newly created candidate
solution, whether better or worse than the current best solution--i.e., it is
doing a <em>random walk</em> in the solution space. As iteration number increases
(i.e., as the temperature cools) the algorithm's search of the solution space
becomes less permissive, until at T = 0, the behavior is identical to a simple
hill-climbing algorithm (i.e., only solutions better than the current best
solution are accepted).</p>
<p><strong><em>Genetic Algorithms</em></strong> are very different. For one thing--and this is a big
thing--it generates not a single candidate solution but an entire 'population
of them'. It works like this: GA calls the cost function on each member
(candidate solution) of the population. It then ranks them, from best to
worse, ordered by the value returned from the cost function ('best' has the
lowest value). From these ranked values (and their corresponding candidate
solutions) the next population is created. New members of the population are
created in essentially one of three ways. The first is usually referred to as
'elitism' and in practice usually refers to just taking the highest ranked
candidate solutions and passing them straight through--unmodified--to the next
generation. The other two ways that new members of the population are usually
referred to as 'mutation' and 'crossover'. Mutation usually involves a change
in one element in a candidate solution vector from the current population to
create a solution vector in the new population, e.g., [4, 5, 1, 0, 2] =&gt; [4,
5, 2, 0, 2]. The result of the crossover operation is like what would happen
if vectors could have sex--i.e., a new child vector whose elements are
comprised of some from each of two parents.</p>
<p>So those are the algorithmic differences between GA and SA. What about the
differences in performance?</p>
<p>In practice: (my observations are limited to combinatorial optimization
problems) GA nearly always beats SA (returns a lower 'best' return value from
the cost function--ie, a value close to the solution space's global minimum),
but at a higher computation cost. As far as i am aware, the textbooks and
technical publications recite the same conclusion on resolution.</p>
<p>but here's the thing: GA is inherently parallelizable; what's more, it's
trivial to do so because the individual "search agents" comprising each
population do not need to exchange messages--ie, they work independently of
each other. Obviously that means <em>GA computation can be distributed</em> , which
means <strong>in practice, you can get much better results (closer to the global
minimum) and better performance (execution speed).</strong></p>
<p>In what circumstances might SA outperform GA? The general scenario i think
would be those optimization problems having a small solution space so that the
result from SA and GA are practically the same, yet the execution context
(e.g., hundreds of similar problems run in batch mode) favors the faster
algorithm (which should always be SA).</p>
<p><br></p>
<h3>Suggest</h3>
<p>It is really difficult to compare the two since they were inspired from
different domains..</p>
<p>A Genetic Algorithm maintains a population of possible solutions, and at each
step, selects pairs of possible solution, combines them (crossover), and
applies some random changes (mutation). The algorithm is based the idea of
"survival of the fittest" where the selection process is done according to a
fitness criteria (usually in optimization problems it is simply the value of
the objective function evaluated using the current solution). The crossover is
done in hope that two good solutions, when combined, might give even better
solution.</p>
<p>On the other hand, Simulated Annealing only tracks one solution in the space
of possible solutions, and at each iteration considers whether to move to a
neighboring solution or stay in the current one according to some
probabilities (which decays over time). This is different from a heuristic
search (say greedy search) in that it doesn't suffer from the problems of
local optimum since it can get unstuck from cases where all neighboring
solutions are worst the current one.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/manhattan-distance-is-over-estimating-and-making-me-crazy/" class="u-url">Manhattan distance is over estimating and making me crazy</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/manhattan-distance-is-over-estimating-and-making-me-crazy/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T02:48:35+08:00" itemprop="datePublished" title="2023-02-28 02:48">2023-02-28 02:48</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>I'm implementing <strong>a-star algorithm</strong> with <strong>Manhattan distance</strong> to solve the
<strong>8-puzzle</strong> (in C). It seems to work very well and passes a lot of unit tests
but it fails to find the shortest path in one case (it finds 27 steps instead
of 25).</p>
<p>When I change the heuristic function to Hamming distance it finds in 25 steps.
Also finds in 25 steps when I make the Manhattan distance function to return a
half of the actual cost.</p>
<p>That's why I believe the problem lies somewhere in Manhattan distance function
and it is over estimating the cost (hence inadmissible). I thought maybe
something else is going wrong in the C program so I wrote a little Python
script to test and verify the output of the Manhattan distance function only
and they both produce the exact same result.</p>
<p>I'm really confused because the heuristic function seems to be the only point
of failure and it seems to be correct at the same time.</p>
<p><img alt="8-puzzle start goal" src="images/R8fXD.gif"></p>
<p>You can try <strong>this solver</strong> and put the tile order like "2,6,1,0,7,8,3,5,4"
Choose the algorithm <em>Manhattan distance</em> and it finds in 25 steps. Now change
it to <em>Manhattan distance + linear conflict</em> and it finds 27 steps.</p>
<p>But my Manhattan distance (without linear conflict) finds in 27 steps.</p>
<p>Here's my general algorithm:</p>
<div class="code"><pre class="code literal-block"><span class="nv">manhattan_distance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>
<span class="nv">iterate</span><span class="w"> </span><span class="nv">over</span><span class="w"> </span><span class="nv">all</span><span class="w"> </span><span class="nv">tiles</span>
<span class="k">if</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">tile</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">not</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">blank</span><span class="w"> </span><span class="nv">tile</span>:
<span class="nv">find</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">coordinates</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">this</span><span class="w"> </span><span class="nv">tile</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">goal</span><span class="w"> </span><span class="nv">board</span>
<span class="nv">manhattan_distance</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="nv">abs</span><span class="ss">(</span><span class="nv">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">goal_x</span><span class="ss">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">abs</span><span class="ss">(</span><span class="nv">y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">goal_y</span><span class="ss">)</span>
</pre></div>

<p>I think if there was something very badly wrong with some important part it
wouldn't pass all 25+ previous tests so this might be some sort of edge case.</p>
<p>Here's commented Manhattan distance function in C:</p>
<div class="code"><pre class="code literal-block"><span class="nv">int</span><span class="w"> </span><span class="nv">ManhattanDistance</span><span class="ss">(</span><span class="nv">Puzzle</span><span class="w"> </span><span class="nv">p</span>,<span class="w"> </span><span class="nv">State</span><span class="w"> </span><span class="nv">b</span><span class="ss">)</span>{
<span class="w">   </span><span class="nv">State</span><span class="w"> </span><span class="nv">goal</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">getFinalState</span><span class="ss">(</span><span class="nv">p</span><span class="ss">)</span><span class="c1">;</span>
<span class="w">   </span><span class="nv">int</span><span class="w"> </span><span class="nv">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">getSize</span><span class="ss">(</span><span class="nv">b</span><span class="ss">)</span><span class="c1">;</span>
<span class="w">   </span><span class="nv">int</span><span class="w"> </span><span class="nv">distance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="c1">;</span>
<span class="w">   </span><span class="k">if</span><span class="w"> </span><span class="ss">(</span><span class="nv">getSize</span><span class="ss">(</span><span class="nv">goal</span><span class="ss">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nv">size</span><span class="ss">)</span>{<span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="nv">both</span><span class="w"> </span><span class="nv">states</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">same</span><span class="w"> </span><span class="nv">size</span>
<span class="w">      </span><span class="nv">int</span><span class="w"> </span><span class="nv">i</span>,<span class="w"> </span><span class="nv">j</span><span class="c1">;</span>
<span class="w">      </span><span class="k">for</span><span class="ss">(</span><span class="nv">i</span><span class="o">=</span><span class="mi">0</span><span class="c1">; i&lt;size; i++){</span>
<span class="w">         </span><span class="k">for</span><span class="ss">(</span><span class="nv">j</span><span class="o">=</span><span class="mi">0</span><span class="c1">; j&lt;size; j++){ // iterate over all tiles</span>
<span class="w">            </span><span class="nv">int</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">getStateValue</span><span class="ss">(</span><span class="nv">b</span>,<span class="w"> </span><span class="nv">i</span>,<span class="w"> </span><span class="nv">j</span><span class="ss">)</span><span class="c1">; // what is the number on this tile?</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="ss">(</span><span class="nv">a</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="s1">'B'</span><span class="ss">)</span>{<span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nv">it</span><span class="err">'s not the blank tile</span>
<span class="err">               int final_cordinates[2];</span>
<span class="err">               getTileCoords(goal, a, final_cordinates); // find the coordinates on the other board</span>
<span class="err">               int final_i = final_cordinates[0];</span>
<span class="err">               int final_j = final_cordinates[1];</span>
<span class="err">               distance +=  abs(i - final_i) + abs(j - final_j);</span>
<span class="err">            }</span>
<span class="err">         }</span>
<span class="err">      }</span>
<span class="err">   }</span>
<span class="err">   return distance;</span>
<span class="err">}</span>
</pre></div>

<p>Please help me.</p>
<p><strong>EDIT:</strong> As discussed in comments, the code provided for opening nodes can be
found here</p>
<p><br><br></p>
<h2>Answer</h2>
<p>The problem seems to be not in your heuristic function, but in the algorithm
itself. From your description of the problem, and the fact that it occures
only on some specific cases, I believe it has to do with the re-opening of a
closed vertice, once you find a better path to it.</p>
<p>While reading the code you have provided [in comments], I think I understood
where the problem lays, in line 20:</p>
<div class="code"><pre class="code literal-block"><span class="k">if</span><span class="p">(</span><span class="n">getG</span><span class="p">(</span><span class="k">current</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">getG</span><span class="p">(</span><span class="n">children</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">))</span><span class="err">{</span>
</pre></div>

<p>This is wrong! You are checking if <code>g(current) + 1 &lt; g(children[i])</code>, you
actually want to check for: <code>f(current) + 1 + h(children[i]) &lt;
g(children[i])</code>, since you want to check this value with the heuristic
function of <code>children[i]</code>, and not of <code>current</code>!<br>
Note that it is identical as to set <code>f(children[i]) =
min{f(children[i]),f(current)+1}</code>, and then adding <code>h(children[i])</code> to get the
<code>g</code> value.</p>
    </div>
    </article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="index-1382.html" rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-1380.html" rel="next">Older posts</a>
            </li>
        </ul></nav>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow-ZH</a>  
            
        </footer>
</div>
</div>


            <script src="assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="assets/js/search.js"></script>
</body>
</html>
