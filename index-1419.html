<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="This is a snapshot site for StackOverflow">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>StackOverflow Snapshot (old posts, page 1419) | StackOverflow Snapshot</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/index-1419.html">
<link rel="prev" href="index-1420.html" type="text/html">
<link rel="next" href="index-1418.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="archive.html">Archive</a>
                </li>
<li>
<a href="categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            

    


    
<div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/structured-factored-and-atomic-representation/" class="u-url">Structured, factored and atomic representation?</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/structured-factored-and-atomic-representation/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:30:02+08:00" itemprop="datePublished" title="2023-02-28 03:30">2023-02-28 03:30</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>I am currently reading "Artificial Intelligence: A modern Approach". Though
the terminology factored, structured and atomic representation is confusing
what do these mean exactly?</p>
<p>In relation with programming...</p>
<p>Thanks</p>
<p><br><br></p>
<h2>Answer</h2>
<p>I'm not thrilled with the lines that Russell and Norvig draw, but: Generally,
when you're using AI techniques to solve a problem, you're going to have a
programmed model of the situation. Atomic/factored/structured is a qualitative
measure of how much "internal structure" those models have, from least to
most.</p>
<p>Atomic models have no internal structure; the state either does or does not
match what you're looking for. In a sliding tile puzzle, for instance, you
either have the correct alignment of tiles or you do not.</p>
<p>Factored models have more internal structure, although exactly what will
depend on the problem. Typically, you're looking at variables or performance
metrics of interest; in a sliding puzzle, this might be a simple heuristic
like "number of tiles out of place," or "sum of manhatten distances."</p>
<p>Structured models have still more; again, exactly what depends on the problem,
but they're often relations either of components of the model to itself, or
components of the model to components of the environment.</p>
<p>It is very easy, especially when looking at very simple problems like the
sliding tile, to unconsciously do all the hard intelligence work yourself, at
a glance, and forget that your model doesn't have all your insight. For
example, if you were to make a program to do a graph search technique on the
sliding puzzle, you'd probably make some engine that took as input a puzzle
state and an action, and generated a new puzzle state from that. The puzzle
states are still atomic, but <em>you the programmer</em> are using a much more
detailed model to link those inputs and outputs together.</p>
<p><br></p>
<h3>Suggest</h3>
<p>I like explanation given by Novak. My 2 cents is to make clarity on difference
between <strong>factored</strong> vs <strong>structured</strong>. Here is extract from definitions:</p>
<ul>
<li>An atomic representation is one in which each state is treated as a black box. </li>
<li>A factored representation is one in which the states are defined by set of features.</li>
<li>A structured representation is one in which the states are expressed in form of objects and relations between them. Such knowledge about relations called facts.</li>
</ul>
<p>Examples:</p>
<div class="code"><pre class="code literal-block">atomicState == goal: Y/N  // Is goal reached?
</pre></div>

<p>It is the only question we can ask to black box.</p>
<div class="code"><pre class="code literal-block">factoredState{18} == goal{42}: N  // Is goal reached?
diff( goal{42}, factoredState{18}) = 24 // How much is difference?
// some other questions. the more features =&gt; more type of questions
</pre></div>

<p>The simplest factored state must have at least one feature (of some type),
that gives us ability to ask more questions. Normally it defines quantitative
difference between states. The example has one feature of integer type.</p>
<div class="code"><pre class="code literal-block"><span class="mf">11</span><span class="n">grade</span><span class="err">@</span><span class="n">schoolA</span><span class="err">{</span><span class="n">John</span><span class="p">(</span><span class="n">Math</span><span class="o">=</span><span class="n">A</span><span class="o">-</span><span class="p">),</span><span class="w"> </span><span class="n">Marry</span><span class="p">(</span><span class="n">Music</span><span class="o">=</span><span class="n">A</span><span class="o">+</span><span class="p">),</span><span class="w"> </span><span class="n">Job1</span><span class="p">(</span><span class="n">doMath</span><span class="p">)</span><span class="mf">..</span><span class="err">}</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="kr">go</span><span class="n">al</span><span class="err">{</span><span class="mf">50</span><span class="err">%</span><span class="w"> </span><span class="kr">read</span><span class="n">y</span><span class="w"> </span><span class="kr">for</span><span class="w"> </span><span class="n">jobs</span><span class="err">}</span>
</pre></div>

<p>The key here - structured representation, allows higher level of formal
logical reasoning at the search. See First-Order Logic @berkley for
introductory info.</p>
<p>This subject easily confuses a practitioner (especially beginner) but makes
great sense for comparing different goal searching algorithms. Such "world"
state representation classification logically separates algorithms into
different classes. It is very useful to draw lines in academic research and
compare apples to apples when reasoning academically.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/what-are-the-uses-of-recurrent-neural-networks-when-using-them-with-reinforcement-learning/" class="u-url">What are the uses of recurrent neural networks when using them with Reinforcement Learning?</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/what-are-the-uses-of-recurrent-neural-networks-when-using-them-with-reinforcement-learning/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:29:34+08:00" itemprop="datePublished" title="2023-02-28 03:29">2023-02-28 03:29</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>I do know that feedforward multi-layer neural networks with backprop are used
with Reinforcement Learning as to help it generalize the actions our agent
does. This is, if we have a big state space, we can do some actions, and they
will help generalize over the whole state space.</p>
<p>What do recurrent neural networks do, instead? To what tasks are they used
for, in general?</p>
<p><br><br></p>
<h2>Answer</h2>
<p>Recurrent Neural Networks, RNN for short (although beware that <em>RNN</em> is often
used in the literature to designate <em>Random Neural Networks</em> , which
effectively are a special case of Recurrent NN), come in very different
"flavors" which causes them to exhibit various behaviors and characteristics.
In general, however these many shades of behaviors and characteristics are
<strong>rooted in the availability of [feedback] input to individual neurons</strong>. Such
feedback comes from other parts of the network, be it local or distant, from
the same layer (including in some cases "self"), or even on different layers
(*). Feedback information it treated as "normal" input the neuron and can then
influence, at least in part, its output.</p>
<p>Unlike <strong>back propagation</strong> which is used <em>during the learning phase</em> of a
Feed-forward Network for the purpose of fine-tuning the relative weights of
the various [Feedfoward-only] connections, FeedBack in RNNs constitute true a
input to the neurons they connect to.</p>
<p>One of the uses of feedback is <strong>to make the network more resilient to noise
and other imperfections in the input</strong> (i.e. <em>input</em> to the network as a
whole). The reason for this is that in addition to inputs "directly"
pertaining to the network input (the types of input that would have been
present in a Feedforward Network), neurons have the information about what
other neurons are "thinking". This extra info then leads to <strong>Hebbian
learning</strong> , i.e. the idea that neurons that [usually] fire together should
"encourage" each other to fire. In practical terms this extra input from
"like-firing" neighbor neurons (or no-so neighbors) may prompt a neuron to
fire even though its non-feedback inputs may have been such that it would have
not fired (or fired less strongly, depending on type of network).</p>
<p>An example of this resilience to input imperfections is with <strong>associative
memory</strong> , a common employ of RNNs. The idea is to use the feeback info to
"fill-in the blanks".</p>
<p>Another related but distinct use of feedback is with <strong>inhibitory signals</strong> ,
whereby a given neuron may learn that while all its other inputs would prompt
it to fire, a particular feedback input from some other part of the network
typically indicative that somehow the other inputs are not to be trusted (in
this particular context).</p>
<p>Another extremely important use of feedback, is that in some architectures it
can <strong>introduce a temporal element to the system</strong>. A particular [feedback]
input may not so much instruct the neuron of what it "thinks" [now], but
instead "remind" the neuron that say, two cycles ago (whatever cycles may
represent), the network's state (or one of its a sub-states) was "X". Such
ability to "remember" the [typically] recent past is another factor of
resilience to noise in the input, but its main interest may be in introducing
"prediction" into the learning process. These time-delayed input may be seen
as predictions from other parts of the network: "I've heard footsteps in the
hallway, expect to hear the door bell [or keys shuffling]".</p>
<p>(<em>) BTW such a broad freedom in the "rules" that dictate the allowed
connections, whether feedback or feed-forward, explains </em><em>why there are so
many different RNN architectures</em>* and variations thereof). Another reason for
these many different architectures is that one of the characteristics of RNN
is that they are not readily as tractable, mathematically or otherwise,
compared with the feed-forward model. As a result, driven by mathematical
insight or plain trial-and-error approach, many different possibilities are
being tried.</p>
<p>This is not to say that feedback network are total black boxes, in fact some
of the RNNs such as the <strong>Hopfield Networks</strong> are rather well understood. It's
just that the math is typically more complicated (at least to me ;-) )</p>
<p>I think the above, generally (too generally!), addresses <em>devoured elysium</em> 's
(the OP) questions of " <em>what do RNN do instead</em> ", and the " <em>general tasks
they are used for</em> ". To many complement this information, here's an
incomplete and informal survey of applications of RNNs. The difficulties in
gathering such a list are multiple:</p>
<ul>
<li>the overlap of applications between Feed-forward Networks and RNNs (as a result this hides the specificity of RNNs)</li>
<li>the often highly specialized nature of applications (we either stay in with too borad concepts such as "classification" or we dive into "Prediction of Carbon shifts in series of saturated benzenes" ;-) )</li>
<li>the hype often associated with neural networks, when described in vulgarization texts</li>
</ul>
<p>Anyway, here's the list</p>
<ul>
<li>modeling, in particular the learning of [oft' non-linear] dynamic systems</li>
<li>Classification (now, FF Net are also used for that...)</li>
<li>Combinatorial optimization</li>
</ul>
<p>Also there are a lots of applications associated with the temporal dimension
of the RNNs (another area where FF networks would typically not be found)</p>
<ul>
<li>Motion detection </li>
<li>load forecasting (as with utilities or services: predicting the load in the short term)</li>
<li>signal processing : filtering and control</li>
</ul>
<p><br></p>
<h3>Suggest</h3>
<p>There is an assumption in the basic Reinforcement Learning framework that your
state/action/reward sequence is a Markov Decision Process. That basically
means that you do not need to remember any information about previous states
from this episode to make decisions.</p>
<p>But this is obviously not true for all problems. Sometimes you do need to
remember some recent things to make informed decisions. Sometimes you can
explicitly build the things that need to be remembered into the state signal,
but in general we'd like our system to learn what it needs to remember. This
is called a Partially Observable Markov Decision Process (POMDP), and there
are a variety of methods used to deal with it. One possibly solution is to use
a recurrent neural network, since they incorporate details from previous time
steps into the current decision.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/how-does-a-back-propagation-training-algorithm-work/" class="u-url">How does a back-propagation training algorithm work?</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/how-does-a-back-propagation-training-algorithm-work/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:29:02+08:00" itemprop="datePublished" title="2023-02-28 03:29">2023-02-28 03:29</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>I've been trying to learn how back-propagation works with neural networks, but
yet to find a good explanation from a less technical aspect.</p>
<p>How does back-propagation work? How does it learn from a training dataset
provided? I will have to code this, but until then I need to gain a stronger
understanding of it.</p>
<p><br><br></p>
<h2>Answer</h2>
<p>Back-propagation works in a logic very similar to that of <em>feed-forward</em>. The
difference is the direction of data flow. In the feed-forward step, you have
the inputs and the output observed from it. You can propagate the values
forward to <em>train</em> the neurons ahead.</p>
<p>In the back-propagation step, you cannot know the errors occurred in every
neuron but the ones in the output layer. Calculating the errors of output
nodes is straightforward - you can take the difference between the output from
the neuron and the <em>actual output</em> for that instance in training set. The
neurons in the hidden layers must fix their errors from this. Thus you have to
pass the error values back to them. From these values, the hidden neurons can
update their weights and other parameters using the weighted sum of errors
from the layer ahead.</p>
<p>A step-by-step demo of feed-forward and back-propagation steps can be found
here.</p>
<hr>
<h4>Edit</h4>
<p>If you're a beginner to neural networks, you can begin learning from
<strong>Perceptron</strong> , then advance to NN, which actually is a multilayer
perceptron.</p>
<p><br></p>
<h3>Suggest</h3>
<h4>High-level description of the backpropagation algorithm</h4>
<p>Backpropagation is trying to do a <em>gradient descent</em> on the <em>error surface</em> of
the neural network, adjusting the weights with <em>dynamic programming</em>
techniques to keep the computations tractable.</p>
<p>I will try to explain, in high-level terms, all the just mentioned concepts.</p>
<h4>Error surface</h4>
<p>If you have a neural network with, say, N neurons in the output layer, that
means your output is really an N-dimensional vector, and that vector lives in
an N-dimensional space (or on an N-dimensional surface.) So does the "correct"
output that you're training against. So does the <strong>difference</strong> between your
"correct" answer and the actual output.</p>
<p>That difference, with suitable conditioning (especially some consideration of
absolute values) is the <strong>error vector</strong> , living on the error surface.</p>
<h4>Gradient descent</h4>
<p>With that concept, you can think of training the neural network as the process
of adjusting the weights of your neurons so that the error function is small,
ideally zero. Conceptually, you do this with calculus. If you only had one
output and one weight, this would be simple -- take a few derivatives, which
would tell you which "direction" to move, and make an adjustment in that
direction.</p>
<p>But you don't have one neuron, you have N of them, and substantially more
input weights.</p>
<p>The principle is the same, except instead of using calculus on lines looking
for slopes that you can picture in your head, the equations become vector
algebra expressions that you can't easily picture. The term <strong>gradient</strong> is
the multi-dimensional analogue to <em>slope</em> on a line, and <strong>descent</strong> means you
want to move <em>down</em> that error surface until the errors are small.</p>
<h4>Dynamic programming</h4>
<p>There's another problem, though -- if you have more than one layer, you can't
easily see the change of the weights in some non-output layer vs the actual
output.</p>
<p>Dynamic programming is a bookkeeping method to help track what's going on. At
the very highest level, if you naively try to do all this vector calculus, you
end up calculating some derivatives over and over again. The modern
backpropagation algorithm avoids some of that, and it so happens that you
update the output layer first, then the second to last layer, etc. Updates are
<strong>propagating backwards</strong> from the output, hence the name.</p>
<p>So, if you're lucky enough to have been exposed to gradient descent or vector
calculus before, then hopefully that clicked.</p>
<p>The full derivation of backpropagation can be condensed into about a page of
tight symbolic math, but it's hard to get the sense of the algorithm without a
high-level description. (It's downright intimidating, in my opinion.) If you
haven't got a good handle on vector calculus, then, sorry, the above probably
wasn't helpful. But to get backpropagation to actually work, it's not
necessary to understand the full derivation.</p>
<hr>
<p>I found the following paper (by Rojas) very helpul, when I was trying to
understand this material, even if it's a big PDF of one chapter of his book.</p>
<p>http://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf</p>
    </div>
    </article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="index-1420.html" rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-1418.html" rel="next">Older posts</a>
            </li>
        </ul></nav>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow-ZH</a>  
            
        </footer>
</div>
</div>


            <script src="assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="assets/js/search.js"></script>
</body>
</html>
