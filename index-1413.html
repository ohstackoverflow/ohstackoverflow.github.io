<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="This is a snapshot site for StackOverflow">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>StackOverflow Snapshot (old posts, page 1413) | StackOverflow Snapshot</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/index-1413.html">
<link rel="prev" href="index-1414.html" type="text/html">
<link rel="next" href="index-1412.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="archive.html">Archive</a>
                </li>
<li>
<a href="categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            

    


    
<div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/heuristic-to-identify-if-a-series-of-4-bytes-chunks-of-data-are-integers-or-floats/" class="u-url">Heuristic to identify if a series of 4 bytes chunks of data are integers or floats</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/heuristic-to-identify-if-a-series-of-4-bytes-chunks-of-data-are-integers-or-floats/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:26:31+08:00" itemprop="datePublished" title="2023-02-28 03:26">2023-02-28 03:26</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>What's the best heuristic I can use to identify whether a chunk of X 4-bytes
are integers or floats? A human can do this easily, but I wanted to do it
programmatically.</p>
<p>I realize that since every combination of bits will result in a valid integer
and (almost?) all of them will also result in a valid float, there is no way
to know for sure. But I still would like to identify the most likely candidate
(which will virtually always be correct; or at least, a human can do it).</p>
<p>For example, let's take a series of 4-bytes raw data and print them as
integers first and then as floats:</p>
<div class="code"><pre class="code literal-block"><span class="mf">1</span><span class="w">           </span><span class="mf">1.4013e-45</span>
<span class="mf">10</span><span class="w">          </span><span class="mf">1.4013e-44</span>
<span class="mf">44</span><span class="w">          </span><span class="mf">6.16571e-44</span>
<span class="mf">5000</span><span class="w">        </span><span class="mf">7.00649e-42</span>
<span class="mf">1024</span><span class="w">        </span><span class="mf">1.43493e-42</span>
<span class="mf">0</span><span class="w">           </span><span class="mf">0</span>
<span class="mf">0</span><span class="w">           </span><span class="mf">0</span>
<span class="o">-</span><span class="mf">5</span><span class="w">          </span><span class="o">-</span><span class="n">nan</span>
<span class="mf">11</span><span class="w">          </span><span class="mf">1.54143e-44</span>
</pre></div>

<p>Obviously they will be integers.</p>
<p>Now, another example:</p>
<div class="code"><pre class="code literal-block"><span class="mf">1065353216</span><span class="w">  </span><span class="mf">1</span>
<span class="mf">1084227584</span><span class="w">  </span><span class="mf">5</span>
<span class="mf">1085276160</span><span class="w">  </span><span class="mf">5.5</span>
<span class="mf">1068149391</span><span class="w">  </span><span class="mf">1.33333</span>
<span class="mf">1083179008</span><span class="w">  </span><span class="mf">4.5</span>
<span class="mf">1120403456</span><span class="w">  </span><span class="mf">100</span>
<span class="mf">0</span><span class="w">           </span><span class="mf">0</span>
<span class="o">-</span><span class="mf">1110651699</span><span class="w"> </span><span class="o">-</span><span class="mf">0.1</span>
<span class="mf">1195593728</span><span class="w">  </span><span class="mf">50000</span>
</pre></div>

<p>These will obviously be floats.</p>
<p>PS: I'm using C++ but you can answer in any language, pseudo code or just in
english.</p>
<p><br><br></p>
<h2>Answer</h2>
<p>You are going to be looking at the upper 8 or 9 bits. That's where the sign
and mantissa of a floating point value are. Values of 0x00 0x80 and 0xFF here
are pretty uncommon for valid float data.</p>
<p>In particular if the upper 9 bits are all 0 then this likely to be a valid
floating point value only if all 32 bits are 0. Another way to say this is
that if the exponent is 0, the mantissa should also be zero. If the upper bit
is 1 and the next 8 bits are 0, this is legal, but also not likely to be
valid. It represents -0.0 which is a legal floating point value, but a
meaningless one.</p>
<p>To put this into numerical terms. if the upper byte is 0x00 (or 0x80), then
the value has a magnitude of <em>at most</em> 2.35e-38. Plank's constant is 6.62e-34
m2kg/s that's 4 orders of magnitude larger. The estimated diameter of a proton
is much much larger than that (estimated at 1.6e−15 meters). The smallest non-
zero value for audio data is about 2.3e-10. You aren't likely to see floating
point values are are legitimate measurements of anything real that are smaller
than 2.35e-38 but <em>not</em> zero.</p>
<p>Going the other direction if the upper byte is 0xFF then this value is either
Infinite, a NaN or larger in magnitude than 3.4e+38. The age of the universe
is estimated to be 1.3e+10 years (1.3e+25 femtoseconds). The observable
universe has roughly e+23 stars, Avagadro's number is 6.02e+23. Once again
float values larger than e+38 rarely show up in legitimate measurements.</p>
<p>This is not to say that the FPU can't load or produce such values, and you
will certainly see them in intermediate values of calculations if you are
working with modern FPUs. A modern FPU will load a floating point value that
has a exponent of 0 but the other bits are not 0. These are called
<em>denormalized</em> values. This is why you are seeing small positive integers show
up as float values in the range of e-42 even though the normal range of a
float only goes down to e-38</p>
<p>An exponent of all 1s represents Infinity. You probably won't find infinities
in your data, but you would know better than I. -Infinity is 0xFF800000,
+Infinity is 0x7F800000, any value other than 0 in the mantissa of Infinity is
malformed. malformed infinities are used as NaNs.</p>
<p>Loading a NaN into a float register can cause it to throw an exception, so you
want to use integer math to do your guessing about whether your data is float
or int until you are fairly certain it is int.</p>
<p><br></p>
<h3>Suggest</h3>
<p>Looks like a kolmogorov complexity issue. Basically, from what you show as
example, the shorter number (when printed as string to be read by a human), be
it integer or float, is the right answer for your heuristic.</p>
<p>Also, obviously if the value is an incorrect float, it is an integer :-)</p>
<p>Seems direct enough to implement.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/neural-network-back-propagation-algorithm-gets-stuck-on-xor-training-pattern/" class="u-url">Neural Network Back-Propagation Algorithm Gets Stuck on XOR Training PAttern</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/neural-network-back-propagation-algorithm-gets-stuck-on-xor-training-pattern/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:26:06+08:00" itemprop="datePublished" title="2023-02-28 03:26">2023-02-28 03:26</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p><strong>Overview</strong></p>
<p>So I'm trying to get a grasp on the mechanics of neural networks. I still
don't totally grasp the math behind it, but I think I understand how to
implement it. I currently have a neural net that can learn AND, OR, and NOR
training patterns. However, I can't seem to get it to implement the XOR
pattern. My <strong>feed forward</strong> neural network consists of <strong>2 inputs, 3 hidden,
and 1 output.</strong> The weights and biases are randomly set between <strong>-0.5 and
0.5</strong> , and outputs are generated with the <strong>sigmoidal activation function</strong></p>
<p><strong>Algorithm</strong></p>
<p>So far, I'm guessing I made a mistake in my training algorithm which is
described below:</p>
<ol>
<li>For each neuron in the output layer, provide an <code>error</code> value that is the <code>desiredOutput - actualOutput</code> -- <em>go to step 3</em>
</li>
<li>For each neuron in a hidden or input layer (working backwards) provide an <code>error</code> value that is the sum of all <code>forward connection weights * the errorGradient of the neuron at the other end of the connection</code> -- <em>go to step 3</em>
</li>
<li>For each neuron, using the <code>error</code> value provided, generate an <code>error gradient</code> that equals <code>output * (1-output) * error</code>. -- <em>go to step 4</em>
</li>
<li>For each neuron, adjust the bias to equal <code>current bias + LEARNING_RATE * errorGradient</code>. Then adjust each backward connection's weight to equal <code>current weight + LEARNING_RATE * output of neuron at other end of connection * this neuron's errorGradient</code>
</li>
</ol>
<p>I'm training my neural net online, so this runs after each training sample.</p>
<p><strong>Code</strong></p>
<p>This is the main code that runs the neural network:</p>
<div class="code"><pre class="code literal-block"><span class="n">private</span><span class="w"> </span><span class="n">void</span><span class="w"> </span><span class="n">simulate</span><span class="p">(</span><span class="k">double</span><span class="w"> </span><span class="n">maximumError</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>

<span class="w">    </span><span class="nc">int</span><span class="w"> </span><span class="n">errorRepeatCount</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">    </span><span class="k">double</span><span class="w"> </span><span class="n">prevError</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">    </span><span class="k">double</span><span class="w"> </span><span class="n">error</span><span class="p">;</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="n">summed</span><span class="w"> </span><span class="n">squares</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">errors</span>
<span class="w">    </span><span class="nc">int</span><span class="w"> </span><span class="n">trialCount</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">    </span><span class="n">do</span><span class="w"> </span><span class="err">{</span>

<span class="w">        </span><span class="n">error</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="n">loop</span><span class="w"> </span><span class="n">through</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="k">set</span>
<span class="w">        </span><span class="k">for</span><span class="p">(</span><span class="nc">int</span><span class="w"> </span><span class="k">index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="k">index</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="k">Parameters</span><span class="p">.</span><span class="n">INPUT_TRAINING_SET</span><span class="p">.</span><span class="n">length</span><span class="p">;</span><span class="w"> </span><span class="k">index</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>

<span class="w">            </span><span class="k">double</span><span class="err">[]</span><span class="w"> </span><span class="n">currentInput</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">Parameters</span><span class="p">.</span><span class="n">INPUT_TRAINING_SET</span><span class="o">[</span><span class="n">index</span><span class="o">]</span><span class="p">;</span>
<span class="w">            </span><span class="k">double</span><span class="err">[]</span><span class="w"> </span><span class="n">expectedOutput</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">Parameters</span><span class="p">.</span><span class="n">OUTPUT_TRAINING_SET</span><span class="o">[</span><span class="n">index</span><span class="o">]</span><span class="p">;</span>
<span class="w">            </span><span class="k">double</span><span class="err">[]</span><span class="w"> </span><span class="k">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">getOutput</span><span class="p">(</span><span class="n">currentInput</span><span class="p">);</span>

<span class="w">            </span><span class="n">train</span><span class="p">(</span><span class="n">expectedOutput</span><span class="p">);</span>

<span class="w">            </span><span class="o">//</span><span class="w"> </span><span class="n">Subtracts</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">expected</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">actual</span><span class="w"> </span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">gets</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">average</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">those</span><span class="w"> </span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="n">squares</span><span class="w"> </span><span class="n">it</span><span class="p">.</span>
<span class="w">            </span><span class="n">error</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">Math</span><span class="p">.</span><span class="n">pow</span><span class="p">(</span><span class="n">getAverage</span><span class="p">(</span><span class="n">subtractArray</span><span class="p">(</span><span class="k">output</span><span class="p">,</span><span class="w"> </span><span class="n">expectedOutput</span><span class="p">)),</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span>



<span class="w">        </span><span class="err">}</span>

<span class="w">    </span><span class="err">}</span><span class="w"> </span><span class="k">while</span><span class="p">(</span><span class="n">error</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">maximumError</span><span class="p">);</span>
</pre></div>

<p>Now the <code>train()</code> function:</p>
<div class="code"><pre class="code literal-block"><span class="k">public</span><span class="w"> </span><span class="n">void</span><span class="w"> </span><span class="n">train</span><span class="p">(</span><span class="k">double</span><span class="err">[]</span><span class="w"> </span><span class="n">expected</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>

<span class="w">    </span><span class="n">layers</span><span class="p">.</span><span class="n">outputLayer</span><span class="p">().</span><span class="n">calculateErrors</span><span class="p">(</span><span class="n">expected</span><span class="p">);</span>

<span class="w">    </span><span class="k">for</span><span class="p">(</span><span class="nc">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">Parameters</span><span class="p">.</span><span class="n">NUM_HIDDEN_LAYERS</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">--</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="n">layers</span><span class="p">.</span><span class="n">allLayers</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">.</span><span class="n">calculateErrors</span><span class="p">();</span>
<span class="w">    </span><span class="err">}</span>

<span class="err">}</span>
</pre></div>

<p>Output layer <code>calculateErrors()</code> function:</p>
<div class="code"><pre class="code literal-block"><span class="k">public</span><span class="w"> </span><span class="n">void</span><span class="w"> </span><span class="n">calculateErrors</span><span class="p">(</span><span class="k">double</span><span class="err">[]</span><span class="w"> </span><span class="n">expectedOutput</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>

<span class="w">    </span><span class="k">for</span><span class="p">(</span><span class="nc">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">numNeurons</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>

<span class="w">        </span><span class="n">Neuron</span><span class="w"> </span><span class="n">neuron</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">neurons</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">;</span>
<span class="w">        </span><span class="k">double</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">expectedOutput</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">neuron</span><span class="p">.</span><span class="n">getOutput</span><span class="p">();</span>
<span class="w">        </span><span class="n">neuron</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">error</span><span class="p">);</span>

<span class="w">    </span><span class="err">}</span>

<span class="err">}</span>
</pre></div>

<p>Normal (Hidden &amp; Input) layer <code>calculateErrors()</code> function:</p>
<div class="code"><pre class="code literal-block"><span class="k">public</span><span class="w"> </span><span class="n">void</span><span class="w"> </span><span class="n">calculateErrors</span><span class="p">()</span><span class="w"> </span><span class="err">{</span>

<span class="w">    </span><span class="k">for</span><span class="p">(</span><span class="nc">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">neurons</span><span class="p">.</span><span class="n">length</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>

<span class="w">        </span><span class="n">Neuron</span><span class="w"> </span><span class="n">neuron</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">neurons</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">;</span>

<span class="w">        </span><span class="k">double</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">        </span><span class="k">for</span><span class="p">(</span><span class="k">Connection</span><span class="w"> </span><span class="k">connection</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">neuron</span><span class="p">.</span><span class="n">forwardConnections</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>

<span class="w">            </span><span class="n">error</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="k">connection</span><span class="p">.</span><span class="k">output</span><span class="p">.</span><span class="n">errorGradient</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">connection</span><span class="p">.</span><span class="n">weight</span><span class="p">;</span>

<span class="w">        </span><span class="err">}</span>

<span class="w">        </span><span class="n">neuron</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">error</span><span class="p">);</span>

<span class="w">    </span><span class="err">}</span>

<span class="err">}</span>
</pre></div>

<p>Full Neuron class:</p>
<div class="code"><pre class="code literal-block"><span class="n">package</span> <span class="n">neuralNet</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">neurons</span><span class="p">;</span>

<span class="kn">import</span> <span class="nn">java.util.ArrayList</span><span class="p">;</span>
<span class="kn">import</span> <span class="nn">java.util.List</span><span class="p">;</span>
<span class="kn">import</span> <span class="nn">java.util.Random</span><span class="p">;</span>

<span class="kn">import</span> <span class="nn">neuralNet.Parameters</span><span class="p">;</span>
<span class="kn">import</span> <span class="nn">neuralNet.layers.NeuronLayer</span><span class="p">;</span>

<span class="n">public</span> <span class="k">class</span> <span class="nc">Neuron</span> <span class="p">{</span>

<span class="n">private</span> <span class="n">double</span> <span class="n">output</span><span class="p">,</span> <span class="n">bias</span><span class="p">;</span>
<span class="n">public</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">Connection</span><span class="o">&gt;</span> <span class="n">forwardConnections</span> <span class="o">=</span> <span class="n">new</span> <span class="n">ArrayList</span><span class="o">&lt;</span><span class="n">Connection</span><span class="o">&gt;</span><span class="p">();</span> <span class="o">//</span> <span class="n">Forward</span> <span class="o">=</span> <span class="n">layer</span> <span class="n">closer</span> <span class="n">to</span> <span class="nb">input</span> <span class="o">-&gt;</span> <span class="n">layer</span> <span class="n">closer</span> <span class="n">to</span> <span class="n">output</span>
<span class="n">public</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">Connection</span><span class="o">&gt;</span> <span class="n">backwardConnections</span> <span class="o">=</span> <span class="n">new</span> <span class="n">ArrayList</span><span class="o">&lt;</span><span class="n">Connection</span><span class="o">&gt;</span><span class="p">();</span> <span class="o">//</span> <span class="n">Backward</span> <span class="o">=</span> <span class="n">layer</span> <span class="n">closer</span> <span class="n">to</span> <span class="n">output</span> <span class="o">-&gt;</span> <span class="n">layer</span> <span class="n">closer</span> <span class="n">to</span> <span class="nb">input</span>

<span class="n">public</span> <span class="n">double</span> <span class="n">errorGradient</span><span class="p">;</span>
<span class="n">public</span> <span class="n">Neuron</span><span class="p">()</span> <span class="p">{</span>

    <span class="n">Random</span> <span class="n">random</span> <span class="o">=</span> <span class="n">new</span> <span class="n">Random</span><span class="p">();</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">nextDouble</span><span class="p">()</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">;</span>

<span class="p">}</span>

<span class="n">public</span> <span class="n">void</span> <span class="n">addConnections</span><span class="p">(</span><span class="n">NeuronLayer</span> <span class="n">prevLayer</span><span class="p">)</span> <span class="p">{</span>

    <span class="o">//</span> <span class="n">This</span> <span class="ow">is</span> <span class="n">true</span> <span class="k">for</span> <span class="nb">input</span> <span class="n">layers</span><span class="o">.</span> <span class="n">They</span> <span class="n">create</span> <span class="n">their</span> <span class="n">connections</span> <span class="n">differently</span><span class="o">.</span> <span class="p">(</span><span class="n">See</span> <span class="n">InputLayer</span> <span class="n">class</span><span class="p">)</span>
    <span class="k">if</span><span class="p">(</span><span class="n">prevLayer</span> <span class="o">==</span> <span class="n">null</span><span class="p">)</span> <span class="k">return</span><span class="p">;</span>

    <span class="k">for</span><span class="p">(</span><span class="n">Neuron</span> <span class="n">neuron</span> <span class="p">:</span> <span class="n">prevLayer</span><span class="o">.</span><span class="n">neurons</span><span class="p">)</span> <span class="p">{</span>

        <span class="n">Connection</span><span class="o">.</span><span class="n">createConnection</span><span class="p">(</span><span class="n">neuron</span><span class="p">,</span> <span class="n">this</span><span class="p">);</span>

    <span class="p">}</span>

<span class="p">}</span>

<span class="n">public</span> <span class="n">void</span> <span class="n">calcOutput</span><span class="p">()</span> <span class="p">{</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">bias</span><span class="p">;</span>

    <span class="k">for</span><span class="p">(</span><span class="n">Connection</span> <span class="n">connection</span> <span class="p">:</span> <span class="n">backwardConnections</span><span class="p">)</span> <span class="p">{</span>

        <span class="n">connection</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">calcOutput</span><span class="p">();</span>
        <span class="n">output</span> <span class="o">+=</span> <span class="n">connection</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">getOutput</span><span class="p">()</span> <span class="o">*</span> <span class="n">connection</span><span class="o">.</span><span class="n">weight</span><span class="p">;</span>

    <span class="p">}</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">output</span><span class="p">);</span>

<span class="p">}</span>

<span class="n">private</span> <span class="n">double</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">double</span> <span class="n">output</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">Math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">output</span><span class="p">));</span>
<span class="p">}</span>

<span class="n">public</span> <span class="n">double</span> <span class="n">getOutput</span><span class="p">()</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">public</span> <span class="n">void</span> <span class="n">train</span><span class="p">(</span><span class="n">double</span> <span class="n">error</span><span class="p">)</span> <span class="p">{</span>

    <span class="n">this</span><span class="o">.</span><span class="n">errorGradient</span> <span class="o">=</span> <span class="n">output</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">output</span><span class="p">)</span> <span class="o">*</span> <span class="n">error</span><span class="p">;</span>

    <span class="n">bias</span> <span class="o">+=</span> <span class="n">Parameters</span><span class="o">.</span><span class="n">LEARNING_RATE</span> <span class="o">*</span> <span class="n">errorGradient</span><span class="p">;</span>

    <span class="k">for</span><span class="p">(</span><span class="n">Connection</span> <span class="n">connection</span> <span class="p">:</span> <span class="n">backwardConnections</span><span class="p">)</span> <span class="p">{</span>

        <span class="o">//</span> <span class="k">for</span> <span class="n">clarification</span><span class="p">:</span> <span class="n">connection</span><span class="o">.</span><span class="n">input</span> <span class="n">refers</span> <span class="n">to</span> <span class="n">a</span> <span class="n">neuron</span> <span class="n">that</span> <span class="n">outputs</span> <span class="n">to</span> <span class="n">this</span> <span class="n">neuron</span>
        <span class="n">connection</span><span class="o">.</span><span class="n">weight</span> <span class="o">+=</span> <span class="n">Parameters</span><span class="o">.</span><span class="n">LEARNING_RATE</span> <span class="o">*</span> <span class="n">connection</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">getOutput</span><span class="p">()</span> <span class="o">*</span> <span class="n">errorGradient</span><span class="p">;</span>

    <span class="p">}</span>

<span class="p">}</span>

<span class="p">}</span>
</pre></div>

<p><strong>Results</strong></p>
<p>When I'm training for AND, OR, or NOR the network can usually converge within
about 1000 epochs, however when I train with XOR, the outputs become fixed and
it never converges. So, what am I doing wrong? Any ideas?</p>
<p><strong>Edit</strong></p>
<p>Following the advice of others, I started over and implemented my neural
network without classes...and it works. I'm still not sure where my problem
lies in the above code, but it's in there somewhere.</p>
<p><br><br></p>
<h2>Answer</h2>
<p>LiKao's comment to simplify my implementation and get rid of the object-
oriented aspects solved my problem. The flaw in the algorithm as it is
described above is unknown, however I now have a working neural network that
is a lot smaller.</p>
<p>Feel free to continue to provide insight on the problem with my previous
implementation, as others may have the same problem in the future.</p>
<p><br></p>
<h3>Suggest</h3>
<p>I faced the same problem short time ago. Finally I found the solution, how to
write a code solving XOR wit the MLP algorithm.</p>
<p>The XOR problem seems to be an easy to learn problem but it isn't for the MLP
because it's not linearly separable. So even if your MLP is OK (I mean there
is no bug in your code) you have to find the good parameters to be able to
learn the XOR problem.</p>
<p>Two hidden and one output neuron is fine. The 2 main thing you have to set:</p>
<ul>
<li>although you have only 4 training samples you have to run the training for a couple of thousands epoch.</li>
<li>if you use sigmoid hidden layers but linear output the network will converge faster</li>
</ul>
<p>Here is the detailed description and sample code:
http://freeconnection.blogspot.hu/2012/09/solving-xor-with-mlp.html</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/what-does-train-on-batch-do-in-keras-model/" class="u-url">What does train_on_batch() do in keras model?</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/what-does-train-on-batch-do-in-keras-model/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:25:46+08:00" itemprop="datePublished" title="2023-02-28 03:25">2023-02-28 03:25</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>I saw a sample of code (too big to paste here) where the author used
<code>model.train_on_batch(in, out)</code> instead of <code>model.fit(in, out)</code>. The official
documentation of Keras says:</p>
<blockquote>
<p>Single gradient update over one batch of samples.</p>
</blockquote>
<p>But I don't get it. Is it the same as <code>fit()</code>, but instead of doing many feed-
forward and backprop steps, it does it once? Or am I wrong?</p>
<p><br><br></p>
<h2>Answer</h2>
<p>Yes, <code>train_on_batch</code> trains using a single batch only and once.</p>
<p>While <code>fit</code> trains many batches for many epochs. (Each batch causes an update
in weights).</p>
<p>The idea of using <code>train_on_batch</code> is probably to do more things yourself
between each batch.</p>
<p><br></p>
<h3>Suggest</h3>
<p>It is used when we want to understand and do some custom changes after each
batch training.</p>
<p>A more precide use case is with the GANs. You have to update discriminator but
during update the GAN network you have to keep the discriminator untrainable.
so you first train the discriminator and then train the gan keeping
discriminator untrainable. see this for more understanding:
https://medium.com/datadriveninvestor/generative-adversarial-network-gan-
using-keras-ce1c05cfdfd3</p>
    </div>
    </article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="index-1414.html" rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-1412.html" rel="next">Older posts</a>
            </li>
        </ul></nav>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow中文网</a>  
            
        </footer>
</div>
</div>


            <script src="assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="assets/js/search.js"></script>
</body>
</html>
