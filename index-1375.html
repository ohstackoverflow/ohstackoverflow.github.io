<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="This is a snapshot site for StackOverflow">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>StackOverflow Snapshot (old posts, page 1375) | StackOverflow Snapshot</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/index-1375.html">
<link rel="prev" href="index-1376.html" type="text/html">
<link rel="next" href="index-1374.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="archive.html">Archive</a>
                </li>
<li>
<a href="categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            

    


    
<div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/derivative-of-sigmoid/" class="u-url">Derivative of sigmoid</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/derivative-of-sigmoid/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T02:42:39+08:00" itemprop="datePublished" title="2023-02-28 02:42">2023-02-28 02:42</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>I'm creating a neural network using the backpropagation technique for
learning.</p>
<p>I understand we need to find the derivative of the activation function used.
I'm using the standard sigmoid function</p>
<div class="code"><pre class="code literal-block">f(x) = 1 / (1 + e^(-x))
</pre></div>

<p>and I've seen that its derivative is</p>
<div class="code"><pre class="code literal-block">dy/dx = f(x)' = f(x) * (1 - f(x))
</pre></div>

<p>This may be a daft question, but does this mean that we have to pass x through
the sigmoid function twice during the equation, so it would expand to</p>
<div class="code"><pre class="code literal-block">dy/dx = f(x)' = 1 / (1 + e^(-x)) * (1 - (1 / (1 + e^(-x))))
</pre></div>

<p>or is it simply a matter of taking the already calculated output of <code>f(x)</code>,
which is the output of the neuron, and replace that value for <code>f(x)</code>?</p>
<p><br><br></p>
<h2>Answer</h2>
<p>The two ways of doing it are equivalent (since mathematical functions don't
have side-effects and always return the same input for a given output), so you
might as well do it the (faster) second way.</p>
<p><br></p>
<h3>Suggest</h3>
<p>The two ways of doing it are equivalent (since mathematical functions don't
have side-effects and always return the same input for a given output), so you
might as well do it the (faster) second way.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/what-is-the-difference-between-greedy-search-and-uniform-cost-search/" class="u-url">What is the difference between Greedy-Search and Uniform-Cost-Search?</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/what-is-the-difference-between-greedy-search-and-uniform-cost-search/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T02:42:18+08:00" itemprop="datePublished" title="2023-02-28 02:42">2023-02-28 02:42</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>When searching in a tree, my understanding of uniform cost search is that for
a given node A, having child nodes B,C,D with associated costs of (10, 5, 7),
my algorithm will choose C, as it has a lower cost. After expanding C, I see
nodes E, F, G with costs of (40, 50, 60). It will choose 40, as it has the
minimum value from both 3.</p>
<p>Now, isn't it just the same as doing a Greedy-Search, where you always choose
what seems to be the best action?</p>
<p>Also, when defining costs from going from certain nodes to others, should we
consider the whole cost from the beginning of the tree to the current node, or
just the cost itself from going from node n to node n'?</p>
<p>Thanks</p>
<p><br><br></p>
<h2>Answer</h2>
<p>Nope. Your understanding isn't quite right.</p>
<p>The next node to be visited in case of uniform-cost-search would be D, as that
has the lowest total cost from the root (7, as opposed to 40+5=45).</p>
<p>Greedy Search doesn't go back up the tree - it picks the lowest value and
commits to that. Uniform-Cost will pick the lowest total cost from the entire
tree.</p>
<p><br></p>
<h3>Suggest</h3>
<p>In a uniform cost search you always consider all unvisited nodes you have seen
so far, not just those that are connected to the node you looked at. So in
your example, after choosing C, you would find that visiting G has a total
cost of 40 + 5 = 45 which is higher than the cost of starting again from the
root and visiting D, which has cost 7. So you would visit D next.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/why-is-a-target-network-required/" class="u-url">Why is a target network required?</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/why-is-a-target-network-required/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T02:42:01+08:00" itemprop="datePublished" title="2023-02-28 02:42">2023-02-28 02:42</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>I have a concern in understanding why a target network is necessary in DQN?
I’m reading paper on “human-level control through deep reinforcement learning”</p>
<p>I understand Q-learning. Q-learning is value-based reinforcement learning
algorithm that learns “optimal” probability distribution between state-action
that will maximize it’s long term discounted reward over a sequence of
timesteps.</p>
<p>The Q-learning is updated using the bellman equation, and a single step of the
q-learning update is given by</p>
<div class="code"><pre class="code literal-block">Q(S, A) = Q(S, A) + $\alpha$[R_(t+1) + $\gamma$ (Q(s’,a;’) - Q(s,a)]
</pre></div>

<p>Where alpha and gamma are learning and discount factors. I can understand that
the reinforcement learning algorithm will become unstable and diverge.</p>
<ul>
<li>
<p>The experience replay buffer is used so that we do not forget past experiences and to de-correlate datasets provided to learn the probability distribution.</p>
</li>
<li>
<p>This is where I fail.</p>
</li>
<li>
<p>Let me break the paragraph from the paper down here for discussion </p>
<ul>
<li>The fact that small updates to $Q$ may significantly change the policy and therefore change the data distribution — understood this part. Changes to Q-network periodically may lead to unstability and changes in distribution. For example, if we always take a left turn or something like this.</li>
<li>and the correlations between the action-values (Q) and the target values <code>r + $gamma$ (argmax(Q(s’,a’))</code> — This says that the reward + gamma * my prediction of the return given that I take what I think is the best action in the current state and follow my policy from then on.</li>
<li>We used an iterative update that adjusts the action-values (Q) towards target values that are only periodically updated, thereby reducing correlations with the target.</li>
</ul>
</li>
</ul>
<p>So, in summary a target network required because the network keeps changing at
each timestep and the “target values” are being updated at each timestep?</p>
<p>But I do not understand how it is going to solve it?</p>
<p><br><br></p>
<h2>Answer</h2>
<blockquote>
<p>So, in summary a target network required because the network keeps changing
at each timestep and the “target values” are being updated at each timestep?</p>
</blockquote>
<p>The difference between Q-learning and DQN is that you have replaced an <em>exact</em>
value function with a function approximator. With Q-learning you are updating
exactly one state/action value at each timestep, whereas with DQN you are
updating many, which you understand. The problem this causes is that you can
affect the action values for the <em>very next state</em> you will be in instead of
guaranteeing them to be stable as they are in Q-learning.</p>
<p>This happens basically all the time with DQN when using a standard deep
network (bunch of layers of the same size fully connected). The effect you
typically see with this is referred to as "catastrophic forgetting" and it can
be quite spectacular. If you are doing something like moon lander with this
sort of network (the simple one, not the pixel one) and track the rolling
average score over the last 100 games or so, you will likely see a nice curve
up in score, then all of a sudden it completely craps out starts making awful
decisions again even as your alpha gets small. This cycle will continue
endlessly regardless of how long you let it run.</p>
<p>Using a stable target network as your error measure is one way of combating
this effect. Conceptually it's like saying, "I have an idea of how to play
this well, I'm going to try it out for a bit until I find something better" as
opposed to saying "I'm going to retrain myself how to play this entire game
after every move". By giving your network more time to consider many actions
that have taken place recently instead of updating <em>all the time</em> , it
hopefully finds a more robust model before you start using it to make actions.</p>
<hr>
<p>On a side note, DQN is essentially obsolete at this point, but the themes from
that paper were the fuse leading up to the RL explosion of the last few years.</p>
    </div>
    </article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="index-1376.html" rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-1374.html" rel="next">Older posts</a>
            </li>
        </ul></nav>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow-ZH</a>  
            
        </footer>
</div>
</div>


            <script src="assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="assets/js/search.js"></script>
</body>
</html>
