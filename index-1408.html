<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="This is a snapshot site for StackOverflow">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>StackOverflow Snapshot (old posts, page 1408) | StackOverflow Snapshot</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/index-1408.html">
<link rel="prev" href="index-1409.html" type="text/html">
<link rel="next" href="index-1407.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="archive.html">Archive</a>
                </li>
<li>
<a href="categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            

    


    
<div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/what-is-the-coolest-ai-project-you-ve-heard-of/" class="u-url">What is the coolest AI project you've heard of?</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/what-is-the-coolest-ai-project-you-ve-heard-of/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:21:03+08:00" itemprop="datePublished" title="2023-02-28 03:21">2023-02-28 03:21</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>As I learn more about Computer Science, AI, and Neural Networks, I am
continually amazed by the cool things a computer can do and learn. I've been
fascinated by projects new and old, and I'm curios of the interesting
projects/applications other SO users have run into.</p>
<p><br><br></p>
<h2>Answer</h2>
<p>The Numenta Platform for Intelligent Computing. They are implementing the type
of neuron described in "On Intelligence" by Jeff Hawkins. For an idea of the
significance, they are working on software neurons that can visually recognize
objects in about 200 steps instead of the thousands and thousands necessary
now.</p>
<p>Edit: Apparently version 1.6.1 of the SDK is available now. Exciting times for
learning software!!</p>
<p><br></p>
<h3>Suggest</h3>
<p>This isn't AI itself, but OpenCyc (and probably it's commercial big brother,
Cyc) could provide the "common sense" AI applications need to really
understand the world in which they exist.</p>
<p>For example, Cyc could provide the enough general knowledge that it could
begin to "read" and reason about encyclopedic content such as Wikipedia, or
surf the "Semantic Web" acting as an agent to develop some domain-specific
knowledge base.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/information-gain-and-entropy/" class="u-url">Information Gain and Entropy</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/information-gain-and-entropy/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:20:45+08:00" itemprop="datePublished" title="2023-02-28 03:20">2023-02-28 03:20</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>I recently read this question regarding information gain and entropy. I think
I have a semi-decent grasp on the main idea, but I'm curious as what to do
with situations such as follows:</p>
<p>If we have a bag of 7 coins, 1 of which is heavier than the others, and 1 of
which is lighter than the others, and we know the heavier coin + the lighter
coin is the same as 2 normal coins, what is the information gain associated
with picking two random coins and weighing them against each other?</p>
<p>Our goal here is to identify the two odd coins. I've been thinking this
problem over for a while, and can't frame it correctly in a decision tree, or
any other way for that matter. Any help?</p>
<p>EDIT: I understand the formula for entropy and the formula for information
gain. What I don't understand is how to frame this problem in a decision tree
format.</p>
<p>EDIT 2: Here is where I'm at so far:</p>
<p>Assuming we pick two coins and they both end up weighing the same, we can
assume our new chances of picking H+L come out to 1/5 * 1/4 = 1/20 , easy
enough.</p>
<p>Assuming we pick two coins and the left side is heavier. There are three
different cases where this can occur:</p>
<p>HM: Which gives us 1/2 chance of picking H and a 1/4 chance of picking L: 1/8
HL: 1/2 chance of picking high, 1/1 chance of picking low: 1/1 ML: 1/2 chance
of picking low, 1/4 chance of picking high: 1/8</p>
<p>However, the odds of us picking HM are 1/7 * 5/6 which is 5/42<br>
The odds of us picking HL are 1/7 * 1/6 which is 1/42<br>
And the odds of us picking ML are 1/7 * 5/6 which is 5/42</p>
<p>If we weight the overall probabilities with these odds, we are given:</p>
<p>(1/8) * (5/42) + (1/1) * (1/42) + (1/8) * (5/42) = 3/56.</p>
<p>The same holds true for option B.</p>
<p>option A = 3/56<br>
option B = 3/56<br>
option C = 1/20</p>
<p>However, option C should be weighted heavier because there is a 5/7 * 4/6
chance to pick two mediums. So I'm assuming from here I weight THOSE odds.</p>
<p>I am pretty sure I've messed up somewhere along the way, but I think I'm on
the right path!</p>
<p>EDIT 3: More stuff.</p>
<p>Assuming the scale is unbalanced, the odds are (10/11) that only one of the
coins is the H or L coin, and (1/11) that both coins are H/L</p>
<p>Therefore we can conclude:<br>
(10 / 11) * (1/2 * 1/5) and<br>
(1 / 11) * (1/2)</p>
<p>EDIT 4: Going to go ahead and say that it is a total 4/42 increase.</p>
<p><br><br></p>
<h2>Answer</h2>
<p>You can construct a decision tree from information-gain considerations, but
that's not the question you posted, which is only the compute the information
gain (presumably the <em>expected</em> information gain;-) from one "information
extraction move" -- picking two random coins and weighing them against each
other. To construct the decision tree, you need to know what moves are
affordable from the initial state (presumably the general rule is: you can
pick two sets of N coins, N &lt; 4, and weigh them against each other -- and
that's the only kind of move, parametric over N), the expected information
gain from each, and that gives you the first leg of the decision tree (the
move with highest expected information gain); then you do the same process for
each of the possible results of that move, and so on down.</p>
<p>So do you need help to compute that expected information gain for each of the
three allowable values of N, only for N==1, or can you try doing it yourself?
If the third possibility obtains, then that would maximize the amount of
learning you get from the exercise -- which after all IS the key purpose of
homework. So why don't you try, edit your answer to show you how you proceeded
and what you got, and we'll be happy to confirm you got it right, or try and
help correct any misunderstanding your procedure might reveal!</p>
<p><strong>Edit</strong> : trying to give some hints rather than serving the OP the ready-
cooked solution on a platter;-). Call the coins H (for heavy), L (for light),
and M (for medium -- five of those). When you pick 2 coins at random you can
get (out of <code>7 * 6 == 42</code> possibilities including order) HL, LH (one each),
HM, MH, LM, ML (5 each), MM (<code>5 * 4 == 20</code> cases) -- 2 plus 20 plus 20 is 42,
check. In the weighting you get 3 possible results, call them A (left
heavier), B (right heavier), C (equal weight). HL, HM, and ML, 11 cases, will
be A; LH, MH, and LM, 11 cases, will be B; MM, 20 cases, will be C. So A and B
aren't really distinguishable (which one is left, which one is right, is
basically arbitrary!), so we have 22 cases where the weight will be different,
20 where they will be equal -- it's a good sign that the cases giving each
results are in pretty close numbers!</p>
<p>So now consider how many (equiprobable) possibilities existed a priori, how
many a posteriori, for each of the experiment's results. You're tasked to pick
the H and L choice. If you did it at random before the experiment, what would
be you chances? 1 in 7 for the random pick of the H; given that succeeds 1 in
6 for the pick of the L -- overall 1 in 42.</p>
<p>After the experiment, how are you doing? If C, you can rule out those two
coins and you're left with a mystery H, a mystery L, and three Ms -- so if you
picked at random you'd have 1 in 5 to pick H, if successful 1 in 4 to pick L,
overall 1 in 20 -- your success chances have slightly more than doubled. It's
trickier to see "what next" for the A (and equivalently B) cases because
they're several, as listed above (and, less obviously, not equiprobable...),
but obviously you won't pick the known-lighter coin for H (and viceversa) and
if you pick one of the 5 unweighed coins for H (or L) only one of the weighed
coins is a candidate for the other role (L or H respectively). Ignoring for
simplicity the "non equiprobable" issue (which is really kind of tricky) can
you compute what your chances of guessing (with a random pick not inconsistent
with the experiment's result) would be...?</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/help-100-accuracy-with-libsvm/" class="u-url">Help--100% accuracy with LibSVM?</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/help-100-accuracy-with-libsvm/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:20:11+08:00" itemprop="datePublished" title="2023-02-28 03:20">2023-02-28 03:20</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>Nominally a good problem to have, but I'm pretty sure it is because something
funny is going on...</p>
<p>As context, I'm working on a problem in the facial expression/recognition
space, so getting 100% accuracy seems incredibly implausible (not that it
would be plausible in most applications...). I'm guessing there is either some
consistent bias in the data set that it making it overly easy for an SVM to
pull out the answer, =or=, more likely, I've done something wrong on the SVM
side.</p>
<p>I'm looking for suggestions to help understand what is going on--is it me (=my
usage of LibSVM)? Or is it the data?</p>
<p>The details:</p>
<ul>
<li>About ~2500 labeled data vectors/instances (transformed video frames of individuals--&lt;20 individual persons total), binary classification problem. ~900 features/instance. Unbalanced data set at about a 1:4 ratio.</li>
<li>Ran subset.py to separate the data into test (500 instances) and train (remaining).</li>
<li>Ran "svm-train -t 0 ". (Note: apparently no need for '-w1 1 -w-1 4'...)</li>
<li>Ran svm-predict on the test file. Accuracy=100%!</li>
</ul>
<p>Things tried:</p>
<ul>
<li>Checked about 10 times over that I'm not training &amp; testing on the same data files, through some inadvertent command-line argument error</li>
<li>re-ran subset.py (even with -s 1) multiple times and did train/test only multiple different data sets (in case I randomly upon the most magical train/test pa</li>
<li>ran a simple diff-like check to confirm that the test file is not a subset of the training data</li>
<li>svm-scale on the data has no effect on accuracy (accuracy=100%). (Although the number of support vectors does drop from nSV=127, bSV=64 to nBSV=72, bSV=0.)</li>
<li>((weird)) using the default RBF kernel (vice linear -- i.e., removing '-t 0') results in accuracy going to garbage(?!)</li>
<li>(sanity check) running svm-predict using a model trained on a scaled data set against an unscaled data set results in accuracy = 80% (i.e., it always guesses the dominant class). This is strictly a sanity check to make sure that somehow svm-predict is nominally acting right on my machine.</li>
</ul>
<p>Tentative conclusion?:</p>
<p>Something with the data is wacked--somehow, within the data set, there is a
subtle, experimenter-driven effect that the SVM is picking up on.</p>
<p>(This doesn't, on first pass, explain why the RBF kernel gives garbage
results, however.)</p>
<p>Would greatly appreciate any suggestions on a) how to fix my usage of LibSVM
(if that is actually the problem) or b) determine what subtle experimenter-
bias in the data LibSVM is picking up on.</p>
<p><br><br></p>
<h2>Answer</h2>
<p>Two other ideas:</p>
<p>Make sure you're not training and testing on the same data. This sounds kind
of dumb, but in computer vision applications you should take care that: make
sure you're not repeating data (say two frames of the same video fall on
different folds), you're not training and testing on the same individual, etc.
It is more subtle than it sounds.</p>
<p>Make sure you search for gamma and C parameters for the RBF kernel. There are
good theoretical (asymptotic) results that justify that a linear classifier is
just a degenerate RBF classifier. So you should just look for a good (C,
gamma) pair.</p>
<p><br></p>
<h3>Suggest</h3>
<p>Notwithstanding that the devil is in the details, here are three simple tests
you could try:</p>
<ol>
<li>Quickie (~2 minutes): Run the data through a decision tree algorithm. This is available in Matlab via <code>classregtree</code>, or you can load into R and use <code>rpart</code>. This could tell you if one or just a few features happen to give a perfect separation.</li>
<li>Not-so-quickie (~10-60 minutes, depending on your infrastructure): Iteratively split the features (i.e. from 900 to 2 sets of 450), train, and test. If one of the subsets gives you perfect classification, split it again. It would take fewer than 10 such splits to find out where the problem variables are. If it happens to "break" with many variables remaining (or even in the first split), select a different random subset of features, shave off fewer variables at a time, etc. It can't possibly need all 900 to split the data.</li>
<li>Deeper analysis (minutes to several hours): try permutations of labels. If you can permute all of them and still get perfect separation, you have some problem in your train/test setup. If you select increasingly larger subsets to permute (or, if going in the other direction, to leave static), you can see where you begin to lose separability. Alternatively, consider decreasing your training set size and if you get separability even with a very small training set, then something is weird.</li>
</ol>
<p>Method #1 is fast &amp; should be insightful. There are some other methods I could
recommend, but #1 and #2 are easy and it would be odd if they don't give any
insights.</p>
    </div>
    </article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="index-1409.html" rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-1407.html" rel="next">Older posts</a>
            </li>
        </ul></nav>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow中文网</a>  
            
        </footer>
</div>
</div>


            <script src="assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="assets/js/search.js"></script>
</body>
</html>
