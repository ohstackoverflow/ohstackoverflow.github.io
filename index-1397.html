<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="This is a snapshot site for StackOverflow">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>StackOverflow Snapshot (old posts, page 1397) | StackOverflow Snapshot</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/index-1397.html">
<link rel="prev" href="index-1398.html" type="text/html">
<link rel="next" href="index-1396.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="archive.html">Archive</a>
                </li>
<li>
<a href="categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            

    


    
<div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/convolutional-neural-network-how-to-get-the-feature-maps/" class="u-url">Convolutional neural network - How to get the feature maps?</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/convolutional-neural-network-how-to-get-the-feature-maps/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:06:23+08:00" itemprop="datePublished" title="2023-02-28 03:06">2023-02-28 03:06</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>I read a few books and articles about Convolutional neural network, it seems I
understand the concept but I don't know how to put it up like in image below:
<img alt="alt text" src="images/LDT3H.png"><br>
(source: what-when-how.com)</p>
<p>from 28x28 normalized pixel INPUT we get 4 feature maps of size 24x24. but how
to get them ? resizing the INPUT image ? or performing image transformations?
but what kind of transformations? or cutting the input image into 4 pieces of
size 24x24 by 4 corner? I don't understand the process, to me it seem they cut
up or resize the image to smaller images at each step. please help thanks.</p>
<p><br><br></p>
<h2>Answer</h2>
<p>This is matlab help file for CONV2 function, which use in CNN Matlab (to get
convolutional layers). Read it carefully and you will see your answer.</p>
<div class="code"><pre class="code literal-block"><span class="c">%CONV2 Two dimensional convolution.</span>
<span class="c">%   C = CONV2(A, B) performs the 2-D convolution of matrices A and B.</span>
<span class="c">%   If [ma,na] = size(A), [mb,nb] = size(B), and [mc,nc] = size(C), then</span>
<span class="c">%   mc = max([ma+mb-1,ma,mb]) and nc = max([na+nb-1,na,nb]).</span>
<span class="c">%</span>
<span class="c">%   C = CONV2(H1, H2, A) convolves A first with the vector H1 along the</span>
<span class="c">%   rows and then with the vector H2 along the columns. If n1 = length(H1)</span>
<span class="c">%   and n2 = length(H2), then mc = max([ma+n1-1,ma,n1]) and </span>
<span class="c">%   nc = max([na+n2-1,na,n2]).</span>
<span class="c">%</span>
<span class="c">%   C = CONV2(..., SHAPE) returns a subsection of the 2-D</span>
<span class="c">%   convolution with size specified by SHAPE:</span>
<span class="c">%     'full'  - (default) returns the full 2-D convolution,</span>
<span class="c">%     'same'  - returns the central part of the convolution</span>
<span class="c">%               that is the same size as A.</span>
<span class="c">%     'valid' - returns only those parts of the convolution</span>
<span class="c">%               that are computed without the zero-padded edges.</span>
<span class="c">%               **size(C) = max([ma-max(0,mb-1),na-max(0,nb-1)],0).**</span>
</pre></div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/robot-programming-with-lisp/" class="u-url">robot programming with lisp?</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/robot-programming-with-lisp/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:06:02+08:00" itemprop="datePublished" title="2023-02-28 03:06">2023-02-28 03:06</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>I'm looking for some examples of robot/AI programming using lisp. Are there
any good online examples available anywhere (preferably not too academic in
nature)?</p>
<p><br><br></p>
<h2>Answer</h2>
<p>XS Lisp on Lego Mindstorms: http://www.xslisp.com/ilc03.pdf There are multiple
references online on Clojure and Lego Mindstorms dealing with a Clojure API
for Lego Mindstorms. In fact Lisp (in some form) and Leog Mindstorms's seems
not uncommon: here's a AAAI paper on Lisp and Mindstorms with some code with a
decided AI education approach: "Lauching into AI's October Sky with Robotics
and Lisp" :
http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/1863</p>
<p>A Robotics Society of Southern California intro to Lisp may have
source/examples on other pages: http://rssc.org/content/introduction-lisp</p>
<p>Aside from something like Mindstorms almost all robotic systems are custom
corporate and/or university projects so you are likely to have to see if any
final project results have published source. There may also be university
examples for robotics classes.</p>
<p>If you are looking for general AI programming then there are numerous
references: Norvig's "Paradigms of Artificial Intelligence Programming: Case
Studies in Common Lisp" is a good starting point.</p>
<p><br></p>
<h3>Suggest</h3>
<p>This is supposed to be among the bestest books on both Common LISP and AI
Programming. Use it wisely.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/pytorch-binary-classification-same-network-structure-simpler-data-but-worse-performance/" class="u-url">PyTorch Binary Classification - same network structure, 'simpler' data, but worse performance?</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/pytorch-binary-classification-same-network-structure-simpler-data-but-worse-performance/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:05:39+08:00" itemprop="datePublished" title="2023-02-28 03:05">2023-02-28 03:05</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>To get to grips with PyTorch (and deep learning in general) I started by
working through some basic classification examples. One such example was
classifying a non-linear dataset created using sklearn (full code available as
notebook here)</p>
<div class="code"><pre class="code literal-block">n_pts = 500
X, y = datasets.make_circles(n_samples=n_pts, random_state=123, noise=0.1, factor=0.2)
x_data = torch.FloatTensor(X)
y_data = torch.FloatTensor(y.reshape(500, 1))
</pre></div>

<p><img alt="enter image description here" src="images/w20Tb.png"></p>
<p>This is then accurately classified using a pretty basic neural net</p>
<div class="code"><pre class="code literal-block"><span class="k">class</span> <span class="n">Model</span>(<span class="n">nn</span>.<span class="n">Module</span>):
    <span class="n">def</span> <span class="n">__init__</span>(<span class="nb">self</span>, <span class="n">input_size</span>, <span class="n">H1</span>, <span class="n">output_size</span>):
        <span class="n">super</span>().<span class="n">__init__</span>()
        <span class="nb">self</span>.<span class="n">linear</span> = <span class="n">nn</span>.<span class="n">Linear</span>(<span class="n">input_size</span>, <span class="n">H1</span>)
        <span class="nb">self</span>.<span class="n">linear2</span> = <span class="n">nn</span>.<span class="n">Linear</span>(<span class="n">H1</span>, <span class="n">output_size</span>)

    <span class="n">def</span> <span class="n">forward</span>(<span class="nb">self</span>, <span class="nb">x</span>):
        <span class="nb">x</span> = <span class="n">torch</span>.<span class="n">sigmoid</span>(<span class="nb">self</span>.<span class="n">linear</span>(<span class="nb">x</span>))
        <span class="nb">x</span> = <span class="n">torch</span>.<span class="n">sigmoid</span>(<span class="nb">self</span>.<span class="n">linear2</span>(<span class="nb">x</span>))
        <span class="k">return</span> <span class="nb">x</span>

    <span class="n">def</span> <span class="n">predict</span>(<span class="nb">self</span>, <span class="nb">x</span>):
        <span class="nb">pred</span> = <span class="nb">self</span>.<span class="n">forward</span>(<span class="nb">x</span>)
        <span class="k">if</span> <span class="nb">pred</span> &gt;= <span class="mf">0.5</span>:
            <span class="k">return</span> <span class="mi">1</span>
        <span class="n">else:</span>
            <span class="k">return</span> <span class="mi">0</span>
</pre></div>

<p>As I have an interest in health data I then decided to try and use the same
network structure to classify some a basic real-world dataset. I took heart
rate data for one patient from here, and altered it so all values &gt; 91 would
be labelled as anomalies (e.g. a <code>1</code> and everything &lt;= 91 labelled a <code>0</code>).
This is completely arbitrary, but I just wanted to see how the classification
would work. The complete notebook for this example is here.</p>
<p><img alt="enter image description here" src="images/6izzI.png"></p>
<p>What is not intuitive to me is why the first example reaches <strong>a loss of
0.0016 after 1,000 epochs</strong> , whereas the second example only reaches <strong>a loss
of 0.4296 after 10,000 epochs</strong></p>
<p><img alt="Training Loss for Example 1" src="images/rDag4.png"></p>
<p><img alt="Training Loss for Heart Rate Example" src="images/PMubT.png"></p>
<p>Perhaps I am being naive in thinking that the heart rate example would be much
easier to classify. Any insights to help me understand why this is not what I
am seeing would be great!</p>
<p><br><br></p>
<h2>Answer</h2>
<h4>TL;DR</h4>
<p>Your input data is not normalized.</p>
<ol>
<li>use <code>x_data = (x_data - x_data.mean()) / x_data.std()</code>
</li>
<li>increase the learning rate <code>optimizer = torch.optim.Adam(model.parameters(), lr=0.01)</code>
</li>
</ol>
<p>You'll get<br><img alt="enter image description here" src="images/vt4VK.png"></p>
<p>convergence in only 1000 iterations.</p>
<h4>More details</h4>
<p>The key difference between the two examples you have is that the data <code>x</code> in
the first example is centered around (0, 0) and has very low variance.<br>
On the other hand, the data in the second example is centered around 92 and
has relatively large variance.</p>
<p>This initial bias in the data is not taken into account when you randomly
initialize the weights which is done based on the assumption that the inputs
are roughly normally distributed around <em>zero</em>.<br>
It is almost impossible for the optimization process to compensate for this
gross deviation - thus the model gets stuck in a sub-optimal solution.</p>
<p>Once you normalize the inputs, by subtracting the mean and dividing by the
std, the optimization process becomes stable again and rapidly converges to a
good solution.</p>
<p>For more details about input normalization and weights initialization, you can
read section 2.2 in <em>He et al</em> <strong>Delving Deep into Rectifiers: Surpassing
Human-Level Performance on ImageNet Classification</strong> (ICCV 2015).</p>
<h4>What if I cannot normalize the data?</h4>
<p>If, for some reason, you cannot compute mean and std data in advance, you can
still use <code>nn.BatchNorm1d</code> to estimate and normalize the data as part of the
training process. For example</p>
<div class="code"><pre class="code literal-block"><span class="k">class</span> <span class="n">Model</span>(<span class="n">nn</span>.<span class="n">Module</span>):
    <span class="n">def</span> <span class="n">__init__</span>(<span class="nb">self</span>, <span class="n">input_size</span>, <span class="n">H1</span>, <span class="n">output_size</span>):
        <span class="n">super</span>().<span class="n">__init__</span>()
        <span class="nb">self</span>.<span class="n">bn</span> = <span class="n">nn</span>.<span class="n">BatchNorm1d</span>(<span class="n">input_size</span>)  <span class="c1"># adding batchnorm</span>
        <span class="nb">self</span>.<span class="n">linear</span> = <span class="n">nn</span>.<span class="n">Linear</span>(<span class="n">input_size</span>, <span class="n">H1</span>)
        <span class="nb">self</span>.<span class="n">linear2</span> = <span class="n">nn</span>.<span class="n">Linear</span>(<span class="n">H1</span>, <span class="n">output_size</span>)

    <span class="n">def</span> <span class="n">forward</span>(<span class="nb">self</span>, <span class="nb">x</span>):
        <span class="nb">x</span> = <span class="n">torch</span>.<span class="n">sigmoid</span>(<span class="nb">self</span>.<span class="n">linear</span>(<span class="nb">self</span>.<span class="n">bn</span>(<span class="nb">x</span>)))  <span class="c1"># batchnorm the input x</span>
        <span class="nb">x</span> = <span class="n">torch</span>.<span class="n">sigmoid</span>(<span class="nb">self</span>.<span class="n">linear2</span>(<span class="nb">x</span>))
        <span class="k">return</span> <span class="nb">x</span>
</pre></div>

<p>This modification <em>without</em> any change to the input data, yields similar
convergance after only 1000 epochs:<br><img alt="enter image description here" src="images/Q9XRw.png"></p>
<h4>A minor comment</h4>
<p>For numerical stability, it is better to use <code>nn.BCEWithLogitsLoss</code> instead of
<code>nn.BCELoss</code>. For this end, you need to remove the <code>torch.sigmoid</code> from the
<code>forward()</code> output, the <code>sigmoid</code> will be computed inside the loss.<br>
See, for example, this thread regarding the related sigmoid + cross entropy
loss for binary predictions.</p>
    </div>
    </article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="index-1398.html" rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-1396.html" rel="next">Older posts</a>
            </li>
        </ul></nav>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow-ZH</a>  
            
        </footer>
</div>
</div>


            <script src="assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="assets/js/search.js"></script>
</body>
</html>
