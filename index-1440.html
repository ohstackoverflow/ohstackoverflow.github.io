<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="This is a snapshot site for StackOverflow">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>StackOverflow Snapshot (old posts, page 1440) | StackOverflow Snapshot</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/index-1440.html">
<link rel="prev" href="index-1441.html" type="text/html">
<link rel="next" href="index-1439.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="archive.html">Archive</a>
                </li>
<li>
<a href="categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            

    


    
<div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/neighbor-selection-in-simulated-annealing-algorithm/" class="u-url">Neighbor selection in simulated annealing algorithm</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/neighbor-selection-in-simulated-annealing-algorithm/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:51:53+08:00" itemprop="datePublished" title="2023-02-28 03:51">2023-02-28 03:51</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>When picking a neighbor should the algorithm's temperature be considered? So
for example if the temperature is high when picking a neighbor should be
permutation be made? Or does the temperature only affect the acceptance
probability?</p>
<p><br><br></p>
<h2>Answer</h2>
<p>The latter is true: Only the acceptance probability is influenced by the
temperature. The higher the temperature, the more "bad" moves are accepted to
escape from local optima. If you preselect neighbors with low energy values,
you'll basically contradict the idea of Simulated Annealing and turn it into a
greedy search.</p>
<p>Pseudocode from Wikipedia:</p>
<div class="code"><pre class="code literal-block"><span class="nt">s</span><span class="w"> </span><span class="err">←</span><span class="w"> </span><span class="nt">s0</span><span class="o">;</span><span class="w"> </span><span class="nt">e</span><span class="w"> </span><span class="err">←</span><span class="w"> </span><span class="nt">E</span><span class="o">(</span><span class="nt">s</span><span class="o">)</span><span class="w">                                  </span><span class="o">//</span><span class="w"> </span><span class="nt">Initial</span><span class="w"> </span><span class="nt">state</span><span class="o">,</span><span class="w"> </span><span class="nt">energy</span><span class="o">.</span>
<span class="nt">sbest</span><span class="w"> </span><span class="err">←</span><span class="w"> </span><span class="nt">s</span><span class="o">;</span><span class="w"> </span><span class="nt">ebest</span><span class="w"> </span><span class="err">←</span><span class="w"> </span><span class="nt">e</span><span class="w">                              </span><span class="o">//</span><span class="w"> </span><span class="nt">Initial</span><span class="w"> </span><span class="s2">"best"</span><span class="w"> </span><span class="nt">solution</span>
<span class="nt">k</span><span class="w"> </span><span class="err">←</span><span class="w"> </span><span class="nt">0</span><span class="w">                                             </span><span class="o">//</span><span class="w"> </span><span class="nt">Energy</span><span class="w"> </span><span class="nt">evaluation</span><span class="w"> </span><span class="nt">count</span><span class="o">.</span>
<span class="nt">while</span><span class="w"> </span><span class="nt">k</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="nt">kmax</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">e</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="nt">emax</span><span class="w">                       </span><span class="o">//</span><span class="w"> </span><span class="nt">While</span><span class="w"> </span><span class="nt">time</span><span class="w"> </span><span class="nt">left</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nt">not</span><span class="w"> </span><span class="nt">good</span><span class="w"> </span><span class="nt">enough</span><span class="o">:</span>
<span class="w">  </span><span class="nt">T</span><span class="w"> </span><span class="err">←</span><span class="w"> </span><span class="nt">temperature</span><span class="o">(</span><span class="nt">k</span><span class="o">/</span><span class="nt">kmax</span><span class="o">)</span><span class="w">                         </span><span class="o">//</span><span class="w"> </span><span class="nt">Temperature</span><span class="w"> </span><span class="nt">calculation</span><span class="o">.</span>
<span class="w">  </span><span class="nt">snew</span><span class="w"> </span><span class="err">←</span><span class="w"> </span><span class="nt">neighbour</span><span class="o">(</span><span class="nt">s</span><span class="o">)</span><span class="w">                             </span><span class="o">//</span><span class="w"> </span><span class="nt">Pick</span><span class="w"> </span><span class="nt">some</span><span class="w"> </span><span class="nt">neighbour</span><span class="o">.</span>
<span class="w">  </span><span class="nt">enew</span><span class="w"> </span><span class="err">←</span><span class="w"> </span><span class="nt">E</span><span class="o">(</span><span class="nt">snew</span><span class="o">)</span><span class="w">                                  </span><span class="o">//</span><span class="w"> </span><span class="nt">Compute</span><span class="w"> </span><span class="nt">its</span><span class="w"> </span><span class="nt">energy</span><span class="o">.</span>
<span class="w">  </span><span class="nt">if</span><span class="w"> </span><span class="nt">P</span><span class="o">(</span><span class="nt">e</span><span class="o">,</span><span class="w"> </span><span class="nt">enew</span><span class="o">,</span><span class="w"> </span><span class="nt">T</span><span class="o">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="nt">random</span><span class="o">()</span><span class="w"> </span><span class="nt">then</span><span class="w">                </span><span class="o">//</span><span class="w"> </span><span class="nt">Should</span><span class="w"> </span><span class="nt">we</span><span class="w"> </span><span class="nt">move</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">it</span><span class="o">?</span>
<span class="w">    </span><span class="nt">s</span><span class="w"> </span><span class="err">←</span><span class="w"> </span><span class="nt">snew</span><span class="o">;</span><span class="w"> </span><span class="nt">e</span><span class="w"> </span><span class="err">←</span><span class="w"> </span><span class="nt">enew</span><span class="w">                            </span><span class="o">//</span><span class="w"> </span><span class="nt">Yes</span><span class="o">,</span><span class="w"> </span><span class="nt">change</span><span class="w"> </span><span class="nt">state</span><span class="o">.</span>
<span class="w">  </span><span class="nt">if</span><span class="w"> </span><span class="nt">enew</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="nt">ebest</span><span class="w"> </span><span class="nt">then</span><span class="w">                            </span><span class="o">//</span><span class="w"> </span><span class="nt">Is</span><span class="w"> </span><span class="nt">this</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">new</span><span class="w"> </span><span class="nt">best</span><span class="o">?</span>
<span class="w">    </span><span class="nt">sbest</span><span class="w"> </span><span class="err">←</span><span class="w"> </span><span class="nt">snew</span><span class="o">;</span><span class="w"> </span><span class="nt">ebest</span><span class="w"> </span><span class="err">←</span><span class="w"> </span><span class="nt">enew</span><span class="w">                    </span><span class="o">//</span><span class="w"> </span><span class="nt">Save</span><span class="w"> </span><span class="s1">'new neighbour'</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="s1">'best found'</span><span class="o">.</span>
<span class="w">  </span><span class="nt">k</span><span class="w"> </span><span class="err">←</span><span class="w"> </span><span class="nt">k</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nt">1</span><span class="w">                                       </span><span class="o">//</span><span class="w"> </span><span class="nt">One</span><span class="w"> </span><span class="nt">more</span><span class="w"> </span><span class="nt">evaluation</span><span class="w"> </span><span class="nt">done</span>
<span class="nt">return</span><span class="w"> </span><span class="nt">sbest</span><span class="w">                                      </span><span class="o">//</span><span class="w"> </span><span class="nt">Return</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">best</span><span class="w"> </span><span class="nt">solution</span><span class="w"> </span><span class="nt">found</span><span class="o">.</span>
</pre></div>

<p><br></p>
<h3>Suggest</h3>
<p>Here is description from wikipedia which states that the temperature should be
in fact calculated in for some problems.</p>
<blockquote>
<p><strong>Efficient candidate generation</strong></p>
<p>A more precise statement of the heuristic is that one should try first
candidate states s' for which P(E(s), E(s'), T) is large. For the "standard"
acceptance function P above, it means that E(s') - E(s) is on the order of T
or less. Thus, in the traveling salesman example above, one could use a
neighbour() function that swaps two random cities, where <strong>the probability
of choosing a city pair vanishes as their distance increases beyond T.</strong></p>
</blockquote>
<p>This does imply that Temperature can be relevant factor when determining
neighbor.</p>
<p>More useful reading on how to write neighbor function: How to efficiently
select neighbour in 1-dimensional and n-dimensional space for Simulated
Annealing</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/similarity-in-data-mining/" class="u-url">'Similarity' in Data Mining</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/similarity-in-data-mining/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:51:32+08:00" itemprop="datePublished" title="2023-02-28 03:51">2023-02-28 03:51</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>In the field of Data Mining, is there a specific sub-discipline called
'Similarity'? If yes, what does it deal with. Any examples, links, references
will be helpful.</p>
<p>Also, being new to the field, I would like the community opinion on how
closely related Data Mining and Artificial Intelligence are. Are they
synonyms, is one the subset of the other?</p>
<p>Thanks in advance for sharing your knowledge.</p>
<p><br><br></p>
<h2>Answer</h2>
<blockquote>
<p>In the field of Data Mining, is there a specific sub-discipline called
'Similarity'?</p>
</blockquote>
<p>Yes. There is a specific subfield in data mining and machine learning called
metric learning, which aims to learn a better distance metric among data
instances.</p>
<p>Do you know any of the following concepts?</p>
<p>Euclidean distance</p>
<p>Mahalanobis distance</p>
<p>Pearson correlation</p>
<p>Cosine similarity and here</p>
<p>Kernel functions</p>
<p>After you know these, you will know what is 'similarity'.</p>
<blockquote>
<p>I would like the community opinion on how closely related Data Mining and
Artificial Intelligence are.</p>
</blockquote>
<p>It is very hard to distinguish what is data mining, what is AI. Don't discuss
this question when you are new in the field. When you have learned 10
algorithms in data mining and read some AI books, you will know the difference
and the relation.</p>
<p><br></p>
<h3>Suggest</h3>
<p>Appropriate definitions of 'similarity' (which features you extract, what you
do with them afterwards) are almost the definition of clustering, and
clustering is a fairly wide sub-field of data mining.</p>
<p>If you make the standard cynical definition of AI as the set of problems we
can't solve well (indeed, that we can't specify well enough to start solving),
data mining shades into it once the space in which you're looking for
correlations starts to be larger than your algorithms can handle.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/google-deep-dream-art-how-to-pick-a-layer-in-a-neural-network-and-enhance-it/" class="u-url">Google Deep Dream art: how to pick a layer in a neural network and enhance it</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/google-deep-dream-art-how-to-pick-a-layer-in-a-neural-network-and-enhance-it/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:51:12+08:00" itemprop="datePublished" title="2023-02-28 03:51">2023-02-28 03:51</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>I am interested in a recent blog post by Google that describes the use of <code>nn</code>
to make art.</p>
<p>I am particularly interested in one technique:</p>
<blockquote>
<p>'In this case we simply feed the network an arbitrary image or photo and let
the network analyze the picture. We then pick a layer and ask the network to
enhance whatever it detected. Each layer of the network deals with features
at a different level of abstraction, so the complexity of features we
generate depends on which layer we choose to enhance. For example, lower
layers tend to produce strokes or simple ornament-like patterns, because
those layers are sensitive to basic features such as edges and their
orientations.'</p>
</blockquote>
<p>The post is http://googleresearch.blogspot.co.uk/2015/06/inceptionism-going-
deeper-into-neural.html?m=1.</p>
<p><strong>My question</strong> : the post describes this as a 'simple' case--is there an
open-source implementation of a nn that could be used for this purpose in a
relatively plug-and-play process? For just the technique described, does the
network need to be trained?</p>
<p>No doubt for other techniques mentioned in the paper one needs a network
already trained on a large number of images, but for the one I've described is
there already some kind of open-source network layer visualization package?</p>
<p><br><br></p>
<h2>Answer</h2>
<p>UPD: Google posted more detail instructions how they implemented it:
https://github.com/google/deepdream/blob/master/dream.ipynb</p>
<p>There's also another project: https://317070.github.io/Dream/</p>
<p>If you read 1,[2],[3],[4] from your link, you'll see that they used Caffe.
This framework already contains the trained networks to play with. You don't
need to train anything manually, just download the models using .sh scripts in
the <code>models/</code> folder.</p>
<p>You want "plug-and-play process", it's not so easy because besides the
framework, we need the code of the scripts they used and, probably, patch
Caffe. I tried to make something using their description. Caffe has Python and
Matlab interface but there's more in its internals.</p>
<p>The text below describes my thoughts on how it could be possibly implemented.
I'm not sure about my words so it's more like an invitation to research with
me than the "plug-and-play process". But as no one still answered, let me put
it here. Maybe someone will fix me.</p>
<p>So</p>
<p>As far as I understand, they run optimization</p>
<p><code>[sum((net.forwardTo(X, n) - enchanced_layer).^2) + lambda * R(X)] -&gt; min</code></p>
<p>I.e. look for such input <code>X</code> so that the particular layer of the netword would
produce the "enchanced" data instead of the "original" data.</p>
<p>There's a regularization constraint <code>R(X)</code>: <code>X</code> should look like "natural
image" (without high-frequency noise).</p>
<p><code>X</code> is our target image. The initial point <code>X0</code> is the original image.
<code>forwardTo(X, n)</code> is what our network produces in the layer <code>n</code> when we feed
the input with X. If speak about Caffe, you can make full-forward pass
(<code>net.forward</code>) and look at the blob you are interested in
(<code>net.blob_vec(n).get_data()</code>).</p>
<p><code>enchanced_layer</code> - we take the original layer blob and "enchance" signals in
it. What does it mean, I don't know. Maybe they just multiply the values by
coefficient, maybe something else.</p>
<p>Thus <code>sum((forwardTo(X, n) - enchanced_net).^2)</code> will become zero when your
input image produces exactly what you want in the layer <code>n</code>.</p>
<p><code>lambda</code> is the regularization parameter and <code>R(X)</code> is how <code>X</code> looks natural.
I didn't implement it and my results look very noisy. As for it's formula, you
can look for it at [2].</p>
<p>I used Matlab and <code>fminlbfgs</code> to optimize.</p>
<p>The key part was to find the gradient of the formula above because the problem
has too many dimensions to calculate the gradient numerically.</p>
<p>As I said, I didn't manage to find the gradient of <code>R(X)</code>. As for the main
part of the formula, I managed to find it this way:</p>
<ul>
<li>Set diff blob at the layer <code>n</code> to <code>forwardTo(X, n) - enchanced_net</code>. (see caffe documentation for <code>set_diff</code> and <code>set_data</code>, <code>set_data</code> is used for forward and waits for data and <code>set_diff</code> is used for backward propagation and waits for data errors).</li>
<li>Perform <em>partial</em> backpropagation from layer <code>n-1</code> to the input.</li>
<li>Input diff blob would contain the gradient we need.</li>
</ul>
<p>Python and Matlab interfaces do NOT contain partial backward propagation but
Caffe C++ internals contain it. I added a patch below to make it available in
Matlab.</p>
<p>Result of enhancing the 4th layer:</p>
<p><img alt="Result of enhancing the 4th layer" src="images/6NRkY.jpg"></p>
<p>I'm not happy with the results but I think there's something in common with
the article.</p>
<ul>
<li>Here's the code that produces the picture above "as is". The entry point is "run2.m", "fit2.m" contains the fitness function: https://github.com/galchinsky/caf</li>
<li>Here's caffe patch to Matlab interface to make partial backpropagation available: https://gist.github.com/anonymous/53d7cb44c072ae6320ff</li>
</ul>
<p><br></p>
<h3>Suggest</h3>
<p>In the link to Ipython notebook Dmitry provided, it says that it does
<strong>gradient</strong> <strong>ascent</strong> with <strong>maximizing</strong> L2 normalization. I believe this
is what Google means to be enhance the feature from a algorithmic perspective.</p>
<p>If you think about it, it's really the case, minimizing L2 would prevent over-
fitting, i.e. make the curve looks smoother. If you do the opposite, you are
making the feature more obvious.</p>
<p>Here is a great link to understand gradient ascent, though it talks about
gradient descent mainly.</p>
<p>I don't know much about implementation details in caffe, since I use theano
mostly. Hope it helps!</p>
<p><strong>Update</strong></p>
<p>So I read about the detailed articles [1],[2],[3],[4] today and find out that
[3] actually talks about the algorithm in details</p>
<blockquote>
<p>A locally-optimal <em>I</em> can be found by the back-propagation method. The
procedure is related to the ConvNet training procedure, where the back-
propagation is used to optimise the layer weights. The difference is that in
our case the optimisation is performed with respect to the input image,
while the weights are fixed to those found during the training stage. We
initialised the optimisation with the zero image (in our case, the ConvNet
was trained on the zero-centred image data), and then added the training set
mean image to the result.</p>
</blockquote>
<p>Therefore, after training the network on classification, you train it again
w.r.t to the input image, using gradient ascent in order to get higher score
for the class.</p>
    </div>
    </article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="index-1441.html" rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-1439.html" rel="next">Older posts</a>
            </li>
        </ul></nav>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow-ZH</a>  
            
        </footer>
</div>
</div>


            <script src="assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="assets/js/search.js"></script>
</body>
</html>
