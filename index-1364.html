<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="This is a snapshot site for StackOverflow">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>StackOverflow Snapshot (old posts, page 1364) | StackOverflow Snapshot</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/index-1364.html">
<link rel="prev" href="index-1365.html" type="text/html">
<link rel="next" href="index-1363.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="archive.html">Archive</a>
                </li>
<li>
<a href="categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            

    


    
<div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/svm-and-neural-network/" class="u-url">SVM and Neural Network</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/svm-and-neural-network/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T02:33:43+08:00" itemprop="datePublished" title="2023-02-28 02:33">2023-02-28 02:33</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>What is difference between SVM and Neural Network? Is it true that linear svm
is same NN, and for non-linear separable problems, NN uses adding hidden
layers and SVM uses changing space dimensions?</p>
<p><br><br></p>
<h2>Answer</h2>
<p>There are two parts to this question. The first part is "what is the form of
function learned by these methods?" For NN and SVM this is typically the same.
For example, a single hidden layer neural network uses exactly the same form
of model as an SVM. That is:</p>
<p>Given an input vector x, the output is: output(x) = sum_over_all_i weight_i *
nonlinear_function_i(x)</p>
<p>Generally the nonlinear functions will also have some parameters. So these
methods need to learn how many nonlinear functions should be used, what their
parameters are, and what the value of all the weight_i weights should be.</p>
<p>Therefore, the difference between a SVM and a NN is in how they decide what
these parameters should be set to. Usually when someone says they are using a
neural network they mean they are trying to find the parameters which minimize
the mean squared prediction error with respect to a set of training examples.
They will also almost always be using the stochastic gradient descent
optimization algorithm to do this. SVM's on the other hand try to minimize
both training error and some measure of "hypothesis complexity". So they will
find a set of parameters that fits the data but also is "simple" in some
sense. You can think of it like Occam's razor for machine learning. The most
common optimization algorithm used with SVMs is sequential minimal
optimization.</p>
<p>Another big difference between the two methods is that stochastic gradient
descent isn't guaranteed to find the optimal set of parameters when used the
way NN implementations employ it. However, any decent SVM implementation is
going to find the optimal set of parameters. People like to say that neural
networks get stuck in a local minima while SVMs don't.</p>
<p><br></p>
<h3>Suggest</h3>
<p>NNs are heuristic, while SVMs are theoretically founded. A SVM is guaranteed
to converge towards the best solution in the PAC (probably approximately
correct) sense. For example, for two linearly separable classes SVM will draw
the separating hyperplane directly halfway between the nearest points of the
two classes (these become <em>support vectors</em> ). A neural network would draw any
line which separates the samples, which is correct for the training set, but
might not have the best generalization properties.</p>
<p>So no, even for linearly separable problems NNs and SVMs are not same.</p>
<p>In case of linearly non-separable classes, both SVMs and NNs apply non-linear
projection into higher-dimensional space. In the case of NNs this is achieved
by introducing additional neurons in the hidden layer(s). For SVMs, a <em>kernel
function</em> is used to the same effect. A neat property of the kernel function
is that the computational complexity doesn't rise with the number of
dimensions, while for NNs it obviously rises with the number of neurons.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/how-to-convert-the-output-of-an-artificial-neural-network-into-probabilities/" class="u-url">How to convert the output of an artificial neural network into probabilities?</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/how-to-convert-the-output-of-an-artificial-neural-network-into-probabilities/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T02:33:21+08:00" itemprop="datePublished" title="2023-02-28 02:33">2023-02-28 02:33</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>I've read about neural network a little while ago and I understand how an ANN
(especially a multilayer perceptron that learns via backpropagation) can learn
to classify an event as true or false.</p>
<p>I think there are two ways :</p>
<p>1) You get one output neuron. It it's value is &gt; 0.5 the events is likely
true, if it's value is &lt;=0.5 the event is likely to be false.</p>
<p>2) You get two output neurons, if the value of the first is &gt; than the value
of the second the event is likely true and vice versa.</p>
<p>In these case, the ANN tells you if an event is likely true or likely false.
It does not tell how likely it is.</p>
<p>Is there a way to convert this value to some odds or to directly get odds out
of the ANN. I'd like to get an output like "The event has a 84% probability to
be true"</p>
<p><br><br></p>
<h2>Answer</h2>
<p>Once a NN has been trained, for eg. using backprogation as mentioned in the
question (whereby the backprogation logic has "nudged" the weights in ways
that minimize the error function) the weights associated with all individual
inputs ("outside" inputs or intra-NN inputs) are fixed. The NN can then be
used for classifying purposes.</p>
<p>Whereby the math (and the "options") during the learning phase can get a bit
thick, it is relatively simple and straightfoward when operating as a
classifier. The main algorithm is to compute an activation value for each
neuron, as the sum of the input x weight for that neuron. This value is then
fed to an activation function which purpose's is to normalize it and convert
it to a boolean (in typical cases, as some networks do not have an all-or-
nothing rule for some of their layers). The activation function can be more
complex than you indicated, in particular it needn't be linear, but whatever
its shape, typically sigmoid, it operate in the same fashion: figuring out
where the activation fits on the curve, and if applicable, above or below a
threshold. The basic algorithm then processes all neurons at a given layer
before proceeding to the next.</p>
<p>With this in mind, the question of using the perceptron's ability to qualify
its guess (or indeed guesses - plural) with a percentage value, finds an easy
answer: you bet it can, its output(s) is real-valued (if anything in need of
normalizing) before we convert it to a discrete value (a boolean or a category
ID in the case of several categories), using the activation functions and the
threshold/comparison methods described in the question.</p>
<p>So... How and Where do I get "my percentages"?... All depends on the NN
implementation, and more importantly, the implementation dictates the type of
normalization functions that can be used to bring activation values in the 0-1
range <em>and</em> in a fashion that the sum of all percentages "add up" to 1. In its
simplest form, the activation function can be used to normalize the value and
the weights of the input to the output layer can be used as factors to ensure
the "add up" to 1 question (provided that these weights are indeed so
normalized themselves).</p>
<p>Et voilà!</p>
<p><strong>Claritication</strong> : (following Mathieu's note)<br>
One doesn't need to change anything in the way the Neural Network itself
works; the only thing needed is to somehow "hook into" the logic of <em>output</em>
neurons to access the [real-valued] activation value they computed, or,
possibly better, to access the real-valued output of the activation function,
<em>prior its boolean conversion</em> (which is typically based on a threshold value
or on some stochastic function).</p>
<p>In other words, the NN works as previously, neither its training nor
recognition logic are altered, the inputs to the NN stay the same, as do the
connections between various layers etc. We only get a copy of the real-valued
activation of the neurons in the output layer, and we use this to compute a
percentage. The actual formula for the percentage calculation depends on the
nature of the activation value and its associated function (its scale, its
range relative to other neurons' output etc.).<br>
Here are a few simple cases (taken from the question's suggested output rules)
1) If there is a single output neuron: the ratio of the value provided by the
activation function relative to the range of that function should do. 2) If
there are two (or more output neurons), as with classifiers for example: If
all output neurons have the same activation function, the percentage for a
given neuron is that of its activation function value divided by the sum of
all activation function values. If the activation functions vary, it becomes a
case by case situation because the distinct activation functions may be
indicative of a purposeful desire to give more weight to some of the neurons,
and the percentage should respect this.</p>
<p><br></p>
<h3>Suggest</h3>
<p>What you can do is to use a sigmoid transfer function on the output layer
nodes (that accepts data ranges (-inf,inf) and outputs a value in [-1,1]).<br>
Then by using the <strong>1-of-n output encoding</strong> (one node for each class), you
can map the range [-1,1] to [0,1] and use it as probability for each class
value (note that this works naturally for more than just two classes).</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/insert-or-delete-a-step-in-scikit-learn-pipeline/" class="u-url">Insert or delete a step in scikit-learn Pipeline</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/insert-or-delete-a-step-in-scikit-learn-pipeline/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T02:32:56+08:00" itemprop="datePublished" title="2023-02-28 02:32">2023-02-28 02:32</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>Is it possible to delete or insert a step in a <code>sklearn.pipeline.Pipeline</code>
object?</p>
<p>I am trying to do a grid search with or without one step in the Pipeline
object. And wondering whether I can insert or delete a step in the pipeline. I
saw in the <code>Pipeline</code> source code, there is a <code>self.steps</code> object holding all
the steps. We can get the steps by <code>named_steps()</code>. Before modifying it, I
want to make sure, I do not cause unexpected effects.</p>
<p>Here is a example code:</p>
<div class="code"><pre class="code literal-block"><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="n">estimators</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">'reduce_dim'</span><span class="p">,</span> <span class="n">PCA</span><span class="p">()),</span> <span class="p">(</span><span class="s1">'svm'</span><span class="p">,</span> <span class="n">SVC</span><span class="p">())]</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">estimators</span><span class="p">)</span>
<span class="n">clf</span>
</pre></div>

<p>Is it possible that we do something like <code>steps = clf.named_steps()</code>, then
insert or delete in this list? Does this cause undesired effect on the clf
object?</p>
<p><br><br></p>
<h2>Answer</h2>
<p>I see that everyone mentioned only the delete step. In case you want to also
insert a step in the pipeline:</p>
<div class="code"><pre class="code literal-block">pipe.steps.append(['step name',transformer()])
</pre></div>

<p><code>pipe.steps</code> works in the same way as lists do, so you can also insert an item
into a specific location:</p>
<div class="code"><pre class="code literal-block">pipe.steps.insert(1,['estimator',transformer()]) #insert as second step
</pre></div>

<p><br></p>
<h3>Suggest</h3>
<p>Based on rudimentary testing you can safely remove a step from a scikit-learn
pipeline just like you would any list item, with a simple</p>
<div class="code"><pre class="code literal-block">clf_pipeline.steps.pop(n)
</pre></div>

<p>where n is the position of the individual estimator you are trying to remove.</p>
    </div>
    </article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="index-1365.html" rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-1363.html" rel="next">Older posts</a>
            </li>
        </ul></nav>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow中文网</a>  
            
        </footer>
</div>
</div>


            <script src="assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="assets/js/search.js"></script>
</body>
</html>
