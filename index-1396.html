<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="This is a snapshot site for StackOverflow">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>StackOverflow Snapshot (old posts, page 1396) | StackOverflow Snapshot</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/index-1396.html">
<link rel="prev" href="index-1397.html" type="text/html">
<link rel="next" href="index-1395.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="archive.html">Archive</a>
                </li>
<li>
<a href="categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            

    


    
<div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/difference-between-neural-network-and-evolutionary-algorithm/" class="u-url">Difference between Neural Network and Evolutionary algorithm</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/difference-between-neural-network-and-evolutionary-algorithm/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:05:15+08:00" itemprop="datePublished" title="2023-02-28 03:05">2023-02-28 03:05</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>I have a good basis on Evolutionary Algorithms, so now i started to read about
Artificial Neural Networks. I come across this tutorial on http://www.ai-
junkie.com/ann/evolved/nnt2.html, showing how to use a ANN to evolve Tanks
that collect mines. It uses a GA to evolve the input weights on each Neuron.</p>
<p>I know i could use GA (without the ANN) to solve the same problem. I already
created a Tetris Bot using only GA to optimize the weights in the grid
evaluation function (check my blog
http://www.bitsrandomicos.blogspot.com.br/).</p>
<p>My question is: what's the conceptual/practical <strong>difference</strong> between using a
ANN + GA in a situation where i could use GA alone? I mean, is my Tetris Bot a
ANN?(I don't think so).</p>
<p>There are several related questions about this, but i couldn't find a answer:</p>
<p>Are evolutionary algorithms and neural networks used in the same domains?</p>
<p>When to use Genetic Algorithms vs. when to use Neural Networks?</p>
<p>Thanks!</p>
<p><br><br></p>
<h2>Answer</h2>
<p>A <strong>genetic algorithm</strong> is an <strong>optimization algorithm</strong>.</p>
<p>An <strong>artificial neural network</strong> is a <strong>function approximator</strong>. In order to
approximate a function you need an optimization algorithm to adjust the
weights. An ANN can be used for supervised learning (classification,
regression) or reinforcement learning and some can even be used for
unsupervised learning.</p>
<p>In supervised learning a derivative-free optimization algorithm like a genetic
algorithm is slower than most of the optimization algorithms that use gradient
information. Thus, it only makes sense to evolve neural networks with genetic
algorithms in reinforcement learning. This is known as "neuroevolution". The
advantage of neural networks like multilayer perceptrons in this setup is that
they can approximate any function with arbitrary precision when they have a
suffficient number of hidden nodes.</p>
<p>When you create a tetris bot you do not necessarily have to use an ANN as a
function approximator. But you need some kind of function approximator to
represent your bot's policy. I guess it was just simpler than an ANN. But when
you want to create a complex nonlinear policy you could do that e. g. with an
ANN.</p>
<p><br></p>
<h3>Suggest</h3>
<p>alfa's answer is perfect. Here is just an image to illustrate what he said:</p>
<p><img alt="enter image description here" src="images/M02TA.jpg"> Meta-Optimizer = None (but
could be)<br>
Optimizer = Genetic Algorithm<br>
Problem = Tetris Bot (e.g. ANN)</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/trouble-understanding-the-backpropagation-algorithm-in-neural-network/" class="u-url">Trouble Understanding the Backpropagation Algorithm in Neural Network</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/trouble-understanding-the-backpropagation-algorithm-in-neural-network/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:04:55+08:00" itemprop="datePublished" title="2023-02-28 03:04">2023-02-28 03:04</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>I'm having trouble understanding the backpropagation algorithm. I read a lot
and searched a lot but I can't understand why my Neural Network don't work. I
want to confirm that I'm doing every part the right way.</p>
<p>Here is my Neural Network when it is initialize and when the first line of
inputs [1, 1] and the output [0] is set (as you can see, I'm trying to do the
XOR Neural Network) :</p>
<p><img alt="My Neural Network" src="images/YrDp0.png"></p>
<p>I have 3 layers : input, hidden and output. The first layer (input) and the
hidden layer contains 2 neurons in which there is 2 synapses each. The last
layer (output) contains one neuron with 2 synapses too.</p>
<p>A synapse contains a weight and it’s previous delta (at the beginning, it is
0). The output connected to the synapse can be found with the sourceNeuron
associated with the synapse or in the inputs array if there is no sourceNeuron
(like in the input layer).</p>
<p>The class <strong>Layer.java</strong> contains a list of neurons. In my
<strong>NeuralNetwork.java</strong> , I initialize the Neural Network then I loop in my
training set. In each iteration, I replace the inputs and the output values
and call train on my BackPropagation Algorithm and the algorithm run certain
number of time (epoch of 1000 times for now) for the current set.</p>
<p>The <strong>activation</strong> fonction I use is the sigmoid.</p>
<p>Training set AND validation set is (input1, input2, output):</p>
<div class="code"><pre class="code literal-block"><span class="mf">1</span><span class="p">,</span><span class="mf">1</span><span class="p">,</span><span class="mf">0</span>
<span class="mf">0</span><span class="p">,</span><span class="mf">1</span><span class="p">,</span><span class="mf">1</span>
<span class="mf">1</span><span class="p">,</span><span class="mf">0</span><span class="p">,</span><span class="mf">1</span>
<span class="mf">0</span><span class="p">,</span><span class="mf">0</span><span class="p">,</span><span class="mf">0</span>
</pre></div>

<p>Here is my <strong>Neuron.java</strong> implementation:</p>
<div class="code"><pre class="code literal-block"><span class="k">public</span><span class="w"> </span><span class="k">class</span><span class="w"> </span><span class="n">Neuron</span><span class="w"> </span><span class="err">{</span>

<span class="w">    </span><span class="n">private</span><span class="w"> </span><span class="n">IActivation</span><span class="w"> </span><span class="n">activation</span><span class="p">;</span>
<span class="w">    </span><span class="n">private</span><span class="w"> </span><span class="n">ArrayList</span><span class="o">&lt;</span><span class="n">Synapse</span><span class="o">&gt;</span><span class="w"> </span><span class="n">synapses</span><span class="p">;</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="n">Inputs</span>
<span class="w">    </span><span class="n">private</span><span class="w"> </span><span class="k">double</span><span class="w"> </span><span class="k">output</span><span class="p">;</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="k">Output</span>
<span class="w">    </span><span class="n">private</span><span class="w"> </span><span class="k">double</span><span class="w"> </span><span class="n">errorToPropagate</span><span class="p">;</span>

<span class="w">    </span><span class="k">public</span><span class="w"> </span><span class="n">Neuron</span><span class="p">(</span><span class="n">IActivation</span><span class="w"> </span><span class="n">activation</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="n">this</span><span class="p">.</span><span class="n">activation</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">activation</span><span class="p">;</span>
<span class="w">        </span><span class="n">this</span><span class="p">.</span><span class="n">synapses</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">ArrayList</span><span class="o">&lt;</span><span class="n">Synapse</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">        </span><span class="n">this</span><span class="p">.</span><span class="k">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">        </span><span class="n">this</span><span class="p">.</span><span class="n">errorToPropagate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">    </span><span class="err">}</span>

<span class="w">    </span><span class="k">public</span><span class="w"> </span><span class="n">void</span><span class="w"> </span><span class="n">updateOutput</span><span class="p">(</span><span class="k">double</span><span class="err">[]</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="k">double</span><span class="w"> </span><span class="n">sumWeights</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">this</span><span class="p">.</span><span class="n">calculateSumWeights</span><span class="p">(</span><span class="n">inputs</span><span class="p">);</span>

<span class="w">        </span><span class="n">this</span><span class="p">.</span><span class="k">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">this</span><span class="p">.</span><span class="n">activation</span><span class="p">.</span><span class="n">activate</span><span class="p">(</span><span class="n">sumWeights</span><span class="p">);</span>
<span class="w">    </span><span class="err">}</span>

<span class="w">    </span><span class="k">public</span><span class="w"> </span><span class="k">double</span><span class="w"> </span><span class="n">calculateSumWeights</span><span class="p">(</span><span class="k">double</span><span class="err">[]</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="k">double</span><span class="w"> </span><span class="n">sumWeights</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">        </span><span class="nc">int</span><span class="w"> </span><span class="k">index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">Synapse</span><span class="w"> </span><span class="n">synapse</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">this</span><span class="p">.</span><span class="n">getSynapses</span><span class="p">())</span><span class="w"> </span><span class="err">{</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">inputs</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="k">null</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>
<span class="w">                </span><span class="n">sumWeights</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">synapse</span><span class="p">.</span><span class="n">getWeight</span><span class="p">()</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">inputs</span><span class="o">[</span><span class="n">index</span><span class="o">]</span><span class="p">;</span>
<span class="w">            </span><span class="err">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="err">{</span>
<span class="w">                </span><span class="n">sumWeights</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">synapse</span><span class="p">.</span><span class="n">getWeight</span><span class="p">()</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">synapse</span><span class="p">.</span><span class="n">getSourceNeuron</span><span class="p">().</span><span class="n">getOutput</span><span class="p">();</span>
<span class="w">            </span><span class="err">}</span>

<span class="w">            </span><span class="k">index</span><span class="o">++</span><span class="p">;</span>
<span class="w">        </span><span class="err">}</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">sumWeights</span><span class="p">;</span>
<span class="w">    </span><span class="err">}</span>

<span class="w">    </span><span class="k">public</span><span class="w"> </span><span class="k">double</span><span class="w"> </span><span class="n">getDerivative</span><span class="p">()</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">this</span><span class="p">.</span><span class="n">activation</span><span class="p">.</span><span class="n">derivative</span><span class="p">(</span><span class="n">this</span><span class="p">.</span><span class="k">output</span><span class="p">);</span>
<span class="w">    </span><span class="err">}</span>

<span class="w">    </span><span class="o">[</span><span class="n">...</span><span class="o">]</span>
<span class="err">}</span>
</pre></div>

<p>The <strong>Synapse.java</strong> contains:</p>
<div class="code"><pre class="code literal-block"><span class="nv">public</span><span class="w"> </span><span class="nv">Synapse</span><span class="ss">(</span><span class="nv">Neuron</span><span class="w"> </span><span class="nv">sourceNeuron</span><span class="ss">)</span><span class="w"> </span>{
<span class="w">    </span><span class="nv">this</span>.<span class="nv">sourceNeuron</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">sourceNeuron</span><span class="c1">;</span>
<span class="w">    </span><span class="k">Random</span><span class="w"> </span><span class="nv">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">new</span><span class="w"> </span><span class="k">Random</span><span class="ss">()</span><span class="c1">;</span>
<span class="w">    </span><span class="nv">this</span>.<span class="nv">weight</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="ss">(</span><span class="o">-</span><span class="mi">0</span>.<span class="mi">5</span><span class="ss">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="ss">(</span><span class="mi">0</span>.<span class="mi">5</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="ss">(</span><span class="o">-</span><span class="mi">0</span>.<span class="mi">5</span><span class="ss">))</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nv">r</span>.<span class="nv">nextDouble</span><span class="ss">()</span><span class="c1">;</span>
<span class="w">    </span><span class="nv">this</span>.<span class="nv">delta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="c1">;</span>
}

[...<span class="w"> </span><span class="nv">getter</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">setter</span><span class="w"> </span>...]
</pre></div>

<p>The <strong>train</strong> method in my class <strong>BackpropagationStrategy.java</strong> run a while
loop and stop after 1000 times (epoch) with one line of the training set. It
looks like this:</p>
<div class="code"><pre class="code literal-block">this.forwardPropagation(neuralNetwork, inputs);

this.backwardPropagation(neuralNetwork, expectedOutput);

this.updateWeights(neuralNetwork);
</pre></div>

<p>Here is all the implementation of the methods above (learningRate = 0.45 and
momentum = 0.9):</p>
<div class="code"><pre class="code literal-block"><span class="nv">public</span><span class="w"> </span><span class="nv">void</span><span class="w"> </span><span class="nv">forwardPropagation</span><span class="ss">(</span><span class="nv">NeuralNetwork</span><span class="w"> </span><span class="nv">neuralNetwork</span>,<span class="w"> </span><span class="nv">double</span>[]<span class="w"> </span><span class="nv">inputs</span><span class="ss">)</span><span class="w"> </span>{

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="ss">(</span><span class="nv">Layer</span><span class="w"> </span><span class="nv">layer</span><span class="w"> </span>:<span class="w"> </span><span class="nv">neuralNetwork</span>.<span class="nv">getLayers</span><span class="ss">())</span><span class="w"> </span>{

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="ss">(</span><span class="nv">Neuron</span><span class="w"> </span><span class="nv">neuron</span><span class="w"> </span>:<span class="w"> </span><span class="nv">layer</span>.<span class="nv">getNeurons</span><span class="ss">())</span><span class="w"> </span>{
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="ss">(</span><span class="nv">layer</span>.<span class="nv">isInput</span><span class="ss">())</span><span class="w"> </span>{
<span class="w">                </span><span class="nv">neuron</span>.<span class="nv">updateOutput</span><span class="ss">(</span><span class="nv">inputs</span><span class="ss">)</span><span class="c1">;</span>
<span class="w">            </span>}<span class="w"> </span><span class="k">else</span><span class="w"> </span>{
<span class="w">                </span><span class="nv">neuron</span>.<span class="nv">updateOutput</span><span class="ss">(</span><span class="nv">null</span><span class="ss">)</span><span class="c1">;</span>
<span class="w">            </span>}
<span class="w">        </span>}
<span class="w">    </span>}
}

<span class="nv">public</span><span class="w"> </span><span class="nv">void</span><span class="w"> </span><span class="nv">backwardPropagation</span><span class="ss">(</span><span class="nv">NeuralNetwork</span><span class="w"> </span><span class="nv">neuralNetwork</span>,<span class="w"> </span><span class="nv">double</span><span class="w"> </span><span class="nv">realOutput</span><span class="ss">)</span><span class="w"> </span>{

<span class="w">    </span><span class="nv">Layer</span><span class="w"> </span><span class="nv">lastLayer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">null</span><span class="c1">;</span>

<span class="w">    </span><span class="o">//</span><span class="w"> </span><span class="k">Loop</span><span class="w"> </span>à<span class="w"> </span><span class="nv">travers</span><span class="w"> </span><span class="nv">les</span><span class="w"> </span><span class="nv">hidden</span><span class="w"> </span><span class="nv">layers</span><span class="w"> </span><span class="nv">et</span><span class="w"> </span><span class="nv">le</span><span class="w"> </span><span class="nv">output</span><span class="w"> </span><span class="nv">layer</span><span class="w"> </span><span class="nv">uniquement</span>
<span class="w">    </span><span class="nv">ArrayList</span><span class="o">&lt;</span><span class="nv">Layer</span><span class="o">&gt;</span><span class="w"> </span><span class="nv">layers</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">neuralNetwork</span>.<span class="nv">getLayers</span><span class="ss">()</span><span class="c1">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="ss">(</span><span class="nv">int</span><span class="w"> </span><span class="nv">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">layers</span>.<span class="nv">size</span><span class="ss">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="c1">; i &gt; 0; i--) {</span>
<span class="w">        </span><span class="nv">Layer</span><span class="w"> </span><span class="nv">layer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">layers</span>.<span class="nv">get</span><span class="ss">(</span><span class="nv">i</span><span class="ss">)</span><span class="c1">;</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="ss">(</span><span class="nv">Neuron</span><span class="w"> </span><span class="nv">neuron</span><span class="w"> </span>:<span class="w"> </span><span class="nv">layer</span>.<span class="nv">getNeurons</span><span class="ss">())</span><span class="w"> </span>{

<span class="w">            </span><span class="nv">double</span><span class="w"> </span><span class="nv">errorToPropagate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">neuron</span>.<span class="nv">getDerivative</span><span class="ss">()</span><span class="c1">;</span>

<span class="w">            </span><span class="o">//</span><span class="w"> </span><span class="nv">Output</span><span class="w"> </span><span class="nv">layer</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="ss">(</span><span class="nv">layer</span>.<span class="nv">isOutput</span><span class="ss">())</span><span class="w"> </span>{

<span class="w">                </span><span class="nv">errorToPropagate</span><span class="w"> </span><span class="o">*=</span><span class="w"> </span><span class="ss">(</span><span class="nv">realOutput</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">neuron</span>.<span class="nv">getOutput</span><span class="ss">())</span><span class="c1">;</span>
<span class="w">            </span>}
<span class="w">            </span><span class="o">//</span><span class="w"> </span><span class="nv">Hidden</span><span class="w"> </span><span class="nv">layers</span>
<span class="w">            </span><span class="k">else</span><span class="w"> </span>{
<span class="w">                </span><span class="nv">double</span><span class="w"> </span><span class="nv">sumFromLastLayer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="c1">;</span>

<span class="w">                </span><span class="k">for</span><span class="w"> </span><span class="ss">(</span><span class="nv">Neuron</span><span class="w"> </span><span class="nv">lastLayerNeuron</span><span class="w"> </span>:<span class="w"> </span><span class="nv">lastLayer</span>.<span class="nv">getNeurons</span><span class="ss">())</span><span class="w"> </span>{
<span class="w">                    </span><span class="k">for</span><span class="w"> </span><span class="ss">(</span><span class="nv">Synapse</span><span class="w"> </span><span class="nv">synapse</span><span class="w"> </span>:<span class="w"> </span><span class="nv">lastLayerNeuron</span>.<span class="nv">getSynapses</span><span class="ss">())</span><span class="w"> </span>{
<span class="w">                        </span><span class="k">if</span><span class="w"> </span><span class="ss">(</span><span class="nv">synapse</span>.<span class="nv">getSourceNeuron</span><span class="ss">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nv">neuron</span><span class="ss">)</span><span class="w"> </span>{
<span class="w">                            </span><span class="nv">sumFromLastLayer</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="ss">(</span><span class="nv">synapse</span>.<span class="nv">getWeight</span><span class="ss">()</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nv">lastLayerNeuron</span>.<span class="nv">getErrorToPropagate</span><span class="ss">())</span><span class="c1">;</span>

<span class="w">                            </span><span class="k">break</span><span class="c1">;</span>
<span class="w">                        </span>}
<span class="w">                    </span>}
<span class="w">                </span>}

<span class="w">                </span><span class="nv">errorToPropagate</span><span class="w"> </span><span class="o">*=</span><span class="w"> </span><span class="nv">sumFromLastLayer</span><span class="c1">;</span>
<span class="w">            </span>}

<span class="w">            </span><span class="nv">neuron</span>.<span class="nv">setErrorToPropagate</span><span class="ss">(</span><span class="nv">errorToPropagate</span><span class="ss">)</span><span class="c1">;</span>
<span class="w">        </span>}

<span class="w">        </span><span class="nv">lastLayer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">layer</span><span class="c1">;</span>
<span class="w">    </span>}
}

<span class="nv">public</span><span class="w"> </span><span class="nv">void</span><span class="w"> </span><span class="nv">updateWeights</span><span class="ss">(</span><span class="nv">NeuralNetwork</span><span class="w"> </span><span class="nv">neuralNetwork</span><span class="ss">)</span><span class="w"> </span>{

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="ss">(</span><span class="nv">int</span><span class="w"> </span><span class="nv">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">neuralNetwork</span>.<span class="nv">getLayers</span><span class="ss">()</span>.<span class="nv">size</span><span class="ss">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="c1">; i &gt; 0; i--) {</span>

<span class="w">        </span><span class="nv">Layer</span><span class="w"> </span><span class="nv">layer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">neuralNetwork</span>.<span class="nv">getLayers</span><span class="ss">()</span>.<span class="nv">get</span><span class="ss">(</span><span class="nv">i</span><span class="ss">)</span><span class="c1">;</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="ss">(</span><span class="nv">Neuron</span><span class="w"> </span><span class="nv">neuron</span><span class="w"> </span>:<span class="w"> </span><span class="nv">layer</span>.<span class="nv">getNeurons</span><span class="ss">())</span><span class="w"> </span>{

<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="ss">(</span><span class="nv">Synapse</span><span class="w"> </span><span class="nv">synapse</span><span class="w"> </span>:<span class="w"> </span><span class="nv">neuron</span>.<span class="nv">getSynapses</span><span class="ss">())</span><span class="w"> </span>{

<span class="w">                </span><span class="nv">double</span><span class="w"> </span><span class="nv">delta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">this</span>.<span class="nv">learningRate</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nv">neuron</span>.<span class="nv">getError</span><span class="ss">()</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nv">synapse</span>.<span class="nv">getSourceNeuron</span><span class="ss">()</span>.<span class="nv">getOutput</span><span class="ss">()</span><span class="c1">;</span>

<span class="w">                </span><span class="nv">synapse</span>.<span class="nv">setWeight</span><span class="ss">(</span><span class="nv">synapse</span>.<span class="nv">getWeight</span><span class="ss">()</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">delta</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">this</span>.<span class="nv">momentum</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nv">synapse</span>.<span class="nv">getDelta</span><span class="ss">())</span><span class="c1">;</span>

<span class="w">                </span><span class="nv">synapse</span>.<span class="nv">setDelta</span><span class="ss">(</span><span class="nv">delta</span><span class="ss">)</span><span class="c1">;</span>
<span class="w">            </span>}
<span class="w">        </span>}
<span class="w">    </span>}
}
</pre></div>

<p>For the validation set, I only run this:</p>
<div class="code"><pre class="code literal-block">this.forwardPropagation(neuralNetwork, inputs);
</pre></div>

<p>And then check the output of the neuron in my output layer.</p>
<p>Did I do something wrong? Need some explanations...</p>
<p>Here are my results after 1000 epoch:</p>
<div class="code"><pre class="code literal-block"><span class="n">Real</span><span class="o">:</span><span class="w"> </span><span class="mf">0.0</span>
<span class="n">Current</span><span class="o">:</span><span class="w"> </span><span class="mf">0.025012156926937503</span>
<span class="n">Real</span><span class="o">:</span><span class="w"> </span><span class="mf">1.0</span>
<span class="n">Current</span><span class="o">:</span><span class="w"> </span><span class="mf">0.022566830709341495</span>
<span class="n">Real</span><span class="o">:</span><span class="w"> </span><span class="mf">1.0</span>
<span class="n">Current</span><span class="o">:</span><span class="w"> </span><span class="mf">0.02768416343491415</span>
<span class="n">Real</span><span class="o">:</span><span class="w"> </span><span class="mf">0.0</span>
<span class="n">Current</span><span class="o">:</span><span class="w"> </span><span class="mf">0.024903432706154027</span>
</pre></div>

<p>Why the synapses in the input layer are not updated? Everywhere it is written
to only update the hidden and output layers.</p>
<p>Like you can see, it is totally wrong! It doesn't go to the 1.0 only to the
first train set output (0.0).</p>
<p><strong>UPDATE 1</strong></p>
<p>Here is one iteration over the network with this set: [1.0,1.0,0.0]. Here is
the result for the forward propagation method:</p>
<div class="code"><pre class="code literal-block">=== Input Layer

== Neuron #1

= Synapse #1
Weight: -0.19283583155573614
Input: 1.0

= Synapse #2
Weight: 0.04023817185601586
Input: 1.0

Sum: -0.15259765969972028
Output: 0.461924442180935

== Neuron #2

= Synapse #1
Weight: -0.3281099260608612
Input: 1.0

= Synapse #2
Weight: -0.4388250065958519
Input: 1.0

Sum: -0.7669349326567131
Output: 0.31714251453174147

=== Hidden Layer

== Neuron #1

= Synapse #1
Weight: 0.16703288052854093
Input: 0.461924442180935

= Synapse #2
Weight: 0.31683996162148054
Input: 0.31714251453174147

Sum: 0.17763999229679783
Output: 0.5442935820534444

== Neuron #2

= Synapse #1
Weight: -0.45330313978424686
Input: 0.461924442180935

= Synapse #2
Weight: 0.3287014377113835
Input: 0.31714251453174147

Sum: -0.10514659949771789
Output: 0.47373754172497556

=== Output Layer

== Neuron #1

= Synapse #1
Weight: 0.08643751629154495
Input: 0.5442935820534444

= Synapse #2
Weight: -0.29715579267218695
Input: 0.47373754172497556

Sum: -0.09372646936373039
Output: 0.47658552081912403
</pre></div>

<p><strong>Update 2</strong></p>
<p>I probably have a bias problem. I will look into it with the help of this
answer: Role of Bias in Neural Networks. It doesn't shift back at the next
dataset so...</p>
<p><br><br></p>
<h2>Answer</h2>
<p>I finally found the problem. For the XOR, I didn't need any bias and it was
converging to the expected values. I got exactly the output when you round the
final output. What was needed is to train then validate, then train again
until the Neural Network is satisfaying. I was training each set until
satisfaction but not the WHOLE set again and again.</p>
<div class="code"><pre class="code literal-block"><span class="c1">// Initialize the Neural Network</span>
<span class="n">algorithm</span><span class="p">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">this</span><span class="p">.</span><span class="n">numberOfInputs</span><span class="p">);</span>

<span class="nb">int</span><span class="w"> </span><span class="n">index</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="nb">double</span><span class="w"> </span><span class="n">errorRate</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="c1">// Loop until satisfaction or after some iterations</span>
<span class="k">do</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Train the Neural Network</span>
<span class="w">    </span><span class="n">algorithm</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">this</span><span class="p">.</span><span class="n">trainingDataSets</span><span class="p">,</span><span class="w"> </span><span class="n">this</span><span class="p">.</span><span class="n">numberOfInputs</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Validate the Neural Network and return the error rate</span>
<span class="w">    </span><span class="n">errorRate</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">algorithm</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">this</span><span class="p">.</span><span class="n">validationDataSets</span><span class="p">,</span><span class="w"> </span><span class="n">this</span><span class="p">.</span><span class="n">numberOfInputs</span><span class="p">);</span>

<span class="w">    </span><span class="n">index</span><span class="o">++</span><span class="p">;</span>
<span class="p">}</span><span class="w"> </span><span class="k">while</span><span class="w"> </span><span class="p">(</span><span class="n">errorRate</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">minErrorRate</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">index</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">numberOfTrainValidateIteration</span><span class="p">);</span>
</pre></div>

<p>With the real datas, I need a bias because the outputs started to diverge.
Here is how I added the bias:</p>
<p>In <strong>Neuron.java</strong> class, I added a bias synapse with a weight and an output
of 1.0. I sum it with all the other synapses then put it in my activation
function.</p>
<div class="code"><pre class="code literal-block"><span class="nv">public</span><span class="w"> </span><span class="nv">class</span><span class="w"> </span><span class="nv">Neuron</span><span class="w"> </span><span class="nv">implements</span><span class="w"> </span><span class="nv">Serializable</span><span class="w"> </span>{

<span class="w">    </span>[...]

<span class="w">    </span><span class="nv">private</span><span class="w"> </span><span class="nv">Synapse</span><span class="w"> </span><span class="nv">bias</span><span class="c1">;</span>

<span class="w">    </span><span class="nv">public</span><span class="w"> </span><span class="nv">Neuron</span><span class="ss">(</span><span class="nv">IActivation</span><span class="w"> </span><span class="nv">activation</span><span class="ss">)</span><span class="w"> </span>{
<span class="w">        </span>[...]
<span class="w">        </span><span class="nv">this</span>.<span class="nv">bias</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">new</span><span class="w"> </span><span class="nv">Synapse</span><span class="ss">(</span><span class="nv">this</span><span class="ss">)</span><span class="c1">;</span>
<span class="w">        </span><span class="nv">this</span>.<span class="nv">bias</span>.<span class="nv">setWeight</span><span class="ss">(</span><span class="mi">0</span>.<span class="mi">5</span><span class="ss">)</span><span class="c1">; // Set initial weight OR keep the random number already set</span>
<span class="w">    </span>}

<span class="w">    </span><span class="nv">public</span><span class="w"> </span><span class="nv">void</span><span class="w"> </span><span class="nv">updateOutput</span><span class="ss">(</span><span class="nv">double</span>[]<span class="w"> </span><span class="nv">inputs</span><span class="ss">)</span><span class="w"> </span>{
<span class="w">        </span><span class="nv">double</span><span class="w"> </span><span class="nv">sumWeights</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">this</span>.<span class="nv">calculateSumWeights</span><span class="ss">(</span><span class="nv">inputs</span><span class="ss">)</span><span class="c1">;</span>

<span class="w">        </span><span class="nv">this</span>.<span class="nv">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">this</span>.<span class="nv">activation</span>.<span class="nv">activate</span><span class="ss">(</span><span class="nv">sumWeights</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">this</span>.<span class="nv">bias</span>.<span class="nv">getWeight</span><span class="ss">()</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">1</span>.<span class="mi">0</span><span class="ss">)</span><span class="c1">;</span>
<span class="w">    </span>}

<span class="w">    </span>[...]
</pre></div>

<p>In <strong>BackPropagationStrategy.java</strong> , I change the weight and the delta of
each bias in the updateWeights method that I renamed updateWeightsAndBias.</p>
<div class="code"><pre class="code literal-block"><span class="nv">public</span><span class="w"> </span><span class="nv">class</span><span class="w"> </span><span class="nv">BackPropagationStrategy</span><span class="w"> </span><span class="nv">implements</span><span class="w"> </span><span class="nv">IStrategy</span>,<span class="w"> </span><span class="nv">Serializable</span><span class="w"> </span>{

<span class="w">    </span>[...]

<span class="w">    </span><span class="nv">public</span><span class="w"> </span><span class="nv">void</span><span class="w"> </span><span class="nv">updateWeightsAndBias</span><span class="ss">(</span><span class="nv">NeuralNetwork</span><span class="w"> </span><span class="nv">neuralNetwork</span>,<span class="w"> </span><span class="nv">double</span>[]<span class="w"> </span><span class="nv">inputs</span><span class="ss">)</span><span class="w"> </span>{

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="ss">(</span><span class="nv">int</span><span class="w"> </span><span class="nv">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">neuralNetwork</span>.<span class="nv">getLayers</span><span class="ss">()</span>.<span class="nv">size</span><span class="ss">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="c1">; i &gt;= 0; i--) {</span>

<span class="w">            </span><span class="nv">Layer</span><span class="w"> </span><span class="nv">layer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">neuralNetwork</span>.<span class="nv">getLayers</span><span class="ss">()</span>.<span class="nv">get</span><span class="ss">(</span><span class="nv">i</span><span class="ss">)</span><span class="c1">;</span>

<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="ss">(</span><span class="nv">Neuron</span><span class="w"> </span><span class="nv">neuron</span><span class="w"> </span>:<span class="w"> </span><span class="nv">layer</span>.<span class="nv">getNeurons</span><span class="ss">())</span><span class="w"> </span>{

<span class="w">                </span>[...]

<span class="w">                </span><span class="nv">Synapse</span><span class="w"> </span><span class="nv">bias</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">neuron</span>.<span class="nv">getBias</span><span class="ss">()</span><span class="c1">;</span>
<span class="w">                </span><span class="nv">double</span><span class="w"> </span><span class="nv">delta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">learning</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">1</span>.<span class="mi">0</span><span class="c1">;</span>
<span class="w">                </span><span class="nv">bias</span>.<span class="nv">setWeight</span><span class="ss">(</span><span class="nv">bias</span>.<span class="nv">getWeight</span><span class="ss">()</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">delta</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">this</span>.<span class="nv">momentum</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nv">bias</span>.<span class="nv">getDelta</span><span class="ss">())</span><span class="c1">;</span>

<span class="w">                </span><span class="nv">bias</span>.<span class="nv">setDelta</span><span class="ss">(</span><span class="nv">delta</span><span class="ss">)</span><span class="c1">;</span>
<span class="w">            </span>}
<span class="w">        </span>}
<span class="w">    </span>}

<span class="w">    </span>[...]
</pre></div>

<p>With the real datas, the Network is converging. It is now a pruning job to
find the perfect variables combo (if it is possible) of learning rate,
momentum, error rate, quantity of neurons, quantity of hidden layers, etc.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/incorporating-user-feedback-in-a-ml-model/" class="u-url">Incorporating user feedback in a ML model</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="posts/incorporating-user-feedback-in-a-ml-model/" rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:04:35+08:00" itemprop="datePublished" title="2023-02-28 03:04">2023-02-28 03:04</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>I have developed a ML model for a classification (0/1) NLP task and deployed
it in production environment. The prediction of the model is displayed to
users, and the users have the option to give a feedback (if the prediction was
right/wrong).</p>
<p>How can I continuously incorporate this feedback in my model ? From a UX stand
point you dont want a user to correct/teach the system more than twice/thrice
for a specific input, system shld learn fast i.e. so the feedback shld be
incorporated "fast". (Google priority inbox does this in a seamless way)</p>
<p>How does one build this "feedback loop" using which my system can improve ? I
have searched a lot on net but could not find relevant material. any pointers
will be of great help.</p>
<p>Pls dont say retrain the model from scratch by including new data points.
Thats surely not how google and facebook build their smart systems</p>
<p>To further explain my question - think of google's spam detector or their
priority inbox or their recent feature of "smart replies". Its a well known
fact that they have the ability to learn / incorporate (fast) user feed.</p>
<p><strong>All the while when it incorporates the user feedback fast (i.e. user has to
teach the system correct output atmost 2-3 times per data point and the system
start to give correct output for that data point) AND it also ensure it
maintains old learnings and does not start to give wrong outputs on older data
points (where it was giving right output earlier) while incorporating the
learning from new data point.</strong></p>
<p><em>I have not found any blog/literature/discussion w.r.t how to build such
systems - An intelligent system that explains in detaieedback loop" in ML
systems</em></p>
<p>Hope my question is little more clear now.</p>
<p>Update: Some related questions I found are:</p>
<ul>
<li>
<p>Does the SVM in sklearn support incremental (online) learning?</p>
</li>
<li>
<p>https://datascience.stackexchange.com/questions/1073/libraries-for-online-machine-learning</p>
</li>
<li>
<p>http://mlwave.com/predicting-click-through-rates-with-online-machine-learning/</p>
</li>
<li>
<p>https://en.wikipedia.org/wiki/Concept_drift</p>
</li>
</ul>
<p>Update: I still dont have a concrete answer but such a recipe does exists.
Read the section "Learning from the feedback" in the following blog Machine
Learning != Learning Machine. In this Jean talks about "adding a feedback
ingestion loop to machine". Same in here, here, here4.</p>
<p><br><br></p>
<h2>Answer</h2>
<p>Build a simple, light model(s) that can be updated per feedback. Online
Machine learning gives a number of candidates for this</p>
<p>Most good online classifiers are linear. In which case we can have a couple of
them and achieve non-linearity by combining them via a small shallow neural
net</p>
<p>https://stats.stackexchange.com/questions/126546/nonlinear-dynamic-online-
classification-looking-for-an-algorithm</p>
    </div>
    </article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="index-1397.html" rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-1395.html" rel="next">Older posts</a>
            </li>
        </ul></nav>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow-ZH</a>  
            
        </footer>
</div>
</div>


            <script src="assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="assets/js/search.js"></script>
</body>
</html>
