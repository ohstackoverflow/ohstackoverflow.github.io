<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>PyTorch Binary Classification - same network structure, 'simpler' data, but worse performance? | StackOverflow Snapshot</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/posts/pytorch-binary-classification-same-network-structure-simpler-data-but-worse-performance/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Arya">
<link rel="prev" href="../difference-between-neural-network-and-evolutionary-algorithm/" title="Difference between Neural Network and Evolutionary algorithm" type="text/html">
<link rel="next" href="../robot-programming-with-lisp/" title="robot programming with lisp?" type="text/html">
<meta property="og:site_name" content="StackOverflow Snapshot">
<meta property="og:title" content="PyTorch Binary Classification - same network structure, 'simpler' data">
<meta property="og:url" content="https://ohstackoverflow.netlify.app/posts/pytorch-binary-classification-same-network-structure-simpler-data-but-worse-performance/">
<meta property="og:description" content="To get to grips with PyTorch (and deep learning in general) I started by
working through some basic classification examples. One such example was
classifying a non-linear dataset created using sklearn">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2023-02-28T03:05:39+08:00">
<meta property="article:tag" content="artificial-intelligence">
<meta property="article:tag" content="deep-learning">
<meta property="article:tag" content="machine-learning">
<meta property="article:tag" content="python">
<meta property="article:tag" content="pytorch">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../archive.html">Archive</a>
                </li>
<li>
<a href="../../categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">PyTorch Binary Classification - same network structure, 'simpler' data, but worse performance?</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    <a class="u-url" href="../../authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:05:39+08:00" itemprop="datePublished" title="2023-02-28 03:05">2023-02-28 03:05</time></a>
            </p>
            

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>To get to grips with PyTorch (and deep learning in general) I started by
working through some basic classification examples. One such example was
classifying a non-linear dataset created using sklearn (full code available as
notebook here)</p>
<div class="code"><pre class="code literal-block">n_pts = 500
X, y = datasets.make_circles(n_samples=n_pts, random_state=123, noise=0.1, factor=0.2)
x_data = torch.FloatTensor(X)
y_data = torch.FloatTensor(y.reshape(500, 1))
</pre></div>

<p><img alt="enter image description here" src="../../images/w20Tb.png"></p>
<p>This is then accurately classified using a pretty basic neural net</p>
<div class="code"><pre class="code literal-block"><span class="k">class</span> <span class="n">Model</span>(<span class="n">nn</span>.<span class="n">Module</span>):
    <span class="n">def</span> <span class="n">__init__</span>(<span class="nb">self</span>, <span class="n">input_size</span>, <span class="n">H1</span>, <span class="n">output_size</span>):
        <span class="n">super</span>().<span class="n">__init__</span>()
        <span class="nb">self</span>.<span class="n">linear</span> = <span class="n">nn</span>.<span class="n">Linear</span>(<span class="n">input_size</span>, <span class="n">H1</span>)
        <span class="nb">self</span>.<span class="n">linear2</span> = <span class="n">nn</span>.<span class="n">Linear</span>(<span class="n">H1</span>, <span class="n">output_size</span>)

    <span class="n">def</span> <span class="n">forward</span>(<span class="nb">self</span>, <span class="nb">x</span>):
        <span class="nb">x</span> = <span class="n">torch</span>.<span class="n">sigmoid</span>(<span class="nb">self</span>.<span class="n">linear</span>(<span class="nb">x</span>))
        <span class="nb">x</span> = <span class="n">torch</span>.<span class="n">sigmoid</span>(<span class="nb">self</span>.<span class="n">linear2</span>(<span class="nb">x</span>))
        <span class="k">return</span> <span class="nb">x</span>

    <span class="n">def</span> <span class="n">predict</span>(<span class="nb">self</span>, <span class="nb">x</span>):
        <span class="nb">pred</span> = <span class="nb">self</span>.<span class="n">forward</span>(<span class="nb">x</span>)
        <span class="k">if</span> <span class="nb">pred</span> &gt;= <span class="mf">0.5</span>:
            <span class="k">return</span> <span class="mi">1</span>
        <span class="n">else:</span>
            <span class="k">return</span> <span class="mi">0</span>
</pre></div>

<p>As I have an interest in health data I then decided to try and use the same
network structure to classify some a basic real-world dataset. I took heart
rate data for one patient from here, and altered it so all values &gt; 91 would
be labelled as anomalies (e.g. a <code>1</code> and everything &lt;= 91 labelled a <code>0</code>).
This is completely arbitrary, but I just wanted to see how the classification
would work. The complete notebook for this example is here.</p>
<p><img alt="enter image description here" src="../../images/6izzI.png"></p>
<p>What is not intuitive to me is why the first example reaches <strong>a loss of
0.0016 after 1,000 epochs</strong> , whereas the second example only reaches <strong>a loss
of 0.4296 after 10,000 epochs</strong></p>
<p><img alt="Training Loss for Example 1" src="../../images/rDag4.png"></p>
<p><img alt="Training Loss for Heart Rate Example" src="../../images/PMubT.png"></p>
<p>Perhaps I am being naive in thinking that the heart rate example would be much
easier to classify. Any insights to help me understand why this is not what I
am seeing would be great!</p>
<p><br><br></p>
<h2>Answer</h2>
<h4>TL;DR</h4>
<p>Your input data is not normalized.</p>
<ol>
<li>use <code>x_data = (x_data - x_data.mean()) / x_data.std()</code>
</li>
<li>increase the learning rate <code>optimizer = torch.optim.Adam(model.parameters(), lr=0.01)</code>
</li>
</ol>
<p>You'll get<br><img alt="enter image description here" src="../../images/vt4VK.png"></p>
<p>convergence in only 1000 iterations.</p>
<h4>More details</h4>
<p>The key difference between the two examples you have is that the data <code>x</code> in
the first example is centered around (0, 0) and has very low variance.<br>
On the other hand, the data in the second example is centered around 92 and
has relatively large variance.</p>
<p>This initial bias in the data is not taken into account when you randomly
initialize the weights which is done based on the assumption that the inputs
are roughly normally distributed around <em>zero</em>.<br>
It is almost impossible for the optimization process to compensate for this
gross deviation - thus the model gets stuck in a sub-optimal solution.</p>
<p>Once you normalize the inputs, by subtracting the mean and dividing by the
std, the optimization process becomes stable again and rapidly converges to a
good solution.</p>
<p>For more details about input normalization and weights initialization, you can
read section 2.2 in <em>He et al</em> <strong>Delving Deep into Rectifiers: Surpassing
Human-Level Performance on ImageNet Classification</strong> (ICCV 2015).</p>
<h4>What if I cannot normalize the data?</h4>
<p>If, for some reason, you cannot compute mean and std data in advance, you can
still use <code>nn.BatchNorm1d</code> to estimate and normalize the data as part of the
training process. For example</p>
<div class="code"><pre class="code literal-block"><span class="k">class</span> <span class="n">Model</span>(<span class="n">nn</span>.<span class="n">Module</span>):
    <span class="n">def</span> <span class="n">__init__</span>(<span class="nb">self</span>, <span class="n">input_size</span>, <span class="n">H1</span>, <span class="n">output_size</span>):
        <span class="n">super</span>().<span class="n">__init__</span>()
        <span class="nb">self</span>.<span class="n">bn</span> = <span class="n">nn</span>.<span class="n">BatchNorm1d</span>(<span class="n">input_size</span>)  <span class="c1"># adding batchnorm</span>
        <span class="nb">self</span>.<span class="n">linear</span> = <span class="n">nn</span>.<span class="n">Linear</span>(<span class="n">input_size</span>, <span class="n">H1</span>)
        <span class="nb">self</span>.<span class="n">linear2</span> = <span class="n">nn</span>.<span class="n">Linear</span>(<span class="n">H1</span>, <span class="n">output_size</span>)

    <span class="n">def</span> <span class="n">forward</span>(<span class="nb">self</span>, <span class="nb">x</span>):
        <span class="nb">x</span> = <span class="n">torch</span>.<span class="n">sigmoid</span>(<span class="nb">self</span>.<span class="n">linear</span>(<span class="nb">self</span>.<span class="n">bn</span>(<span class="nb">x</span>)))  <span class="c1"># batchnorm the input x</span>
        <span class="nb">x</span> = <span class="n">torch</span>.<span class="n">sigmoid</span>(<span class="nb">self</span>.<span class="n">linear2</span>(<span class="nb">x</span>))
        <span class="k">return</span> <span class="nb">x</span>
</pre></div>

<p>This modification <em>without</em> any change to the input data, yields similar
convergance after only 1000 epochs:<br><img alt="enter image description here" src="../../images/Q9XRw.png"></p>
<h4>A minor comment</h4>
<p>For numerical stability, it is better to use <code>nn.BCEWithLogitsLoss</code> instead of
<code>nn.BCELoss</code>. For this end, you need to remove the <code>torch.sigmoid</code> from the
<code>forward()</code> output, the <code>sigmoid</code> will be computed inside the loss.<br>
See, for example, this thread regarding the related sigmoid + cross entropy
loss for binary predictions.</p>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/artificial-intelligence/" rel="tag">artificial-intelligence</a></li>
            <li><a class="tag p-category" href="../../categories/deep-learning/" rel="tag">deep-learning</a></li>
            <li><a class="tag p-category" href="../../categories/machine-learning/" rel="tag">machine-learning</a></li>
            <li><a class="tag p-category" href="../../categories/python/" rel="tag">python</a></li>
            <li><a class="tag p-category" href="../../categories/pytorch/" rel="tag">pytorch</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../difference-between-neural-network-and-evolutionary-algorithm/" rel="prev" title="Difference between Neural Network and Evolutionary algorithm">Previous post</a>
            </li>
            <li class="next">
                <a href="../robot-programming-with-lisp/" rel="next" title="robot programming with lisp?">Next post</a>
            </li>
        </ul></nav></aside></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow中文网</a>  
            
        </footer>
</div>
</div>


            <script src="../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="../../assets/js/search.js"></script>
</body>
</html>
