<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Neural Network Back-Propagation Algorithm Gets Stuck on XOR Training PAttern | StackOverflow Snapshot</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/posts/neural-network-back-propagation-algorithm-gets-stuck-on-xor-training-pattern/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Arya">
<link rel="prev" href="../what-does-train-on-batch-do-in-keras-model/" title="What does train_on_batch() do in keras model?" type="text/html">
<link rel="next" href="../heuristic-to-identify-if-a-series-of-4-bytes-chunks-of-data-are-integers-or-floats/" title="Heuristic to identify if a series of 4 bytes chunks of data are integers or floats" type="text/html">
<meta property="og:site_name" content="StackOverflow Snapshot">
<meta property="og:title" content="Neural Network Back-Propagation Algorithm Gets Stuck on XOR Training P">
<meta property="og:url" content="https://ohstackoverflow.netlify.app/posts/neural-network-back-propagation-algorithm-gets-stuck-on-xor-training-pattern/">
<meta property="og:description" content="Overview
So I'm trying to get a grasp on the mechanics of neural networks. I still
don't totally grasp the math behind it, but I think I understand how to
implement it. I currently have a neural net t">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2023-02-28T03:26:06+08:00">
<meta property="article:tag" content="algorithm">
<meta property="article:tag" content="artificial-intelligence">
<meta property="article:tag" content="java">
<meta property="article:tag" content="machine-learning">
<meta property="article:tag" content="neural-network">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../archive.html">Archive</a>
                </li>
<li>
<a href="../../categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Neural Network Back-Propagation Algorithm Gets Stuck on XOR Training PAttern</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    <a class="u-url" href="../../authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:26:06+08:00" itemprop="datePublished" title="2023-02-28 03:26">2023-02-28 03:26</time></a>
            </p>
            

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p><strong>Overview</strong></p>
<p>So I'm trying to get a grasp on the mechanics of neural networks. I still
don't totally grasp the math behind it, but I think I understand how to
implement it. I currently have a neural net that can learn AND, OR, and NOR
training patterns. However, I can't seem to get it to implement the XOR
pattern. My <strong>feed forward</strong> neural network consists of <strong>2 inputs, 3 hidden,
and 1 output.</strong> The weights and biases are randomly set between <strong>-0.5 and
0.5</strong> , and outputs are generated with the <strong>sigmoidal activation function</strong></p>
<p><strong>Algorithm</strong></p>
<p>So far, I'm guessing I made a mistake in my training algorithm which is
described below:</p>
<ol>
<li>For each neuron in the output layer, provide an <code>error</code> value that is the <code>desiredOutput - actualOutput</code> -- <em>go to step 3</em>
</li>
<li>For each neuron in a hidden or input layer (working backwards) provide an <code>error</code> value that is the sum of all <code>forward connection weights * the errorGradient of the neuron at the other end of the connection</code> -- <em>go to step 3</em>
</li>
<li>For each neuron, using the <code>error</code> value provided, generate an <code>error gradient</code> that equals <code>output * (1-output) * error</code>. -- <em>go to step 4</em>
</li>
<li>For each neuron, adjust the bias to equal <code>current bias + LEARNING_RATE * errorGradient</code>. Then adjust each backward connection's weight to equal <code>current weight + LEARNING_RATE * output of neuron at other end of connection * this neuron's errorGradient</code>
</li>
</ol>
<p>I'm training my neural net online, so this runs after each training sample.</p>
<p><strong>Code</strong></p>
<p>This is the main code that runs the neural network:</p>
<div class="code"><pre class="code literal-block"><span class="n">private</span><span class="w"> </span><span class="n">void</span><span class="w"> </span><span class="n">simulate</span><span class="p">(</span><span class="k">double</span><span class="w"> </span><span class="n">maximumError</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>

<span class="w">    </span><span class="nc">int</span><span class="w"> </span><span class="n">errorRepeatCount</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">    </span><span class="k">double</span><span class="w"> </span><span class="n">prevError</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">    </span><span class="k">double</span><span class="w"> </span><span class="n">error</span><span class="p">;</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="n">summed</span><span class="w"> </span><span class="n">squares</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">errors</span>
<span class="w">    </span><span class="nc">int</span><span class="w"> </span><span class="n">trialCount</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">    </span><span class="n">do</span><span class="w"> </span><span class="err">{</span>

<span class="w">        </span><span class="n">error</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="n">loop</span><span class="w"> </span><span class="n">through</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="k">set</span>
<span class="w">        </span><span class="k">for</span><span class="p">(</span><span class="nc">int</span><span class="w"> </span><span class="k">index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="k">index</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="k">Parameters</span><span class="p">.</span><span class="n">INPUT_TRAINING_SET</span><span class="p">.</span><span class="n">length</span><span class="p">;</span><span class="w"> </span><span class="k">index</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>

<span class="w">            </span><span class="k">double</span><span class="err">[]</span><span class="w"> </span><span class="n">currentInput</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">Parameters</span><span class="p">.</span><span class="n">INPUT_TRAINING_SET</span><span class="o">[</span><span class="n">index</span><span class="o">]</span><span class="p">;</span>
<span class="w">            </span><span class="k">double</span><span class="err">[]</span><span class="w"> </span><span class="n">expectedOutput</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">Parameters</span><span class="p">.</span><span class="n">OUTPUT_TRAINING_SET</span><span class="o">[</span><span class="n">index</span><span class="o">]</span><span class="p">;</span>
<span class="w">            </span><span class="k">double</span><span class="err">[]</span><span class="w"> </span><span class="k">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">getOutput</span><span class="p">(</span><span class="n">currentInput</span><span class="p">);</span>

<span class="w">            </span><span class="n">train</span><span class="p">(</span><span class="n">expectedOutput</span><span class="p">);</span>

<span class="w">            </span><span class="o">//</span><span class="w"> </span><span class="n">Subtracts</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">expected</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">actual</span><span class="w"> </span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">gets</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">average</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">those</span><span class="w"> </span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="n">squares</span><span class="w"> </span><span class="n">it</span><span class="p">.</span>
<span class="w">            </span><span class="n">error</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">Math</span><span class="p">.</span><span class="n">pow</span><span class="p">(</span><span class="n">getAverage</span><span class="p">(</span><span class="n">subtractArray</span><span class="p">(</span><span class="k">output</span><span class="p">,</span><span class="w"> </span><span class="n">expectedOutput</span><span class="p">)),</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span>



<span class="w">        </span><span class="err">}</span>

<span class="w">    </span><span class="err">}</span><span class="w"> </span><span class="k">while</span><span class="p">(</span><span class="n">error</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">maximumError</span><span class="p">);</span>
</pre></div>

<p>Now the <code>train()</code> function:</p>
<div class="code"><pre class="code literal-block"><span class="k">public</span><span class="w"> </span><span class="n">void</span><span class="w"> </span><span class="n">train</span><span class="p">(</span><span class="k">double</span><span class="err">[]</span><span class="w"> </span><span class="n">expected</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>

<span class="w">    </span><span class="n">layers</span><span class="p">.</span><span class="n">outputLayer</span><span class="p">().</span><span class="n">calculateErrors</span><span class="p">(</span><span class="n">expected</span><span class="p">);</span>

<span class="w">    </span><span class="k">for</span><span class="p">(</span><span class="nc">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">Parameters</span><span class="p">.</span><span class="n">NUM_HIDDEN_LAYERS</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">--</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="n">layers</span><span class="p">.</span><span class="n">allLayers</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">.</span><span class="n">calculateErrors</span><span class="p">();</span>
<span class="w">    </span><span class="err">}</span>

<span class="err">}</span>
</pre></div>

<p>Output layer <code>calculateErrors()</code> function:</p>
<div class="code"><pre class="code literal-block"><span class="k">public</span><span class="w"> </span><span class="n">void</span><span class="w"> </span><span class="n">calculateErrors</span><span class="p">(</span><span class="k">double</span><span class="err">[]</span><span class="w"> </span><span class="n">expectedOutput</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>

<span class="w">    </span><span class="k">for</span><span class="p">(</span><span class="nc">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">numNeurons</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>

<span class="w">        </span><span class="n">Neuron</span><span class="w"> </span><span class="n">neuron</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">neurons</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">;</span>
<span class="w">        </span><span class="k">double</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">expectedOutput</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">neuron</span><span class="p">.</span><span class="n">getOutput</span><span class="p">();</span>
<span class="w">        </span><span class="n">neuron</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">error</span><span class="p">);</span>

<span class="w">    </span><span class="err">}</span>

<span class="err">}</span>
</pre></div>

<p>Normal (Hidden &amp; Input) layer <code>calculateErrors()</code> function:</p>
<div class="code"><pre class="code literal-block"><span class="k">public</span><span class="w"> </span><span class="n">void</span><span class="w"> </span><span class="n">calculateErrors</span><span class="p">()</span><span class="w"> </span><span class="err">{</span>

<span class="w">    </span><span class="k">for</span><span class="p">(</span><span class="nc">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">neurons</span><span class="p">.</span><span class="n">length</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>

<span class="w">        </span><span class="n">Neuron</span><span class="w"> </span><span class="n">neuron</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">neurons</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">;</span>

<span class="w">        </span><span class="k">double</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">        </span><span class="k">for</span><span class="p">(</span><span class="k">Connection</span><span class="w"> </span><span class="k">connection</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">neuron</span><span class="p">.</span><span class="n">forwardConnections</span><span class="p">)</span><span class="w"> </span><span class="err">{</span>

<span class="w">            </span><span class="n">error</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="k">connection</span><span class="p">.</span><span class="k">output</span><span class="p">.</span><span class="n">errorGradient</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">connection</span><span class="p">.</span><span class="n">weight</span><span class="p">;</span>

<span class="w">        </span><span class="err">}</span>

<span class="w">        </span><span class="n">neuron</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">error</span><span class="p">);</span>

<span class="w">    </span><span class="err">}</span>

<span class="err">}</span>
</pre></div>

<p>Full Neuron class:</p>
<div class="code"><pre class="code literal-block"><span class="n">package</span> <span class="n">neuralNet</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">neurons</span><span class="p">;</span>

<span class="kn">import</span> <span class="nn">java.util.ArrayList</span><span class="p">;</span>
<span class="kn">import</span> <span class="nn">java.util.List</span><span class="p">;</span>
<span class="kn">import</span> <span class="nn">java.util.Random</span><span class="p">;</span>

<span class="kn">import</span> <span class="nn">neuralNet.Parameters</span><span class="p">;</span>
<span class="kn">import</span> <span class="nn">neuralNet.layers.NeuronLayer</span><span class="p">;</span>

<span class="n">public</span> <span class="k">class</span> <span class="nc">Neuron</span> <span class="p">{</span>

<span class="n">private</span> <span class="n">double</span> <span class="n">output</span><span class="p">,</span> <span class="n">bias</span><span class="p">;</span>
<span class="n">public</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">Connection</span><span class="o">&gt;</span> <span class="n">forwardConnections</span> <span class="o">=</span> <span class="n">new</span> <span class="n">ArrayList</span><span class="o">&lt;</span><span class="n">Connection</span><span class="o">&gt;</span><span class="p">();</span> <span class="o">//</span> <span class="n">Forward</span> <span class="o">=</span> <span class="n">layer</span> <span class="n">closer</span> <span class="n">to</span> <span class="nb">input</span> <span class="o">-&gt;</span> <span class="n">layer</span> <span class="n">closer</span> <span class="n">to</span> <span class="n">output</span>
<span class="n">public</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">Connection</span><span class="o">&gt;</span> <span class="n">backwardConnections</span> <span class="o">=</span> <span class="n">new</span> <span class="n">ArrayList</span><span class="o">&lt;</span><span class="n">Connection</span><span class="o">&gt;</span><span class="p">();</span> <span class="o">//</span> <span class="n">Backward</span> <span class="o">=</span> <span class="n">layer</span> <span class="n">closer</span> <span class="n">to</span> <span class="n">output</span> <span class="o">-&gt;</span> <span class="n">layer</span> <span class="n">closer</span> <span class="n">to</span> <span class="nb">input</span>

<span class="n">public</span> <span class="n">double</span> <span class="n">errorGradient</span><span class="p">;</span>
<span class="n">public</span> <span class="n">Neuron</span><span class="p">()</span> <span class="p">{</span>

    <span class="n">Random</span> <span class="n">random</span> <span class="o">=</span> <span class="n">new</span> <span class="n">Random</span><span class="p">();</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">nextDouble</span><span class="p">()</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">;</span>

<span class="p">}</span>

<span class="n">public</span> <span class="n">void</span> <span class="n">addConnections</span><span class="p">(</span><span class="n">NeuronLayer</span> <span class="n">prevLayer</span><span class="p">)</span> <span class="p">{</span>

    <span class="o">//</span> <span class="n">This</span> <span class="ow">is</span> <span class="n">true</span> <span class="k">for</span> <span class="nb">input</span> <span class="n">layers</span><span class="o">.</span> <span class="n">They</span> <span class="n">create</span> <span class="n">their</span> <span class="n">connections</span> <span class="n">differently</span><span class="o">.</span> <span class="p">(</span><span class="n">See</span> <span class="n">InputLayer</span> <span class="n">class</span><span class="p">)</span>
    <span class="k">if</span><span class="p">(</span><span class="n">prevLayer</span> <span class="o">==</span> <span class="n">null</span><span class="p">)</span> <span class="k">return</span><span class="p">;</span>

    <span class="k">for</span><span class="p">(</span><span class="n">Neuron</span> <span class="n">neuron</span> <span class="p">:</span> <span class="n">prevLayer</span><span class="o">.</span><span class="n">neurons</span><span class="p">)</span> <span class="p">{</span>

        <span class="n">Connection</span><span class="o">.</span><span class="n">createConnection</span><span class="p">(</span><span class="n">neuron</span><span class="p">,</span> <span class="n">this</span><span class="p">);</span>

    <span class="p">}</span>

<span class="p">}</span>

<span class="n">public</span> <span class="n">void</span> <span class="n">calcOutput</span><span class="p">()</span> <span class="p">{</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">bias</span><span class="p">;</span>

    <span class="k">for</span><span class="p">(</span><span class="n">Connection</span> <span class="n">connection</span> <span class="p">:</span> <span class="n">backwardConnections</span><span class="p">)</span> <span class="p">{</span>

        <span class="n">connection</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">calcOutput</span><span class="p">();</span>
        <span class="n">output</span> <span class="o">+=</span> <span class="n">connection</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">getOutput</span><span class="p">()</span> <span class="o">*</span> <span class="n">connection</span><span class="o">.</span><span class="n">weight</span><span class="p">;</span>

    <span class="p">}</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">output</span><span class="p">);</span>

<span class="p">}</span>

<span class="n">private</span> <span class="n">double</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">double</span> <span class="n">output</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">Math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">output</span><span class="p">));</span>
<span class="p">}</span>

<span class="n">public</span> <span class="n">double</span> <span class="n">getOutput</span><span class="p">()</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">public</span> <span class="n">void</span> <span class="n">train</span><span class="p">(</span><span class="n">double</span> <span class="n">error</span><span class="p">)</span> <span class="p">{</span>

    <span class="n">this</span><span class="o">.</span><span class="n">errorGradient</span> <span class="o">=</span> <span class="n">output</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">output</span><span class="p">)</span> <span class="o">*</span> <span class="n">error</span><span class="p">;</span>

    <span class="n">bias</span> <span class="o">+=</span> <span class="n">Parameters</span><span class="o">.</span><span class="n">LEARNING_RATE</span> <span class="o">*</span> <span class="n">errorGradient</span><span class="p">;</span>

    <span class="k">for</span><span class="p">(</span><span class="n">Connection</span> <span class="n">connection</span> <span class="p">:</span> <span class="n">backwardConnections</span><span class="p">)</span> <span class="p">{</span>

        <span class="o">//</span> <span class="k">for</span> <span class="n">clarification</span><span class="p">:</span> <span class="n">connection</span><span class="o">.</span><span class="n">input</span> <span class="n">refers</span> <span class="n">to</span> <span class="n">a</span> <span class="n">neuron</span> <span class="n">that</span> <span class="n">outputs</span> <span class="n">to</span> <span class="n">this</span> <span class="n">neuron</span>
        <span class="n">connection</span><span class="o">.</span><span class="n">weight</span> <span class="o">+=</span> <span class="n">Parameters</span><span class="o">.</span><span class="n">LEARNING_RATE</span> <span class="o">*</span> <span class="n">connection</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">getOutput</span><span class="p">()</span> <span class="o">*</span> <span class="n">errorGradient</span><span class="p">;</span>

    <span class="p">}</span>

<span class="p">}</span>

<span class="p">}</span>
</pre></div>

<p><strong>Results</strong></p>
<p>When I'm training for AND, OR, or NOR the network can usually converge within
about 1000 epochs, however when I train with XOR, the outputs become fixed and
it never converges. So, what am I doing wrong? Any ideas?</p>
<p><strong>Edit</strong></p>
<p>Following the advice of others, I started over and implemented my neural
network without classes...and it works. I'm still not sure where my problem
lies in the above code, but it's in there somewhere.</p>
<p><br><br></p>
<h2>Answer</h2>
<p>LiKao's comment to simplify my implementation and get rid of the object-
oriented aspects solved my problem. The flaw in the algorithm as it is
described above is unknown, however I now have a working neural network that
is a lot smaller.</p>
<p>Feel free to continue to provide insight on the problem with my previous
implementation, as others may have the same problem in the future.</p>
<p><br></p>
<h3>Suggest</h3>
<p>I faced the same problem short time ago. Finally I found the solution, how to
write a code solving XOR wit the MLP algorithm.</p>
<p>The XOR problem seems to be an easy to learn problem but it isn't for the MLP
because it's not linearly separable. So even if your MLP is OK (I mean there
is no bug in your code) you have to find the good parameters to be able to
learn the XOR problem.</p>
<p>Two hidden and one output neuron is fine. The 2 main thing you have to set:</p>
<ul>
<li>although you have only 4 training samples you have to run the training for a couple of thousands epoch.</li>
<li>if you use sigmoid hidden layers but linear output the network will converge faster</li>
</ul>
<p>Here is the detailed description and sample code:
http://freeconnection.blogspot.hu/2012/09/solving-xor-with-mlp.html</p>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/algorithm/" rel="tag">algorithm</a></li>
            <li><a class="tag p-category" href="../../categories/artificial-intelligence/" rel="tag">artificial-intelligence</a></li>
            <li><a class="tag p-category" href="../../categories/java/" rel="tag">java</a></li>
            <li><a class="tag p-category" href="../../categories/machine-learning/" rel="tag">machine-learning</a></li>
            <li><a class="tag p-category" href="../../categories/neural-network/" rel="tag">neural-network</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../what-does-train-on-batch-do-in-keras-model/" rel="prev" title="What does train_on_batch() do in keras model?">Previous post</a>
            </li>
            <li class="next">
                <a href="../heuristic-to-identify-if-a-series-of-4-bytes-chunks-of-data-are-integers-or-floats/" rel="next" title="Heuristic to identify if a series of 4 bytes chunks of data are integers or floats">Next post</a>
            </li>
        </ul></nav></aside></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow中文网</a>  
            
        </footer>
</div>
</div>


            <script src="../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="../../assets/js/search.js"></script>
</body>
</html>
