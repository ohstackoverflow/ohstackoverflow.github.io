<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>What are the differences between contextual embedding and word embedding | StackOverflow Snapshot</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/posts/what-are-the-differences-between-contextual-embedding-and-word-embedding/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Arya">
<link rel="prev" href="../implementing-the-td-gammon-algorithm/" title="Implementing the TD-Gammon algorithm" type="text/html">
<link rel="next" href="../keras-error-on-predict/" title="keras error on predict" type="text/html">
<meta property="og:site_name" content="StackOverflow Snapshot">
<meta property="og:title" content="What are the differences between contextual embedding and word embeddi">
<meta property="og:url" content="https://ohstackoverflow.netlify.app/posts/what-are-the-differences-between-contextual-embedding-and-word-embedding/">
<meta property="og:description" content="I am trying to understand the concept of embedding for the deep learning
models.
I understand how employing word2vec can address the limitations of using the
one-hot vectors.
However, recently I see a">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2023-02-28T03:17:25+08:00">
<meta property="article:tag" content="artificial-intelligence">
<meta property="article:tag" content="deep-learning">
<meta property="article:tag" content="machine-learning">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../archive.html">Archive</a>
                </li>
<li>
<a href="../../categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">What are the differences between contextual embedding and word embedding</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    <a class="u-url" href="../../authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:17:25+08:00" itemprop="datePublished" title="2023-02-28 03:17">2023-02-28 03:17</time></a>
            </p>
            

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>I am trying to understand the concept of embedding for the deep learning
models.</p>
<p>I understand how employing <code>word2vec</code> can address the limitations of using the
one-hot vectors.</p>
<p>However, recently I see a plethora of blog posts stating ELMo, BERT, etc.
talking about contextual embedding.</p>
<p>How are word embeddings different from contextual embeddings?</p>
<p><br><br></p>
<h2>Answer</h2>
<p>Both embedding techniques, traditional <strong>word embedding</strong> (e.g. word2vec,
Glove) and <strong>contextual embedding</strong> (e.g. ELMo, BERT), aim to learn a
<strong>continuous (vector) representation</strong> for each word in the documents.
Continuous representations can be used in downstream machine learning tasks.</p>
<p>Traditional <strong>word embedding techniques</strong> learn a global word embedding. They
first build a global vocabulary using unique words in the documents by
ignoring the meaning of words in different context. Then, similar
representations are learnt for the words appeared more frequently close each
other in the documents. The problem is that in such word representations the
words' contextual meaning (the meaning derived from the words' surroundings),
is ignored. For example, <strong>only one</strong> representation is learnt for "left" in
sentence "I <strong>left</strong> my phone on the <strong>left</strong> side of the table." However,
"left" has two different meanings in the sentence, and needs to have two
different representations in the embedding space.</p>
<p>On the other hand, <strong>contextual embedding methods</strong> are used to learn
<strong>sequence-level semantics</strong> by considering the sequence of all words in the
documents. Thus, such techniques learn <strong>different representations</strong> for
<strong>polysemous words</strong> , e.g. "left" in example above, based on their context.</p>
<p><br></p>
<h3>Suggest</h3>
<p>Word embeddings and contextual embeddings are slightly different.</p>
<p>While both word embeddings and contextual embeddings are obtained from the
models using unsupervised learning, there are some differences.</p>
<p>Word embeddings provided by <code>word2vec</code> or <code>fastText</code> has a vocabulary
(dictionary) of words. The elements of this vocabulary (or dictionary) are
words and its corresponding word embeddings. Hence, given a word, its
embeddings is always the same in whichever sentence it occurs. Here, the pre-
trained word embeddings are <code>static</code>.</p>
<p>However, contextual embeddings (are generally obtained from the transformer
based models). The emeddings are obtained from a model by passing the entire
sentence to the pre-trained model. Note that, here there is a vocabulary of
words, but the vocabulary will not contain the contextual embeddings. The
embeddings generated for each word depends on the other words in a given
sentence. (The other words in a given sentence is referred as <code>context</code>. The
transformer based models work on attention mechanism, and attention is a way
to look at the relation between a word with its neighbors). Thus, given a
word, it will not have a static embeddings, but the embeddings are dynamically
generated from pre-trained (or fine-tuned) model.</p>
<p>For example, consider the two sentences:</p>
<ol>
<li>I will show you a valid point of reference and talk to the point.</li>
<li>Where have you placed the point.</li>
</ol>
<p>Now, the word embeddings from a pre-trained embeddings such as word2vec, the
embeddings for the word <code>'point'</code> is same for both of its occurrences in
example 1 and also the same for the word <code>'point'</code> in example 2. (all three
occurrences has same embeddings).</p>
<p>While, the embeddings from BERT or ELMO or any such transformer based models,
the the two occurrences of the word <code>'point'</code> in example 1 will have different
embeddings. Also, the word <code>'point'</code> occurring in example 2 will have
different embeddings than the ones in example 1.</p>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/artificial-intelligence/" rel="tag">artificial-intelligence</a></li>
            <li><a class="tag p-category" href="../../categories/deep-learning/" rel="tag">deep-learning</a></li>
            <li><a class="tag p-category" href="../../categories/machine-learning/" rel="tag">machine-learning</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../implementing-the-td-gammon-algorithm/" rel="prev" title="Implementing the TD-Gammon algorithm">Previous post</a>
            </li>
            <li class="next">
                <a href="../keras-error-on-predict/" rel="next" title="keras error on predict">Next post</a>
            </li>
        </ul></nav></aside></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow-ZH</a>  
            
        </footer>
</div>
</div>


            <script src="../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="../../assets/js/search.js"></script>
</body>
</html>
