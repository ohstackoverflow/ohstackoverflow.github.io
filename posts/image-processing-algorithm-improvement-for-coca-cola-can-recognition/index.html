<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Image Processing: Algorithm Improvement for 'Coca-Cola Can' Recognition | StackOverflow Snapshot</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/posts/image-processing-algorithm-improvement-for-coca-cola-can-recognition/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Arya">
<link rel="prev" href="../how-to-align-checkboxes-and-their-labels-consistently-cross-browsers/" title="How to align checkboxes and their labels consistently cross-browsers" type="text/html">
<link rel="next" href="../check-existence-of-input-argument-in-a-bash-shell-script/" title="Check existence of input argument in a Bash shell script" type="text/html">
<meta property="og:site_name" content="StackOverflow Snapshot">
<meta property="og:title" content="Image Processing: Algorithm Improvement for 'Coca-Cola Can' Recognitio">
<meta property="og:url" content="https://ohstackoverflow.netlify.app/posts/image-processing-algorithm-improvement-for-coca-cola-can-recognition/">
<meta property="og:description" content="One of the most interesting projects I've worked on in the past couple of
years was a project about image processing. The goal was to develop a system
to be able to recognize Coca-Cola 'cans' (note th">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2023-02-16T23:53:07+08:00">
<meta property="article:tag" content="algorithm">
<meta property="article:tag" content="c++">
<meta property="article:tag" content="image-processing">
<meta property="article:tag" content="opencv">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../archive.html">Archive</a>
                </li>
<li>
<a href="../../categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Image Processing: Algorithm Improvement for 'Coca-Cola Can' Recognition</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    <a class="u-url" href="../../authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2023-02-16T23:53:07+08:00" itemprop="datePublished" title="2023-02-16 23:53">2023-02-16 23:53</time></a>
            </p>
            

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>One of the most interesting projects I've worked on in the past couple of
years was a project about image processing. The goal was to develop a system
to be able to recognize Coca-Cola <strong>'cans'</strong> (note that I'm stressing the word
'cans', you'll see why in a minute). You can see a sample below, with the can
recognized in the <em>green rectangle</em> with scale and rotation.</p>
<p><img alt="Template matching" src="../../images/irQtR.png"></p>
<p>Some constraints on the project:</p>
<ul>
<li>The background could be very noisy.</li>
<li>The <em>can</em> could have any <em>scale</em> or <em>rotation</em> or even orientation (within reasonable limits).</li>
<li>The image could have some degree of fuzziness (contours might not be entirely straight).</li>
<li>There could be Coca-Cola bottles in the image, and the algorithm should only detect the <em>can</em>!</li>
<li>The brightness of the image could vary a lot (so you can't rely "too much" on color detection).</li>
<li>The <em>can</em> could be partly hidden on the sides or the middle and possibly partly hidden behind a bottle.</li>
<li>There could be no <em>can</em> at all in the image, in which case you had to find nothing and write a message saying so.</li>
</ul>
<p>So you could end up with tricky things like this (which in this case had my
algorithm totally fail):</p>
<p><img alt="Total fail" src="../../images/Byw82.png"></p>
<p>I did this project a while ago, and had a lot of fun doing it, and I had a
decent implementation. Here are some details about my implementation:</p>
<p><strong>Language</strong> : Done in C++ using OpenCV library.</p>
<p><strong>Pre-processing</strong> : For the image pre-processing, i.e. transforming the image
into a more raw form to give to the algorithm, I used 2 methods:</p>
<ol>
<li>Changing color domain from RGB to HSV and filtering based on "red" hue, saturation above a certain threshold to avoid orange-like colors, and filtering of low value to avoid dark tones. The end result was a binary black and white image, where all white pixels would represent the pixels that match this threshold. Obviously there is still a lot of crap in the image, but this reduces the number of dimensions you have to work with. <img alt="Binarized image" src="../../images/ktdAB.png">
</li>
<li>Noise filtering using median filtering (taking the median pixel value of all neighbors and replace the pixel by this value) to reduce noise.</li>
<li>Using Canny Edge Detection Filter to get the contours of all items after 2 precedent steps. <img alt="Contour detection" src="../../images/F9319.png">
</li>
</ol>
<p><strong>Algorithm</strong> : The algorithm itself I chose for this task was taken from this
awesome book on feature extraction and called Generalized Hough Transform
(pretty different from the regular Hough Transform). It basically says a few
things:</p>
<ul>
<li>You can describe an object in space without knowing its analytical equation (which is the case here).</li>
<li>It is resistant to image deformations such as scaling and rotation, as it will basically test your image for every combination of scale factor and rotation factor.</li>
<li>It uses a base model (a template) that the algorithm will "learn".</li>
<li>Each pixel remaining in the contour image will vote for another pixel which will supposedly be the center (in terms of gravity) of your object, based on what it learned from the model.</li>
</ul>
<p>In the end, you end up with a heat map of the votes, for example here all the
pixels of the contour of the can will vote for its gravitational center, so
you'll have a lot of votes in the same pixel corresponding to the center, and
will see a peak in the heat map as below:</p>
<p><img alt="GHT" src="../../images/wxrT1.png"></p>
<p>Once you have that, a simple threshold-based heuristic can give you the
location of the center pixel, from which you can derive the scale and rotation
and then plot your little rectangle around it (final scale and rotation factor
will obviously be relative to your original template). In theory at least...</p>
<p><strong>Results</strong> : Now, while this approach worked in the basic cases, it was
severely lacking in some areas:</p>
<ul>
<li>It is <strong>extremely slow</strong>! I'm not stressing this enough. Almost a full day was needed to process the 30 test images, obviously because I had a very high scaling factor for rotation and translation, since some of the cans were very small.</li>
<li>It was completely lost when bottles were in the image, and for some reason almost always found the bottle instead of the can (perhaps because bottles were bigger, thus had more pixels, thus more votes)</li>
<li>Fuzzy images were also no good, since the votes ended up in pixel at random locations around the center, thus ending with a very noisy heat map.</li>
<li>In-variance in translation and rotation was achieved, but not in orientation, meaning that a can that was not directly facing the camera objective wasn't recognized.</li>
</ul>
<p>Can you help me improve my <strong>specific</strong> algorithm, using <strong>exclusively
OpenCV</strong> features, to resolve the <strong>four specific</strong> issues mentioned?</p>
<p>I hope some people will also learn something out of it as well, after all I
think not only people who ask questions should learn. :)</p>
<p><br><br></p>
<h2>Answer</h2>
<p>An alternative approach would be to extract features (keypoints) using the
scale-invariant feature transform (SIFT) or Speeded Up Robust Features (SURF).</p>
<p>You can find a nice <code>OpenCV</code> code example in <code>Java</code>, <code>C++</code>, and <code>Python</code> on
this page: <em>Features2D + Homography to find a known object</em></p>
<p>Both algorithms are invariant to scaling and rotation. Since they work with
features, you can also handle occlusion (as long as enough keypoints are
visible).</p>
<p><img alt="Enter image description here" src="../../images/kF63R.jpg"></p>
<p>Image source: tutorial example</p>
<p>The processing takes a few hundred ms for SIFT, SURF is bit faster, but it not
suitable for real-time applications. ORB uses FAST which is weaker regarding
rotation invariance.</p>
<h4>The original papers</h4>
<ul>
<li>SURF: Speeded Up Robust Features</li>
<li>Distinctive Image Features from Scale-Invariant Keypoints</li>
<li>ORB: an efficient alternative to SIFT or SURF</li>
</ul>
<p><br></p>
<h3>Suggest</h3>
<p>To speed things up, I would take advantage of the fact that you are not asked
to find an arbitrary image/object, but specifically one with the Coca-Cola
logo. This is significant because this logo is very distinctive, and it should
have a characteristic, scale-invariant signature in the frequency domain,
particularly in the red channel of RGB. That is to say, the alternating
pattern of red-to-white-to-red encountered by a horizontal scan line (trained
on a horizontally aligned logo) will have a distinctive "rhythm" as it passes
through the central axis of the logo. That rhythm will "speed up" or "slow
down" at different scales and orientations, but will remain proportionally
equivalent. You could identify/define a few dozen such scanlines, both
horizontally and vertically through the logo and several more diagonally, in a
starburst pattern. Call these the "signature scan lines."</p>
<p><img alt="Signature scan line" src="../../images/KWVZw.jpg"></p>
<p>Searching for this signature in the target image is a simple matter of
scanning the image in horizontal strips. Look for a high-frequency in the red-
channel (indicating moving from a red region to a white one), and once found,
see if it is followed by one of the frequency rhythms identified in the
training session. Once a match is found, you will instantly know the scan-
line's orientation and location in the logo (if you keep track of those things
during training), so identifying the boundaries of the logo from there is
trivial.</p>
<p>I would be surprised if this weren't a linearly-efficient algorithm, or nearly
so. It obviously doesn't address your can-bottle discrimination, but at least
you'll have your logos.</p>
<p>(Update: for bottle recognition I would look for coke (the brown liquid)
adjacent to the logo -- that is, <em>inside</em> the bottle. Or, in the case of an
empty bottle, I would look for a <em>cap</em> which will always have the same basic
shape, size, and distance from the logo and will typically be all white or
red. Search for a solid color eliptical shape where a cap <em>should</em> be,
relative to the logo. Not foolproof of course, but your goal here should be to
find the <em>easy</em> ones <em>fast</em>.)</p>
<p>(It's been a few years since my image processing days, so I kept this
suggestion high-level and conceptual. I think it might slightly approximate
how a human eye might operate -- or at least how my brain does!)</p>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/algorithm/" rel="tag">algorithm</a></li>
            <li><a class="tag p-category" href="../../categories/c%2B%2B/" rel="tag">c++</a></li>
            <li><a class="tag p-category" href="../../categories/image-processing/" rel="tag">image-processing</a></li>
            <li><a class="tag p-category" href="../../categories/opencv/" rel="tag">opencv</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../how-to-align-checkboxes-and-their-labels-consistently-cross-browsers/" rel="prev" title="How to align checkboxes and their labels consistently cross-browsers">Previous post</a>
            </li>
            <li class="next">
                <a href="../check-existence-of-input-argument-in-a-bash-shell-script/" rel="next" title="Check existence of input argument in a Bash shell script">Next post</a>
            </li>
        </ul></nav></aside></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow-ZH</a>  
            
        </footer>
</div>
</div>


            <script src="../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="../../assets/js/search.js"></script>
</body>
</html>
