<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>With neural networks, should the learning rate be in some way proportional to hidden layer sizes? Should they affect each other? | StackOverflow Snapshot</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/posts/with-neural-networks-should-the-learning-rate-be-in-some-way-proportional-to-hidden-layer-sizes-should-they-affect-each-other/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Arya">
<link rel="prev" href="../valueerror-unknown-activation-function-my-custom-activation-function/" title="ValueError: Unknown activation function: my_custom_activation_function" type="text/html">
<link rel="next" href="../what-are-the-problems-associated-to-best-first-search-in-artificial-intelligence/" title="What are the problems associated to Best First Search in Artificial intelligence?" type="text/html">
<meta property="og:site_name" content="StackOverflow Snapshot">
<meta property="og:title" content="With neural networks, should the learning rate be in some way proporti">
<meta property="og:url" content="https://ohstackoverflow.netlify.app/posts/with-neural-networks-should-the-learning-rate-be-in-some-way-proportional-to-hidden-layer-sizes-should-they-affect-each-other/">
<meta property="og:description" content="My neural network is normal feed-forward and back prop. Has 10 outputs, which
should be a vector where one of the output is 1, and the rest 0. So something
like [0,0,0,0,1,0,0,0,0]. So an output I wou">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2023-02-28T04:02:19+08:00">
<meta property="article:tag" content="artificial-intelligence">
<meta property="article:tag" content="machine-learning">
<meta property="article:tag" content="neural-network">
<meta property="article:tag" content="python">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../archive.html">Archive</a>
                </li>
<li>
<a href="../../categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">With neural networks, should the learning rate be in some way proportional to hidden layer sizes? Should they affect each other?</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    <a class="u-url" href="../../authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T04:02:19+08:00" itemprop="datePublished" title="2023-02-28 04:02">2023-02-28 04:02</time></a>
            </p>
            

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>My neural network is normal feed-forward and back prop. Has 10 outputs, which
should be a vector where one of the output is 1, and the rest 0. So something
like [0,0,0,0,1,0,0,0,0]. So an output I would expect is something like this:</p>
<div class="code"><pre class="code literal-block">[ 0.21332215,0.13782996,0.13548511,0.09321094,0.16769843,0.20333131, 0.06613014,0.10699013,0.10622562,0.09809167]
</pre></div>

<p>and ideally once trained, this:</p>
<div class="code"><pre class="code literal-block">[ 0.21332215,0.13782996,0.93548511 ,0.09321094 ,**0.9**676984,0.20333131, 0.06613014,0.1069901,0.10622562, 0.09809167]
</pre></div>

<p>When I have 30 neurons on the hidden layer, and a learning rate of &gt; 0.1 but &lt;
1, i get these results. However, when i have 100 neurons on hidden, and have a
learning rate of 0.01, i get results like this:</p>
<div class="code"><pre class="code literal-block">[  1.75289110e-05,1.16433042e-04 ,2.83848791e-01,4.47291309e-02, 1.63011592e-01,8.12974408e-05 , 1.06284533e-03 , 2.95174797e-02, 7.54112632e-05, 1.33177529e-03]
</pre></div>

<p>Why is this? Is this what over-learning looks like?</p>
<p>Then, when I change the learning rate to 0.0001 with 100 neurons on hidden, it
get normal results again.</p>
<p>So my question is: how should the learning rate affect the hidden layer count?
Should bigger hidden layers mean lower learning rates?</p>
<p><br><br></p>
<h2>Answer</h2>
<p>It can be said that there is a slight relation between the hidden unit count
and the learning rate, in general, when you increase the hidden unit count,
you obtain a more heavily parametrised model with a higher capacity and such a
model is always more prone to overfitting on the same training set. In
addition to that, this model operates in a space with a larger dimension and
has a more complex error surface compared to a thinner model. When you apply a
larger learning rate in such a complex error regime, the SGD process may
easily diverge to meaningless locations, which I believe, is the real reason
you are getting that weird results with the higher learning rate. In short, it
is logical that smaller learning rates work more reasonably when the model is
too complex.</p>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/artificial-intelligence/" rel="tag">artificial-intelligence</a></li>
            <li><a class="tag p-category" href="../../categories/machine-learning/" rel="tag">machine-learning</a></li>
            <li><a class="tag p-category" href="../../categories/neural-network/" rel="tag">neural-network</a></li>
            <li><a class="tag p-category" href="../../categories/python/" rel="tag">python</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../valueerror-unknown-activation-function-my-custom-activation-function/" rel="prev" title="ValueError: Unknown activation function: my_custom_activation_function">Previous post</a>
            </li>
            <li class="next">
                <a href="../what-are-the-problems-associated-to-best-first-search-in-artificial-intelligence/" rel="next" title="What are the problems associated to Best First Search in Artificial intelligence?">Next post</a>
            </li>
        </ul></nav></aside></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow-ZH</a>  
            
        </footer>
</div>
</div>


            <script src="../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="../../assets/js/search.js"></script>
</body>
</html>
