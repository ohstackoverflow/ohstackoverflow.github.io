<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Difference between a linear problem and a non-linear problem? Essence of Dot-Product and Kernel trick | StackOverflow Snapshot</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/posts/difference-between-a-linear-problem-and-a-non-linear-problem-essence-of-dot-product-and-kernel-trick/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Arya">
<link rel="prev" href="../algorithm-and-data-structure-for-solving-the-game-globs-flood-fill-floodit/" title='Algorithm and data structure for solving the game "Globs"/flood fill/"FloodIt"' type="text/html">
<link rel="next" href="../what-is-the-difference-between-uniform-cost-search-and-best-first-search-methods/" title="What is the difference between uniform-cost search and best-first search methods?" type="text/html">
<meta property="og:site_name" content="StackOverflow Snapshot">
<meta property="og:title" content="Difference between a linear problem and a non-linear problem? Essence ">
<meta property="og:url" content="https://ohstackoverflow.netlify.app/posts/difference-between-a-linear-problem-and-a-non-linear-problem-essence-of-dot-product-and-kernel-trick/">
<meta property="og:description" content="The kernel trick maps a non-linear problem into a linear problem.
My questions are:
1. What is the main difference between a linear and a non-linear problem?
What is the intuition behind the differenc">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2023-02-28T02:58:53+08:00">
<meta property="article:tag" content="algorithm">
<meta property="article:tag" content="artificial-intelligence">
<meta property="article:tag" content="language-agnostic">
<meta property="article:tag" content="machine-learning">
<meta property="article:tag" content="math">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../archive.html">Archive</a>
                </li>
<li>
<a href="../../categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Difference between a linear problem and a non-linear problem? Essence of Dot-Product and Kernel trick</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    <a class="u-url" href="../../authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T02:58:53+08:00" itemprop="datePublished" title="2023-02-28 02:58">2023-02-28 02:58</time></a>
            </p>
            

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>The kernel trick maps a non-linear problem into a linear problem.</p>
<p>My questions are:<br>
1. What is the main difference between a linear and a non-linear problem?
What is the intuition behind the difference of these two classes of problem?
And How does kernel trick helps use the linear classifiers on a non-linear
problem?<br>
2. Why is the dot product so important in the two cases?</p>
<p>Thanks.</p>
<p><br><br></p>
<h2>Answer</h2>
<p>Many classifiers, among them the linear Support Vector Machine (SVM), can only
solve problems that are linearly separable, i.e. where the points belonging to
class 1 can be separated from the points belonging to class 2 by a hyperplane.</p>
<p>In many cases, a problem that is not linearly separable can be solved by
applying a transform phi() to the data points; this transform is said to
transform the points to <em>feature space</em>. The hope is that, in feature space,
the points will be linearly separable. (Note: This is not the kernel trick
yet... stay tuned.)</p>
<p>It can be shown that, the higher the dimension of the feature space, the
greater the number of problems that are linearly separable in that space.
Therefore, one would ideally want the feature space to be as high-dimensional
as possible.</p>
<p>Unfortunately, as the dimension of feature space increases, so does the amount
of computation required. This is where the kernel trick comes in. Many machine
learning algorithms (among them the SVM) can be formulated in such a way that
the only operation they perform on the data points is a scalar product between
two data points. (I will denote a scalar product between x1 and x2 by <code>&lt;x1,
x2&gt;</code>.)</p>
<p>If we transform our points to feature space, the scalar product now looks like
this:</p>
<p><code>&lt;phi(x1), phi(x2)&gt;</code></p>
<p>The key insight is that there exists a class of functions called <em>kernels</em>
that can be used to optimize the computation of this scalar product. A kernel
is a function <code>K(x1, x2)</code> that has the property that</p>
<p><code>K(x1, x2) = &lt;phi(x1), phi(x2)&gt;</code></p>
<p>for some function phi(). In other words: We can evaluate the scalar product in
the low-dimensional data space (where x1 and x2 "live") without having to
transform to the high-dimensional feature space (where phi(x1) and phi(x2)
"live") -- but we still get the benefits of transforming to the high-
dimensional feature space. This is called the <em>kernel trick</em>.</p>
<p>Many popular kernels, such as the Gaussian kernel, actually correspond to a
transform phi() that transforms into an <em>infinte-dimensional</em> feature space.
The kernel trick allows us to compute scalar products in this space without
having to represent points in this space explicitly (which, obviously, is
impossible on computers with finite amounts of memory).</p>
<p><br></p>
<h3>Suggest</h3>
<p>Many classifiers, among them the linear Support Vector Machine (SVM), can only
solve problems that are linearly separable, i.e. where the points belonging to
class 1 can be separated from the points belonging to class 2 by a hyperplane.</p>
<p>In many cases, a problem that is not linearly separable can be solved by
applying a transform phi() to the data points; this transform is said to
transform the points to <em>feature space</em>. The hope is that, in feature space,
the points will be linearly separable. (Note: This is not the kernel trick
yet... stay tuned.)</p>
<p>It can be shown that, the higher the dimension of the feature space, the
greater the number of problems that are linearly separable in that space.
Therefore, one would ideally want the feature space to be as high-dimensional
as possible.</p>
<p>Unfortunately, as the dimension of feature space increases, so does the amount
of computation required. This is where the kernel trick comes in. Many machine
learning algorithms (among them the SVM) can be formulated in such a way that
the only operation they perform on the data points is a scalar product between
two data points. (I will denote a scalar product between x1 and x2 by <code>&lt;x1,
x2&gt;</code>.)</p>
<p>If we transform our points to feature space, the scalar product now looks like
this:</p>
<p><code>&lt;phi(x1), phi(x2)&gt;</code></p>
<p>The key insight is that there exists a class of functions called <em>kernels</em>
that can be used to optimize the computation of this scalar product. A kernel
is a function <code>K(x1, x2)</code> that has the property that</p>
<p><code>K(x1, x2) = &lt;phi(x1), phi(x2)&gt;</code></p>
<p>for some function phi(). In other words: We can evaluate the scalar product in
the low-dimensional data space (where x1 and x2 "live") without having to
transform to the high-dimensional feature space (where phi(x1) and phi(x2)
"live") -- but we still get the benefits of transforming to the high-
dimensional feature space. This is called the <em>kernel trick</em>.</p>
<p>Many popular kernels, such as the Gaussian kernel, actually correspond to a
transform phi() that transforms into an <em>infinte-dimensional</em> feature space.
The kernel trick allows us to compute scalar products in this space without
having to represent points in this space explicitly (which, obviously, is
impossible on computers with finite amounts of memory).</p>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/algorithm/" rel="tag">algorithm</a></li>
            <li><a class="tag p-category" href="../../categories/artificial-intelligence/" rel="tag">artificial-intelligence</a></li>
            <li><a class="tag p-category" href="../../categories/language-agnostic/" rel="tag">language-agnostic</a></li>
            <li><a class="tag p-category" href="../../categories/machine-learning/" rel="tag">machine-learning</a></li>
            <li><a class="tag p-category" href="../../categories/math/" rel="tag">math</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../algorithm-and-data-structure-for-solving-the-game-globs-flood-fill-floodit/" rel="prev" title='Algorithm and data structure for solving the game "Globs"/flood fill/"FloodIt"'>Previous post</a>
            </li>
            <li class="next">
                <a href="../what-is-the-difference-between-uniform-cost-search-and-best-first-search-methods/" rel="next" title="What is the difference between uniform-cost search and best-first search methods?">Next post</a>
            </li>
        </ul></nav></aside></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow-ZH</a>  
            
        </footer>
</div>
</div>


            <script src="../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="../../assets/js/search.js"></script>
</body>
</html>
