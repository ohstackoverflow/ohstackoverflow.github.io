<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>How to cluster similar sentences using BERT | StackOverflow Snapshot</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/posts/how-to-cluster-similar-sentences-using-bert/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Arya">
<link rel="prev" href="../how-to-program-a-neural-network-for-chess/" title="How to program a neural network for chess?" type="text/html">
<link rel="next" href="../object-oriented-bayesian-spam-filtering/" title="Object Oriented Bayesian Spam Filtering?" type="text/html">
<meta property="og:site_name" content="StackOverflow Snapshot">
<meta property="og:title" content="How to cluster similar sentences using BERT">
<meta property="og:url" content="https://ohstackoverflow.netlify.app/posts/how-to-cluster-similar-sentences-using-bert/">
<meta property="og:description" content="For ElMo, FastText and Word2Vec, I'm averaging the word embeddings within a
sentence and using HDBSCAN/KMeans clustering to group similar sentences.
A good example of the implementation can be seen in">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2023-02-28T02:40:53+08:00">
<meta property="article:tag" content="artificial-intelligence">
<meta property="article:tag" content="bert-language-model">
<meta property="article:tag" content="nlp">
<meta property="article:tag" content="python">
<meta property="article:tag" content="word-embedding">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../archive.html">Archive</a>
                </li>
<li>
<a href="../../categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">How to cluster similar sentences using BERT</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    <a class="u-url" href="../../authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T02:40:53+08:00" itemprop="datePublished" title="2023-02-28 02:40">2023-02-28 02:40</time></a>
            </p>
            

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>For ElMo, FastText and Word2Vec, I'm averaging the word embeddings within a
sentence and using HDBSCAN/KMeans clustering to group similar sentences.</p>
<p>A good example of the implementation can be seen in this short article:
http://ai.intelligentonlinetools.com/ml/text-clustering-word-embedding-
machine-learning/</p>
<p>I would like to do the same thing using BERT (using the BERT python package
from hugging face), however I am rather unfamiliar with how to extract the raw
word/sentence vectors in order to input them into a clustering algorithm. I
know that BERT can output sentence representations - so how would I actually
extract the raw vectors from a sentence?</p>
<p>Any information would be helpful.</p>
<p><br><br></p>
<h2>Answer</h2>
<p>As Subham Kumar mentioned, one can use this Python 3 library to compute
sentence similarity: https://github.com/UKPLab/sentence-transformers</p>
<p>The library has a few code examples to perform clustering:</p>
<p><code>fast_clustering.py</code>:</p>
<div class="code"><pre class="code literal-block"><span class="sd">"""</span>
<span class="sd">This is a more complex example on performing clustering on large scale dataset.</span>

<span class="sd">This examples find in a large set of sentences local communities, i.e., groups of sentences that are highly</span>
<span class="sd">similar. You can freely configure the threshold what is considered as similar. A high threshold will</span>
<span class="sd">only find extremely similar sentences, a lower threshold will find more sentence that are less similar.</span>

<span class="sd">A second parameter is 'min_community_size': Only communities with at least a certain number of sentences will be returned.</span>

<span class="sd">The method for finding the communities is extremely fast, for clustering 50k sentences it requires only 5 seconds (plus embedding comuptation).</span>

<span class="sd">In this example, we download a large set of questions from Quora and then find similar questions in this set.</span>
<span class="sd">"""</span>
<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">util</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">import</span> <span class="nn">time</span>


<span class="c1"># Model for computing sentence embeddings. We use one trained for similar questions detection</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s1">'paraphrase-MiniLM-L6-v2'</span><span class="p">)</span>

<span class="c1"># We donwload the Quora Duplicate Questions Dataset (https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs)</span>
<span class="c1"># and find similar question in it</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">"http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv"</span>
<span class="n">dataset_path</span> <span class="o">=</span> <span class="s2">"quora_duplicate_questions.tsv"</span>
<span class="n">max_corpus_size</span> <span class="o">=</span> <span class="mi">50000</span> <span class="c1"># We limit our corpus to only the first 50k questions</span>


<span class="c1"># Check if the dataset exists. If not, download and extract</span>
<span class="c1"># Download dataset if needed</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Download dataset"</span><span class="p">)</span>
    <span class="n">util</span><span class="o">.</span><span class="n">http_get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">dataset_path</span><span class="p">)</span>

<span class="c1"># Get all unique sentences from the file</span>
<span class="n">corpus_sentences</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">'utf8'</span><span class="p">)</span> <span class="k">as</span> <span class="n">fIn</span><span class="p">:</span>
    <span class="n">reader</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">DictReader</span><span class="p">(</span><span class="n">fIn</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">'</span><span class="se">\t</span><span class="s1">'</span><span class="p">,</span> <span class="n">quoting</span><span class="o">=</span><span class="n">csv</span><span class="o">.</span><span class="n">QUOTE_MINIMAL</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">reader</span><span class="p">:</span>
        <span class="n">corpus_sentences</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">'question1'</span><span class="p">])</span>
        <span class="n">corpus_sentences</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">'question2'</span><span class="p">])</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">corpus_sentences</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">max_corpus_size</span><span class="p">:</span>
            <span class="k">break</span>

<span class="n">corpus_sentences</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">corpus_sentences</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Encode the corpus. This might take a while"</span><span class="p">)</span>
<span class="n">corpus_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">corpus_sentences</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">convert_to_tensor</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">"Start clustering"</span><span class="p">)</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="c1">#Two parameters to tune:</span>
<span class="c1">#min_cluster_size: Only consider cluster that have at least 25 elements</span>
<span class="c1">#threshold: Consider sentence pairs with a cosine-similarity larger than threshold as similar</span>
<span class="n">clusters</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">community_detection</span><span class="p">(</span><span class="n">corpus_embeddings</span><span class="p">,</span> <span class="n">min_community_size</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Clustering done after </span><span class="si">{:.2f}</span><span class="s2"> sec"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">))</span>

<span class="c1">#Print for all clusters the top 3 and bottom 3 elements</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">clusters</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Cluster </span><span class="si">{}</span><span class="s2">, #</span><span class="si">{}</span><span class="s2"> Elements "</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cluster</span><span class="p">)))</span>
    <span class="k">for</span> <span class="n">sentence_id</span> <span class="ow">in</span> <span class="n">cluster</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">"</span><span class="p">,</span> <span class="n">corpus_sentences</span><span class="p">[</span><span class="n">sentence_id</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">"</span><span class="p">,</span> <span class="s2">"..."</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">sentence_id</span> <span class="ow">in</span> <span class="n">cluster</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:]:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">"</span><span class="p">,</span> <span class="n">corpus_sentences</span><span class="p">[</span><span class="n">sentence_id</span><span class="p">])</span>
</pre></div>

<p><code>kmeans.py</code>:</p>
<div class="code"><pre class="code literal-block"><span class="sd">"""</span>
<span class="sd">This is a simple application for sentence embeddings: clustering</span>

<span class="sd">Sentences are mapped to sentence embeddings and then k-mean clustering is applied.</span>
<span class="sd">"""</span>
<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="n">embedder</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s1">'paraphrase-MiniLM-L6-v2'</span><span class="p">)</span>

<span class="c1"># Corpus with example sentences</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'A man is eating food.'</span><span class="p">,</span>
          <span class="s1">'A man is eating a piece of bread.'</span><span class="p">,</span>
          <span class="s1">'A man is eating pasta.'</span><span class="p">,</span>
          <span class="s1">'The girl is carrying a baby.'</span><span class="p">,</span>
          <span class="s1">'The baby is carried by the woman'</span><span class="p">,</span>
          <span class="s1">'A man is riding a horse.'</span><span class="p">,</span>
          <span class="s1">'A man is riding a white horse on an enclosed ground.'</span><span class="p">,</span>
          <span class="s1">'A monkey is playing drums.'</span><span class="p">,</span>
          <span class="s1">'Someone in a gorilla costume is playing a set of drums.'</span><span class="p">,</span>
          <span class="s1">'A cheetah is running behind its prey.'</span><span class="p">,</span>
          <span class="s1">'A cheetah chases prey on across a field.'</span>
          <span class="p">]</span>
<span class="n">corpus_embeddings</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>

<span class="c1"># Perform kmean clustering</span>
<span class="n">num_clusters</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">clustering_model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">num_clusters</span><span class="p">)</span>
<span class="n">clustering_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">corpus_embeddings</span><span class="p">)</span>
<span class="n">cluster_assignment</span> <span class="o">=</span> <span class="n">clustering_model</span><span class="o">.</span><span class="n">labels_</span>

<span class="n">clustered_sentences</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_clusters</span><span class="p">)]</span>
<span class="k">for</span> <span class="n">sentence_id</span><span class="p">,</span> <span class="n">cluster_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cluster_assignment</span><span class="p">):</span>
    <span class="n">clustered_sentences</span><span class="p">[</span><span class="n">cluster_id</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">corpus</span><span class="p">[</span><span class="n">sentence_id</span><span class="p">])</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">clustered_sentences</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Cluster "</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">cluster</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
</pre></div>

<p><code>agglomerative.py</code>:</p>
<div class="code"><pre class="code literal-block"><span class="sd">"""</span>
<span class="sd">This is a simple application for sentence embeddings: clustering</span>

<span class="sd">Sentences are mapped to sentence embeddings and then agglomerative clustering with a threshold is applied.</span>
<span class="sd">"""</span>
<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">AgglomerativeClustering</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">embedder</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s1">'paraphrase-MiniLM-L6-v2'</span><span class="p">)</span>

<span class="c1"># Corpus with example sentences</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'A man is eating food.'</span><span class="p">,</span>
          <span class="s1">'A man is eating a piece of bread.'</span><span class="p">,</span>
          <span class="s1">'A man is eating pasta.'</span><span class="p">,</span>
          <span class="s1">'The girl is carrying a baby.'</span><span class="p">,</span>
          <span class="s1">'The baby is carried by the woman'</span><span class="p">,</span>
          <span class="s1">'A man is riding a horse.'</span><span class="p">,</span>
          <span class="s1">'A man is riding a white horse on an enclosed ground.'</span><span class="p">,</span>
          <span class="s1">'A monkey is playing drums.'</span><span class="p">,</span>
          <span class="s1">'Someone in a gorilla costume is playing a set of drums.'</span><span class="p">,</span>
          <span class="s1">'A cheetah is running behind its prey.'</span><span class="p">,</span>
          <span class="s1">'A cheetah chases prey on across a field.'</span>
          <span class="p">]</span>
<span class="n">corpus_embeddings</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>

<span class="c1"># Normalize the embeddings to unit length</span>
<span class="n">corpus_embeddings</span> <span class="o">=</span> <span class="n">corpus_embeddings</span> <span class="o">/</span>  <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">corpus_embeddings</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Perform kmean clustering</span>
<span class="n">clustering_model</span> <span class="o">=</span> <span class="n">AgglomerativeClustering</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">distance_threshold</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span> <span class="c1">#, affinity='cosine', linkage='average', distance_threshold=0.4)</span>
<span class="n">clustering_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">corpus_embeddings</span><span class="p">)</span>
<span class="n">cluster_assignment</span> <span class="o">=</span> <span class="n">clustering_model</span><span class="o">.</span><span class="n">labels_</span>

<span class="n">clustered_sentences</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">sentence_id</span><span class="p">,</span> <span class="n">cluster_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cluster_assignment</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">cluster_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">clustered_sentences</span><span class="p">:</span>
        <span class="n">clustered_sentences</span><span class="p">[</span><span class="n">cluster_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">clustered_sentences</span><span class="p">[</span><span class="n">cluster_id</span><span class="p">]</span><span class="o">.</span><span class="kp">append</span><span class="p">(</span><span class="n">corpus</span><span class="p">[</span><span class="n">sentence_id</span><span class="p">])</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="n">clustered_sentences</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Cluster "</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">cluster</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
</pre></div>

<p><br></p>
<h3>Suggest</h3>
<p>You will need to generate bert embeddidngs for the sentences first. bert-as-
service provides a very easy way to generate embeddings for sentences.</p>
<p>This is how you can geberate bert vectors for a list of sentences you need to
cluster. It is explained very well in the bert-as-service repository:
https://github.com/hanxiao/bert-as-service</p>
<p>Installations:</p>
<div class="code"><pre class="code literal-block">pip install bert-serving-server  # server
pip install bert-serving-client  # client, independent of `bert-serving-server`
</pre></div>

<p>Download one of the pre-trained models available at https://github.com/google-
research/bert</p>
<p>Start the service:</p>
<div class="code"><pre class="code literal-block">bert-serving-start -model_dir /your_model_directory/ -num_worker=4
</pre></div>

<p>Generate the vectors for the list of sentences:</p>
<div class="code"><pre class="code literal-block"><span class="kn">from</span> <span class="nn">bert_serving.client</span> <span class="kn">import</span> <span class="n">BertClient</span>
<span class="n">bc</span> <span class="o">=</span> <span class="n">BertClient</span><span class="p">()</span>
<span class="n">vectors</span><span class="o">=</span><span class="n">bc</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">your_list_of_sentences</span><span class="p">)</span>
</pre></div>

<p>This would give you a list of vectors, you could write them into a csv and use
any clustering algorithm as the sentences are reduced to numbers.</p>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/artificial-intelligence/" rel="tag">artificial-intelligence</a></li>
            <li><a class="tag p-category" href="../../categories/bert-language-model/" rel="tag">bert-language-model</a></li>
            <li><a class="tag p-category" href="../../categories/nlp/" rel="tag">nlp</a></li>
            <li><a class="tag p-category" href="../../categories/python/" rel="tag">python</a></li>
            <li><a class="tag p-category" href="../../categories/word-embedding/" rel="tag">word-embedding</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../how-to-program-a-neural-network-for-chess/" rel="prev" title="How to program a neural network for chess?">Previous post</a>
            </li>
            <li class="next">
                <a href="../object-oriented-bayesian-spam-filtering/" rel="next" title="Object Oriented Bayesian Spam Filtering?">Next post</a>
            </li>
        </ul></nav></aside></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow-ZH</a>  
            
        </footer>
</div>
</div>


            <script src="../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="../../assets/js/search.js"></script>
</body>
</html>
