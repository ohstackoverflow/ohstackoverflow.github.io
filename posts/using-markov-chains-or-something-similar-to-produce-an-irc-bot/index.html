<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Using Markov chains (or something similar) to produce an IRC-bot | StackOverflow Snapshot</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/posts/using-markov-chains-or-something-similar-to-produce-an-irc-bot/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Arya">
<link rel="prev" href="../machine-learning-challenge-diagnosing-program-in-java-groovy-datamining-machine-learning/" title="Machine learning challenge: diagnosing program in java/groovy (datamining, machine learning)" type="text/html">
<link rel="next" href="../monte-carlo-tree-searching-uct-implementation/" title="Monte Carlo Tree Searching UCT implementation" type="text/html">
<meta property="og:site_name" content="StackOverflow Snapshot">
<meta property="og:title" content="Using Markov chains (or something similar) to produce an IRC-bot">
<meta property="og:url" content="https://ohstackoverflow.netlify.app/posts/using-markov-chains-or-something-similar-to-produce-an-irc-bot/">
<meta property="og:description" content="I tried google and found little that I could understand.
I understand Markov chains to a very basic level: It's a mathematical model
that only depends on previous input to change states..so sort of a ">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2023-02-28T02:54:10+08:00">
<meta property="article:tag" content="artificial-intelligence">
<meta property="article:tag" content="markov-chains">
<meta property="article:tag" content="nlp">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../archive.html">Archive</a>
                </li>
<li>
<a href="../../categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Using Markov chains (or something similar) to produce an IRC-bot</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    <a class="u-url" href="../../authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T02:54:10+08:00" itemprop="datePublished" title="2023-02-28 02:54">2023-02-28 02:54</time></a>
            </p>
            

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>I tried google and found little that I could understand.</p>
<p>I understand Markov chains to a very basic level: It's a mathematical model
that only depends on previous input to change states..so sort of a FSM with
weighted random chances instead of different criteria?</p>
<p>I've heard that you can use them to generate semi-intelligent nonsense, given
sentences of existing words to use as a dictionary of kinds.</p>
<p>I can't think of search terms to find this, so can anyone link me or explain
how I could produce something that gives a semi-intelligent answer? (if you
asked it about pie, it would not start going on about the vietnam war it had
heard about)</p>
<p>I plan on:</p>
<ul>
<li>Having this bot idle in IRC channels for a bit</li>
<li>Strip any usernames out of the string and store as sentences or whatever</li>
<li>Over time, use this as the basis for the above.</li>
</ul>
<p><br><br></p>
<h2>Answer</h2>
<p>Yes, a Markov chain is a finite-state machine with probabilistic state
transitions. To generate random text with a simple, first-order Markov chain:</p>
<ol>
<li>Collect bigram (adjacent word pair) statistics from a corpus (collection of text).</li>
<li>Make a markov chain with one state per word. Reserve a special state for end-of-text.</li>
<li>The probability of jumping from state/word <em>x</em> to <em>y</em> is the probability of the words <em>y</em> immediately following <em>x</em> , estimated from relative bigram frequencies in the training corpus.</li>
<li>Start with a random word <em>x</em> (perhaps determined by how often that word occurs as the first word of a sentence in the corpus). Then pick a state/word <em>y</em> to jump to randomly, taking into account the probability of <em>y</em> following <em>x</em> (the state transition probability). Repeat until you hit end-of-text.</li>
</ol>
<p>If you want to get something semi-intelligent out of this, then your best shot
is to train it on lots of carefully collected texts. The "lots" part makes it
produce proper sentences (or plausible IRC speak) with high probability; the
"carefully collected" part means you control what it talks about. Introducing
higher-order Markov chains also helps in both areas, but takes more storage to
store the necessary statistics. You may also look into things like statistical
smoothing.</p>
<p>However, having your IRC bot actually respond to what is said to it takes a
<em>lot</em> more than Markov chains. It may be done by doing text categorization
(aka topic spotting) on what is said, then picking a domain-specific Markov
chain for text generation. Naïve Bayes is a popular model for topic spotting.</p>
<p>Kernighan and Pike in <em>The Practice of Programming</em> explore various
implementation strategies for Markov chain algorithms. These, and natural
language generation in general, is covered in great depth by Jurafsky and
Martin, <em>Speech and Language Processing</em>.</p>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/artificial-intelligence/" rel="tag">artificial-intelligence</a></li>
            <li><a class="tag p-category" href="../../categories/markov-chains/" rel="tag">markov-chains</a></li>
            <li><a class="tag p-category" href="../../categories/nlp/" rel="tag">nlp</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../machine-learning-challenge-diagnosing-program-in-java-groovy-datamining-machine-learning/" rel="prev" title="Machine learning challenge: diagnosing program in java/groovy (datamining, machine learning)">Previous post</a>
            </li>
            <li class="next">
                <a href="../monte-carlo-tree-searching-uct-implementation/" rel="next" title="Monte Carlo Tree Searching UCT implementation">Next post</a>
            </li>
        </ul></nav></aside></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow-ZH</a>  
            
        </footer>
</div>
</div>


            <script src="../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="../../assets/js/search.js"></script>
</body>
</html>
