<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Neural Network Architecture Design | StackOverflow Snapshot</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/posts/neural-network-architecture-design/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Arya">
<link rel="prev" href="../any-python-support-vector-machine-library-around-that-allows-online-learning/" title="Any python Support Vector Machine library around that allows online learning?" type="text/html">
<link rel="next" href="../assembling-a-function-as-needed-and-computing-it-fast/" title="Assembling a function as needed and computing it fast" type="text/html">
<meta property="og:site_name" content="StackOverflow Snapshot">
<meta property="og:title" content="Neural Network Architecture Design">
<meta property="og:url" content="https://ohstackoverflow.netlify.app/posts/neural-network-architecture-design/">
<meta property="og:description" content="I'm playing around with Neural Networks trying to understand the best
practices for designing their architecture based on the kind of problem you
need to solve.
I generated a very simple data set comp">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2023-02-28T03:39:41+08:00">
<meta property="article:tag" content="artificial-intelligence">
<meta property="article:tag" content="backpropagation">
<meta property="article:tag" content="neural-network">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../archive.html">Archive</a>
                </li>
<li>
<a href="../../categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Neural Network Architecture Design</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    <a class="u-url" href="../../authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:39:41+08:00" itemprop="datePublished" title="2023-02-28 03:39">2023-02-28 03:39</time></a>
            </p>
            

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>I'm playing around with Neural Networks trying to understand the best
practices for designing their architecture based on the kind of problem you
need to solve.</p>
<p>I generated a very simple data set composed of a single convex region as you
can see below:</p>
<p><img alt="enter image description here" src="../../images/4omgO.png"></p>
<p>Everything works fine when I use an architecture with L = 1, or L = 2 hidden
layers (plus the output layer), but as soon as I add a third hidden layer (L =
3) my performance drops down to slightly better than chance.</p>
<p>I know that the more complexity you add to a network (number of weights and
parameters to learn) the more you tend to go towards over-fitting your data,
but I believe this is not the nature of my problem for two reasons:</p>
<ul>
<li>my performance on the Training set is also around 60% (whereas over-fitting typically means you have a very low training error and high test error),</li>
<li>and I have a very large amount of data examples (don't look at the figure that's only a toy figure I uplaoded).</li>
</ul>
<blockquote>
<p>Can anybody help me understand why adding an extra hidden layer gives me
this drop in performances on such a simple task?</p>
</blockquote>
<p>Here is an image of my performance as a function of the number of layers used:</p>
<p><img alt="enter image description here" src="../../images/1K3b5.png"></p>
<p><strong>ADDED PART DUE TO COMMENTS:</strong></p>
<ul>
<li>I am using a sigmoid functions assuming values between 0 and 1, <code>L(s) = 1 / 1 + exp(-s)</code>
</li>
<li>I am using early stopping (after 40000 iterations of backprop) as a criteria to stop the learning. I know it is not the best way to stop but I thought that it would ok for such a simple classification task, if you believe this is the main reason I'm not converging I I might implement some better criteria. </li>
</ul>
<p><br><br></p>
<h2>Answer</h2>
<p>At least on the surface of it, this appears to be a case of the so-called
"vanishing gradient" problem.</p>
<p><strong>Activation functions</strong></p>
<p>Your neurons activate according to the logistic sigmoid function, f(x) = 1 /
(1 + e^-x) :</p>
<p><img alt="sigmoid function" src="../../images/Vzk32.png"></p>
<p>This activation function is used frequently because it has several nice
properties. One of these nice properties is that the derivative of f(x) is
expressible computationally using the value of the function itself, as f'(x) =
f(x)(1 - f(x)). This function has a nonzero value for x near zero, but quickly
goes to zero as |x| gets large :</p>
<p><img alt="sigmoid first derivative" src="../../images/oZiJL.png"></p>
<p><strong>Gradient descent</strong></p>
<p>In a feedforward neural network with logistic activations, the error is
typically propagated backwards through the network using the first derivative
as a learning signal. The usual update for a weight in your network is
proportional to the error attributable to that weight times the current weight
value times the derivative of the logistic function.</p>
<div class="code"><pre class="code literal-block">delta_w(w) ~= w * f'(err(w)) * err(w)
</pre></div>

<p>As the product of three potentially very small values, the first derivative in
such networks can become small very rapidly if the weights in the network fall
outside the "middle" regime of the logistic function's derivative. In
addition, this rapidly vanishing derivative becomes exacerbated by adding more
layers, because the error in a layer gets "split up" and partitioned out to
each unit in the layer. This, in turn, further reduces the gradient in layers
below that.</p>
<p>In networks with more than, say, two hidden layers, this can become a serious
problem for training the network, since the first-order gradient information
will lead you to believe that the weights cannot usefully change.</p>
<p>However, there are some solutions that can help ! The ones I can think of
involve changing your learning method to use something more sophisticated than
first-order gradient descent, generally incorporating some second-order
derivative information.</p>
<p><strong>Momentum</strong></p>
<p>The simplest solution to approximate using some second-order information is to
include a momentum term in your network parameter updates. Instead of updating
parameters using :</p>
<div class="code"><pre class="code literal-block">w_new = w_old - learning_rate * delta_w(w_old)
</pre></div>

<p>incorporate a momentum term :</p>
<div class="code"><pre class="code literal-block">w_dir_new = mu * w_dir_old - learning_rate * delta_w(w_old)
w_new = w_old + w_dir_new
</pre></div>

<p>Intuitively, you want to use information from past derivatives to help
determine whether you want to follow the new derivative entirely (which you
can do by setting mu = 0), or to keep going in the direction you were heading
on the previous update, tempered by the new gradient information (by setting
mu &gt; 0).</p>
<p>You can actually get even better than this by using "Nesterov's Accelerated
Gradient" :</p>
<div class="code"><pre class="code literal-block">w_dir_new = mu * w_dir_old - learning_rate * delta_w(w_old + mu * w_dir_old)
w_new = w_old + w_dir_new
</pre></div>

<p>I think the idea here is that instead of computing the derivative at the "old"
parameter value <code>w</code>, compute it at what would be the "new" setting for <code>w</code> if
you went ahead and moved there according to a standard momentum term. Read
more in a neural-networks context here (PDF).</p>
<p><strong>Hessian-Free</strong></p>
<p>The textbook way to incorporate second-order gradient information into your
neural network training algorithm is to use Newton's Method to compute the
first <em>and</em> second order derivatives of your objective function with respect
to the parameters. However, the second order derivative, called the Hessian
matrix, is often extremely large and prohibitively expensive to compute.</p>
<p>Instead of computing the entire Hessian, some clever research in the past few
years has indicated a way to compute just the values of the Hessian in a
particular search direction. You can then use this process to identify a
better parameter update than just the first-order gradient.</p>
<p>You can learn more about this by reading through a research paper (PDF) or
looking at a sample implementation.</p>
<p><strong>Others</strong></p>
<p>There are many other optimization methods that could be useful for this task
-- conjugate gradient (PDF -- definitely worth a read), Levenberg-Marquardt
(PDF), L-BFGS -- but from what I've seen in the research literature, momentum
and Hessian-free methods seem to be the most common ones.</p>
<p><br></p>
<h3>Suggest</h3>
<p>Because the number of iterations of training required for convergence
increases as you add complexity to a neural network, holding the length of
training constant while adding layers to a neural network will certainly
result in you eventually observing a drop like this. To figure out whether
that is the explanation for this particular observation, try increasing the
number of iterations of training that you're using and see if it improves.
Using a more intelligent stopping criterion is also a good option, but a
simple increase in the cut-off will give you answers faster.</p>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/artificial-intelligence/" rel="tag">artificial-intelligence</a></li>
            <li><a class="tag p-category" href="../../categories/backpropagation/" rel="tag">backpropagation</a></li>
            <li><a class="tag p-category" href="../../categories/neural-network/" rel="tag">neural-network</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../any-python-support-vector-machine-library-around-that-allows-online-learning/" rel="prev" title="Any python Support Vector Machine library around that allows online learning?">Previous post</a>
            </li>
            <li class="next">
                <a href="../assembling-a-function-as-needed-and-computing-it-fast/" rel="next" title="Assembling a function as needed and computing it fast">Next post</a>
            </li>
        </ul></nav></aside></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow-ZH</a>  
            
        </footer>
</div>
</div>


            <script src="../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="../../assets/js/search.js"></script>
</body>
</html>
