<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>What is the difference between Q-learning and SARSA? | StackOverflow Snapshot</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/posts/what-is-the-difference-between-q-learning-and-sarsa/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Arya">
<link rel="prev" href="../what-s-is-the-difference-between-train-validation-and-test-set-in-neural-networks/" title="What's is the difference between train, validation and test set, in neural networks?" type="text/html">
<link rel="next" href="../how-to-compute-precision-recall-accuracy-and-f1-score-for-the-multiclass-case-with-scikit-learn/" title="How to compute precision, recall, accuracy and f1-score for the multiclass case with scikit learn?" type="text/html">
<meta property="og:site_name" content="StackOverflow Snapshot">
<meta property="og:title" content="What is the difference between Q-learning and SARSA?">
<meta property="og:url" content="https://ohstackoverflow.netlify.app/posts/what-is-the-difference-between-q-learning-and-sarsa/">
<meta property="og:description" content="Although I know that SARSA is on-policy while Q-learning is off-policy, when
looking at their formulas it's hard (to me) to see any difference between
these two algorithms.
According to the book Reinf">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2023-02-28T02:23:27+08:00">
<meta property="article:tag" content="artificial-intelligence">
<meta property="article:tag" content="q-learning">
<meta property="article:tag" content="reinforcement-learning">
<meta property="article:tag" content="sarsa">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../archive.html">Archive</a>
                </li>
<li>
<a href="../../categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">What is the difference between Q-learning and SARSA?</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    <a class="u-url" href="../../authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T02:23:27+08:00" itemprop="datePublished" title="2023-02-28 02:23">2023-02-28 02:23</time></a>
            </p>
            

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>Although I know that SARSA is on-policy while Q-learning is off-policy, when
looking at their formulas it's hard (to me) to see any difference between
these two algorithms.</p>
<p>According to the book Reinforcement Learning: An Introduction (by Sutton and
Barto). In the SARSA algorithm, given a policy, the corresponding action-value
function Q (in the state s and action a, at timestep t), i.e. Q(st, at), can
be updated as follows</p>
<blockquote>
<p>Q(st, at) = Q(st, at) + α<em>(rt + γ</em>Q(st+1, at+1) - Q(st, at))</p>
</blockquote>
<p>On the other hand, the update step for the Q-learning algorithm is the
following</p>
<blockquote>
<p>Q(st, at) = Q(st, at) + α<em>(rt + γ</em>maxa Q(st+1, a) - Q(st, at))</p>
</blockquote>
<p>which can also be written as</p>
<blockquote>
<p>Q(st, at) = (1 - α) * Q(st, at) + α * (rt + γ*maxa Q(st+1, a))</p>
</blockquote>
<p>where γ (gamma) is the discount factor and rt is the reward received from the
environment at timestep t.</p>
<p>Is the difference between these two algorithms the fact that SARSA only looks
up the next policy value while Q-learning looks up the next <em>maximum</em> policy
value?</p>
<p><strong>TLDR (and my own answer)</strong></p>
<p>Thanks to all those answering this question since I first asked it. I've made
a github repo playing with Q-Learning and empirically understood what the
difference is. It all amounts to how <em><strong>you select your next best action</strong></em> ,
which from an algorithmic standpoint can be a <em>mean</em> , <em>max</em> or <em>best</em> action
depending on how you chose to implement it.</p>
<p>The other main difference is <em>when</em> this selection is happening (e.g.,
<em>online</em> vs <em>offline</em> ) and how/why that affects learning. If you are reading
this in 2019 and are more of a hands-on person, playing with a RL toy problem
is probably the best way to understand the differences.</p>
<p>One last <strong>important</strong> note is that both Suton &amp; Barto as well as Wikipedia
often have <em>mixed, confusing</em> or <em>wrong</em> formulaic representations with
regards to the <em>next state best/max action and reward</em> :</p>
<blockquote>
<p>r(t+1)</p>
</blockquote>
<p>is in fact</p>
<blockquote>
<p>r(t)</p>
</blockquote>
<p><br><br></p>
<h2>Answer</h2>
<p>Yes, this is the only difference. On-policy SARSA learns action values
relative to the policy it follows, while off-policy Q-Learning does it
relative to the greedy policy. Under some common conditions, they both
converge to the real value function, but at different rates. Q-Learning tends
to converge a little slower, but has the capabilitiy to continue learning
while changing policies. Also, Q-Learning is not guaranteed to converge when
combined with linear approximation.</p>
<p>In practical terms, under the ε-greedy policy, Q-Learning computes the
difference between Q(s,a) and the maximum action value, while SARSA computes
the difference between Q(s,a) and the weighted sum of the average action value
and the maximum:</p>
<p>Q-Learning: Q(st+1,at+1) = maxaQ(st+1,a)</p>
<p>SARSA: Q(st+1,at+1) = ε·meanaQ(st+1,a) + (1-ε)·maxaQ(st+1,a)</p>
<p><br></p>
<h3>Suggest</h3>
<p>Yes, this is the only difference. On-policy SARSA learns action values
relative to the policy it follows, while off-policy Q-Learning does it
relative to the greedy policy. Under some common conditions, they both
converge to the real value function, but at different rates. Q-Learning tends
to converge a little slower, but has the capabilitiy to continue learning
while changing policies. Also, Q-Learning is not guaranteed to converge when
combined with linear approximation.</p>
<p>In practical terms, under the ε-greedy policy, Q-Learning computes the
difference between Q(s,a) and the maximum action value, while SARSA computes
the difference between Q(s,a) and the weighted sum of the average action value
and the maximum:</p>
<p>Q-Learning: Q(st+1,at+1) = maxaQ(st+1,a)</p>
<p>SARSA: Q(st+1,at+1) = ε·meanaQ(st+1,a) + (1-ε)·maxaQ(st+1,a)</p>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/artificial-intelligence/" rel="tag">artificial-intelligence</a></li>
            <li><a class="tag p-category" href="../../categories/q-learning/" rel="tag">q-learning</a></li>
            <li><a class="tag p-category" href="../../categories/reinforcement-learning/" rel="tag">reinforcement-learning</a></li>
            <li><a class="tag p-category" href="../../categories/sarsa/" rel="tag">sarsa</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../what-s-is-the-difference-between-train-validation-and-test-set-in-neural-networks/" rel="prev" title="What's is the difference between train, validation and test set, in neural networks?">Previous post</a>
            </li>
            <li class="next">
                <a href="../how-to-compute-precision-recall-accuracy-and-f1-score-for-the-multiclass-case-with-scikit-learn/" rel="next" title="How to compute precision, recall, accuracy and f1-score for the multiclass case with scikit learn?">Next post</a>
            </li>
        </ul></nav></aside></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow-ZH</a>  
            
        </footer>
</div>
</div>


            <script src="../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="../../assets/js/search.js"></script>
</body>
</html>
