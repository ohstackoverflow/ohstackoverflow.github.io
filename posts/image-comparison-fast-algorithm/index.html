<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Image comparison - fast algorithm | StackOverflow Snapshot</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/posts/image-comparison-fast-algorithm/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Arya">
<link rel="prev" href="../way-to-go-from-recursion-to-iteration/" title="Way to go from recursion to iteration" type="text/html">
<link rel="next" href="../why-do-many-examples-use-fig-ax-plt-subplots-in-matplotlib-pyplot-python/" title="Why do many examples use `fig, ax = plt.subplots()` in Matplotlib/pyplot/python" type="text/html">
<meta property="og:site_name" content="StackOverflow Snapshot">
<meta property="og:title" content="Image comparison - fast algorithm">
<meta property="og:url" content="https://ohstackoverflow.netlify.app/posts/image-comparison-fast-algorithm/">
<meta property="og:description" content="I'm looking to create a base table of images and then compare any new images
against that to determine if the new image is an exact (or close) duplicate of
the base.
For example: if you want to reduce">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2023-03-03T12:52:19+08:00">
<meta property="article:tag" content="algorithm">
<meta property="article:tag" content="comparison">
<meta property="article:tag" content="computer-vision">
<meta property="article:tag" content="image">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../archive.html">Archive</a>
                </li>
<li>
<a href="../../categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Image comparison - fast algorithm</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    <a class="u-url" href="../../authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2023-03-03T12:52:19+08:00" itemprop="datePublished" title="2023-03-03 12:52">2023-03-03 12:52</time></a>
            </p>
            

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>I'm looking to create a base table of images and then compare any new images
against that to determine if the new image is an exact (or close) duplicate of
the base.</p>
<p>For example: if you want to reduce storage of the same image 100's of times,
you could store one copy of it and provide reference links to it. When a new
image is entered you want to compare to an existing image to make sure it's
not a duplicate ... ideas?</p>
<p>One idea of mine was to reduce to a small thumbnail and then randomly pick 100
pixel locations and compare.</p>
<p><br><br></p>
<h2>Answer</h2>
<p>Below are three approaches to solving this problem (and there are many
others).</p>
<ul>
<li>
<p>The first is a standard approach in computer vision, keypoint matching. This may require some background knowledge to implement, and can be slow.</p>
</li>
<li>
<p>The second method uses only elementary image processing, and is potentially faster than the first approach, and is straightforward to implement. However, what it gains in understandability, it lacks in robustness -- matching fails on scaled, rotated, or discolored images.</p>
</li>
<li>
<p>The third method is both fast and robust, but is potentially the hardest to implement.</p>
</li>
</ul>
<p><strong>Keypoint Matching</strong></p>
<p>Better than picking 100 random points is picking 100 <em>important</em> points.
Certain parts of an image have more information than others (particularly at
edges and corners), and these are the ones you'll want to use for smart image
matching. Google "keypoint extraction" and "keypoint matching" and you'll find
quite a few academic papers on the subject. These days, SIFT keypoints are
arguably the most popular, since they can match images under different scales,
rotations, and lighting. Some SIFT implementations can be found here.</p>
<p>One downside to keypoint matching is the running time of a naive
implementation: O(n^2m), where n is the number of keypoints in each image, and
m is the number of images in the database. Some clever algorithms might find
the closest match faster, like quadtrees or binary space partitioning.</p>
<hr>
<p><strong>Alternative solution: Histogram method</strong></p>
<p>Another less robust but potentially faster solution is to build feature
histograms for each image, and choose the image with the histogram closest to
the input image's histogram. I implemented this as an undergrad, and we used 3
color histograms (red, green, and blue), and two texture histograms, direction
and scale. I'll give the details below, but I should note that this only
worked well for matching images VERY similar to the database images. Re-
scaled, rotated, or discolored images can fail with this method, but small
changes like cropping won't break the algorithm</p>
<p>Computing the color histograms is straightforward -- just pick the range for
your histogram buckets, and for each range, tally the number of pixels with a
color in that range. For example, consider the "green" histogram, and suppose
we choose 4 buckets for our histogram: 0-63, 64-127, 128-191, and 192-255.
Then for each pixel, we look at the green value, and add a tally to the
appropriate bucket. When we're done tallying, we divide each bucket total by
the number of pixels in the entire image to get a normalized histogram for the
green channel.</p>
<p>For the texture direction histogram, we started by performing edge detection
on the image. Each edge point has a normal vector pointing in the direction
perpendicular to the edge. We quantized the normal vector's angle into one of
6 buckets between 0 and PI (since edges have 180-degree symmetry, we converted
angles between -PI and 0 to be between 0 and PI). After tallying up the number
of edge points in each direction, we have an un-normalized histogram
representing texture direction, which we normalized by dividing each bucket by
the total number of edge points in the image.</p>
<p>To compute the texture scale histogram, for each edge point, we measured the
distance to the next-closest edge point with the same direction. For example,
if edge point A has a direction of 45 degrees, the algorithm walks in that
direction until it finds another edge point with a direction of 45 degrees (or
within a reasonable deviation). After computing this distance for each edge
point, we dump those values into a histogram and normalize it by dividing by
the total number of edge points.</p>
<p>Now you have 5 histograms for each image. To compare two images, you take the
absolute value of the difference between each histogram bucket, and then sum
these values. For example, to compare images A and B, we would compute</p>
<div class="code"><pre class="code literal-block">|A.green_histogram.bucket_1 - B.green_histogram.bucket_1|
</pre></div>

<p>for each bucket in the green histogram, and repeat for the other histograms,
and then sum up all the results. The smaller the result, the better the match.
Repeat for all images in the database, and the match with the smallest result
wins. You'd probably want to have a threshold, above which the algorithm
concludes that no match was found.</p>
<hr>
<p><strong>Third Choice - Keypoints + Decision Trees</strong></p>
<p>A third approach that is probably much faster than the other two is using
semantic texton forests (PDF). This involves extracting simple keypoints and
using a collection decision trees to classify the image. This is faster than
simple SIFT keypoint matching, because it avoids the costly matching process,
and keypoints are much simpler than SIFT, so keypoint extraction is much
faster. However, it preserves the SIFT method's invariance to rotation, scale,
and lighting, an important feature that the histogram method lacked.</p>
<p><strong>Update</strong> :</p>
<p>My mistake -- the Semantic Texton Forests paper isn't specifically about image
matching, but rather region labeling. The original paper that does matching is
this one: Keypoint Recognition using Randomized Trees. Also, the papers below
continue to develop the ideas and represent the state of the art (c. 2010):</p>
<ul>
<li>Fast Keypoint Recognition using Random Ferns - faster and more scalable than Lepetit 06</li>
<li>
<del>BRIEF: Binary Robust Independent Elementary Features</del> - less robust but very fast -- I think the goal here is real-time matching on smart phones and other handhelds</li>
</ul>
<p><br></p>
<h3>Suggest</h3>
<p>The best method I know of is to use a Perceptual Hash. There appears to be a
good open source implementation of such a hash available at:</p>
<p>http://phash.org/</p>
<p>The main idea is that each image is reduced down to a small hash code or
'fingerprint' by identifying salient features in the original image file and
hashing a compact representation of those features (rather than hashing the
image data directly). This means that the false positives rate is much reduced
over a simplistic approach such as reducing images down to a tiny thumbprint
sized image and comparing thumbprints.</p>
<p>phash offers several types of hash and can be used for images, audio or video.</p>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/algorithm/" rel="tag">algorithm</a></li>
            <li><a class="tag p-category" href="../../categories/comparison/" rel="tag">comparison</a></li>
            <li><a class="tag p-category" href="../../categories/computer-vision/" rel="tag">computer-vision</a></li>
            <li><a class="tag p-category" href="../../categories/image/" rel="tag">image</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../way-to-go-from-recursion-to-iteration/" rel="prev" title="Way to go from recursion to iteration">Previous post</a>
            </li>
            <li class="next">
                <a href="../why-do-many-examples-use-fig-ax-plt-subplots-in-matplotlib-pyplot-python/" rel="next" title="Why do many examples use `fig, ax = plt.subplots()` in Matplotlib/pyplot/python">Next post</a>
            </li>
        </ul></nav></aside></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow中文网</a>  
            
        </footer>
</div>
</div>


            <script src="../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="../../assets/js/search.js"></script>
</body>
</html>
