<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>How do I check if PyTorch is using the GPU? | StackOverflow Snapshot</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/posts/how-do-i-check-if-pytorch-is-using-the-gpu/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Arya">
<link rel="prev" href="../how-do-i-get-the-current-username-in-windows-powershell/" title="How do I get the current username in Windows PowerShell?" type="text/html">
<link rel="next" href="../git-stash-changes-apply-to-new-branch/" title="git stash changes apply to new branch?" type="text/html">
<meta property="og:site_name" content="StackOverflow Snapshot">
<meta property="og:title" content="How do I check if PyTorch is using the GPU?">
<meta property="og:url" content="https://ohstackoverflow.netlify.app/posts/how-do-i-check-if-pytorch-is-using-the-gpu/">
<meta property="og:description" content="How do I check if PyTorch is using the GPU? The nvidia-smi command can
detect GPU activity, but I want to check it directly from inside a Python
script.

Answer
These functions should help:
&gt;&gt;&gt; import">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2023-03-03T10:39:09+08:00">
<meta property="article:tag" content="gpu">
<meta property="article:tag" content="memory-management">
<meta property="article:tag" content="nvidia">
<meta property="article:tag" content="python">
<meta property="article:tag" content="pytorch">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../archive.html">Archive</a>
                </li>
<li>
<a href="../../categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">How do I check if PyTorch is using the GPU?</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    <a class="u-url" href="../../authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2023-03-03T10:39:09+08:00" itemprop="datePublished" title="2023-03-03 10:39">2023-03-03 10:39</time></a>
            </p>
            

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>How do I check if PyTorch is using the GPU? The <code>nvidia-smi</code> command can
detect GPU activity, but I want to check it directly from inside a Python
script.</p>
<p><br><br></p>
<h2>Answer</h2>
<p>These functions should help:</p>
<div class="code"><pre class="code literal-block"><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">torch</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
<span class="kc">True</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
<span class="mi">1</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span>
<span class="mi">0</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="o">&lt;</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span> <span class="n">at</span> <span class="mh">0x7efce0b03be0</span><span class="o">&gt;</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="s1">'GeForce GTX 950M'</span>
</pre></div>

<p>This tells us:</p>
<ul>
<li>CUDA is available and can be used by one device.</li>
<li>
<code>Device 0</code> refers to the GPU <code>GeForce GTX 950M</code>, and it is currently chosen by PyTorch.</li>
</ul>
<p><br></p>
<h3>Suggest</h3>
<p>As it hasn't been proposed here, I'm adding a method using <code>torch.device</code>, as
this is quite handy, also when initializing tensors on the correct <code>device</code>.</p>
<div class="code"><pre class="code literal-block">#<span class="w"> </span><span class="nv">setting</span><span class="w"> </span><span class="nv">device</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">GPU</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nv">available</span>,<span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="nv">CPU</span>
<span class="nv">device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">torch</span>.<span class="nv">device</span><span class="ss">(</span><span class="s1">'cuda'</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nv">torch</span>.<span class="nv">cuda</span>.<span class="nv">is_available</span><span class="ss">()</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">'cpu'</span><span class="ss">)</span>
<span class="nv">print</span><span class="ss">(</span><span class="s1">'Using device:'</span>,<span class="w"> </span><span class="nv">device</span><span class="ss">)</span>
<span class="nv">print</span><span class="ss">()</span>

#<span class="nv">Additional</span><span class="w"> </span><span class="nv">Info</span><span class="w"> </span><span class="nv">when</span><span class="w"> </span><span class="nv">using</span><span class="w"> </span><span class="nv">cuda</span>
<span class="k">if</span><span class="w"> </span><span class="nv">device</span>.<span class="nv">type</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s1">'cuda'</span>:
<span class="w">    </span><span class="nv">print</span><span class="ss">(</span><span class="nv">torch</span>.<span class="nv">cuda</span>.<span class="nv">get_device_name</span><span class="ss">(</span><span class="mi">0</span><span class="ss">))</span>
<span class="w">    </span><span class="nv">print</span><span class="ss">(</span><span class="s1">'Memory Usage:'</span><span class="ss">)</span>
<span class="w">    </span><span class="nv">print</span><span class="ss">(</span><span class="s1">'Allocated:'</span>,<span class="w"> </span><span class="nv">round</span><span class="ss">(</span><span class="nv">torch</span>.<span class="nv">cuda</span>.<span class="nv">memory_allocated</span><span class="ss">(</span><span class="mi">0</span><span class="ss">)</span><span class="o">/</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>,<span class="mi">1</span><span class="ss">)</span>,<span class="w"> </span><span class="s1">'GB'</span><span class="ss">)</span>
<span class="w">    </span><span class="nv">print</span><span class="ss">(</span><span class="s1">'Cached:   '</span>,<span class="w"> </span><span class="nv">round</span><span class="ss">(</span><span class="nv">torch</span>.<span class="nv">cuda</span>.<span class="nv">memory_reserved</span><span class="ss">(</span><span class="mi">0</span><span class="ss">)</span><span class="o">/</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>,<span class="mi">1</span><span class="ss">)</span>,<span class="w"> </span><span class="s1">'GB'</span><span class="ss">)</span>
</pre></div>

<p><em>Edit:<code>torch.cuda.memory_cached</code> has been renamed to
<code>torch.cuda.memory_reserved</code>. So use <code>memory_cached</code> for older versions.</em></p>
<p><em><strong>Output:</strong></em></p>
<div class="code"><pre class="code literal-block">Using device: cuda

Tesla K80
Memory Usage:
Allocated: 0.3 GB
Cached:    0.6 GB
</pre></div>

<p>As mentioned above, using <code>device</code> it is <em>possible to</em> :</p>
<ul>
<li>
<p><strong>To <em>move</em> tensors to the respective <code>device</code>:</strong></p>
<div class="code"><pre class="code literal-block">torch.rand(10).to(device)
</pre></div>

</li>
<li>
<p><strong>To <em>create</em> a tensor directly on the <code>device</code>:</strong></p>
<div class="code"><pre class="code literal-block">torch.rand(10, device=device)
</pre></div>

</li>
</ul>
<p>Which makes switching between <strong>CPU</strong> and <strong>GPU</strong> comfortable without changing
the actual code.</p>
<hr>
<h3><strong>Edit:</strong></h3>
<p>As there has been some questions and confusion about the <em>cached</em> and
<em>allocated</em> memory I'm adding some additional information about it:</p>
<ul>
<li>
<strong><code>torch.cuda.max_memory_cached(device=None)</code></strong>  </li>
</ul>
<p><em>Returns the maximum GPU memory managed by the caching allocator in bytes for
a given device.</em>  </p>
<ul>
<li>
<strong><code>torch.cuda.memory_allocated(device=None)</code></strong>  </li>
</ul>
<p><em>Returns the current GPU memory usage by tensors in bytes for a given device.</em></p>
<p>You can either directly hand over a <strong><code>device</code></strong> as specified further above in
the post or you can leave it <strong>None</strong> and it will use the
<strong><code>current_device()</code></strong>.</p>
<hr>
<p><em><strong>Additional note: Old graphic cards with Cuda compute capability 3.0 or
lower may be visible but cannot be used by Pytorch!</strong><br>
Thanks to hekimgil for pointing this out! - "Found GPU0 GeForce GT 750M which
is of cuda capability 3.0. PyTorch no longer supports this GPU because it is
too old. The minimum cuda capability that we support is 3.5."</em></p>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/gpu/" rel="tag">gpu</a></li>
            <li><a class="tag p-category" href="../../categories/memory-management/" rel="tag">memory-management</a></li>
            <li><a class="tag p-category" href="../../categories/nvidia/" rel="tag">nvidia</a></li>
            <li><a class="tag p-category" href="../../categories/python/" rel="tag">python</a></li>
            <li><a class="tag p-category" href="../../categories/pytorch/" rel="tag">pytorch</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../how-do-i-get-the-current-username-in-windows-powershell/" rel="prev" title="How do I get the current username in Windows PowerShell?">Previous post</a>
            </li>
            <li class="next">
                <a href="../git-stash-changes-apply-to-new-branch/" rel="next" title="git stash changes apply to new branch?">Next post</a>
            </li>
        </ul></nav></aside></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow中文网</a>  
            
        </footer>
</div>
</div>


            <script src="../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="../../assets/js/search.js"></script>
</body>
</html>
