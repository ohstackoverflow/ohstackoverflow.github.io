<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>How to convert the output of an artificial neural network into probabilities? | StackOverflow Snapshot</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/posts/how-to-convert-the-output-of-an-artificial-neural-network-into-probabilities/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Arya">
<link rel="prev" href="../insert-or-delete-a-step-in-scikit-learn-pipeline/" title="Insert or delete a step in scikit-learn Pipeline" type="text/html">
<link rel="next" href="../svm-and-neural-network/" title="SVM and Neural Network" type="text/html">
<meta property="og:site_name" content="StackOverflow Snapshot">
<meta property="og:title" content="How to convert the output of an artificial neural network into probabi">
<meta property="og:url" content="https://ohstackoverflow.netlify.app/posts/how-to-convert-the-output-of-an-artificial-neural-network-into-probabilities/">
<meta property="og:description" content="I've read about neural network a little while ago and I understand how an ANN
(especially a multilayer perceptron that learns via backpropagation) can learn
to classify an event as true or false.
I th">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2023-02-28T02:33:21+08:00">
<meta property="article:tag" content="artificial-intelligence">
<meta property="article:tag" content="neural-network">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../archive.html">Archive</a>
                </li>
<li>
<a href="../../categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">How to convert the output of an artificial neural network into probabilities?</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    <a class="u-url" href="../../authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T02:33:21+08:00" itemprop="datePublished" title="2023-02-28 02:33">2023-02-28 02:33</time></a>
            </p>
            

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>I've read about neural network a little while ago and I understand how an ANN
(especially a multilayer perceptron that learns via backpropagation) can learn
to classify an event as true or false.</p>
<p>I think there are two ways :</p>
<p>1) You get one output neuron. It it's value is &gt; 0.5 the events is likely
true, if it's value is &lt;=0.5 the event is likely to be false.</p>
<p>2) You get two output neurons, if the value of the first is &gt; than the value
of the second the event is likely true and vice versa.</p>
<p>In these case, the ANN tells you if an event is likely true or likely false.
It does not tell how likely it is.</p>
<p>Is there a way to convert this value to some odds or to directly get odds out
of the ANN. I'd like to get an output like "The event has a 84% probability to
be true"</p>
<p><br><br></p>
<h2>Answer</h2>
<p>Once a NN has been trained, for eg. using backprogation as mentioned in the
question (whereby the backprogation logic has "nudged" the weights in ways
that minimize the error function) the weights associated with all individual
inputs ("outside" inputs or intra-NN inputs) are fixed. The NN can then be
used for classifying purposes.</p>
<p>Whereby the math (and the "options") during the learning phase can get a bit
thick, it is relatively simple and straightfoward when operating as a
classifier. The main algorithm is to compute an activation value for each
neuron, as the sum of the input x weight for that neuron. This value is then
fed to an activation function which purpose's is to normalize it and convert
it to a boolean (in typical cases, as some networks do not have an all-or-
nothing rule for some of their layers). The activation function can be more
complex than you indicated, in particular it needn't be linear, but whatever
its shape, typically sigmoid, it operate in the same fashion: figuring out
where the activation fits on the curve, and if applicable, above or below a
threshold. The basic algorithm then processes all neurons at a given layer
before proceeding to the next.</p>
<p>With this in mind, the question of using the perceptron's ability to qualify
its guess (or indeed guesses - plural) with a percentage value, finds an easy
answer: you bet it can, its output(s) is real-valued (if anything in need of
normalizing) before we convert it to a discrete value (a boolean or a category
ID in the case of several categories), using the activation functions and the
threshold/comparison methods described in the question.</p>
<p>So... How and Where do I get "my percentages"?... All depends on the NN
implementation, and more importantly, the implementation dictates the type of
normalization functions that can be used to bring activation values in the 0-1
range <em>and</em> in a fashion that the sum of all percentages "add up" to 1. In its
simplest form, the activation function can be used to normalize the value and
the weights of the input to the output layer can be used as factors to ensure
the "add up" to 1 question (provided that these weights are indeed so
normalized themselves).</p>
<p>Et voilà!</p>
<p><strong>Claritication</strong> : (following Mathieu's note)<br>
One doesn't need to change anything in the way the Neural Network itself
works; the only thing needed is to somehow "hook into" the logic of <em>output</em>
neurons to access the [real-valued] activation value they computed, or,
possibly better, to access the real-valued output of the activation function,
<em>prior its boolean conversion</em> (which is typically based on a threshold value
or on some stochastic function).</p>
<p>In other words, the NN works as previously, neither its training nor
recognition logic are altered, the inputs to the NN stay the same, as do the
connections between various layers etc. We only get a copy of the real-valued
activation of the neurons in the output layer, and we use this to compute a
percentage. The actual formula for the percentage calculation depends on the
nature of the activation value and its associated function (its scale, its
range relative to other neurons' output etc.).<br>
Here are a few simple cases (taken from the question's suggested output rules)
1) If there is a single output neuron: the ratio of the value provided by the
activation function relative to the range of that function should do. 2) If
there are two (or more output neurons), as with classifiers for example: If
all output neurons have the same activation function, the percentage for a
given neuron is that of its activation function value divided by the sum of
all activation function values. If the activation functions vary, it becomes a
case by case situation because the distinct activation functions may be
indicative of a purposeful desire to give more weight to some of the neurons,
and the percentage should respect this.</p>
<p><br></p>
<h3>Suggest</h3>
<p>What you can do is to use a sigmoid transfer function on the output layer
nodes (that accepts data ranges (-inf,inf) and outputs a value in [-1,1]).<br>
Then by using the <strong>1-of-n output encoding</strong> (one node for each class), you
can map the range [-1,1] to [0,1] and use it as probability for each class
value (note that this works naturally for more than just two classes).</p>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/artificial-intelligence/" rel="tag">artificial-intelligence</a></li>
            <li><a class="tag p-category" href="../../categories/neural-network/" rel="tag">neural-network</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../insert-or-delete-a-step-in-scikit-learn-pipeline/" rel="prev" title="Insert or delete a step in scikit-learn Pipeline">Previous post</a>
            </li>
            <li class="next">
                <a href="../svm-and-neural-network/" rel="next" title="SVM and Neural Network">Next post</a>
            </li>
        </ul></nav></aside></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow中文网</a>  
            
        </footer>
</div>
</div>


            <script src="../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="../../assets/js/search.js"></script>
</body>
</html>
