<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>How to tell if tensorflow is using gpu acceleration from inside python shell? | StackOverflow Snapshot</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/posts/how-to-tell-if-tensorflow-is-using-gpu-acceleration-from-inside-python-shell/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Arya">
<link rel="prev" href="../how-to-use-hex-color-values/" title="How to use hex color values" type="text/html">
<link rel="next" href="../typescript-input-onchange-event-target-value/" title="Typescript input onchange event.target.value" type="text/html">
<meta property="og:site_name" content="StackOverflow Snapshot">
<meta property="og:title" content="How to tell if tensorflow is using gpu acceleration from inside python">
<meta property="og:url" content="https://ohstackoverflow.netlify.app/posts/how-to-tell-if-tensorflow-is-using-gpu-acceleration-from-inside-python-shell/">
<meta property="og:description" content="I have installed tensorflow in my ubuntu 16.04 using the second answer here
with ubuntu's builtin apt cuda installation.
Now my question is how can I test if tensorflow is really using gpu? I have a
g">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2023-03-03T14:21:28+08:00">
<meta property="article:tag" content="gpu">
<meta property="article:tag" content="python">
<meta property="article:tag" content="tensorflow">
<meta property="article:tag" content="ubuntu">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../archive.html">Archive</a>
                </li>
<li>
<a href="../../categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">How to tell if tensorflow is using gpu acceleration from inside python shell?</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    <a class="u-url" href="../../authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2023-03-03T14:21:28+08:00" itemprop="datePublished" title="2023-03-03 14:21">2023-03-03 14:21</time></a>
            </p>
            

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>I have installed tensorflow in my ubuntu 16.04 using the second answer here
with ubuntu's builtin apt cuda installation.</p>
<p>Now my question is how can I test if tensorflow is really using gpu? I have a
gtx 960m gpu. When I <code>import tensorflow</code> this is the output</p>
<div class="code"><pre class="code literal-block"><span class="n">I</span><span class="w"> </span><span class="n">tensorflow</span><span class="o">/</span><span class="n">stream_executor</span><span class="o">/</span><span class="n">dso_loader</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">105</span><span class="p">]</span><span class="w"> </span><span class="n">successfully</span><span class="w"> </span><span class="n">opened</span><span class="w"> </span><span class="n">CUDA</span><span class="w"> </span><span class="n">library</span><span class="w"> </span><span class="n">libcublas</span><span class="o">.</span><span class="n">so</span><span class="w"> </span><span class="n">locally</span>
<span class="n">I</span><span class="w"> </span><span class="n">tensorflow</span><span class="o">/</span><span class="n">stream_executor</span><span class="o">/</span><span class="n">dso_loader</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">105</span><span class="p">]</span><span class="w"> </span><span class="n">successfully</span><span class="w"> </span><span class="n">opened</span><span class="w"> </span><span class="n">CUDA</span><span class="w"> </span><span class="n">library</span><span class="w"> </span><span class="n">libcudnn</span><span class="o">.</span><span class="n">so</span><span class="w"> </span><span class="n">locally</span>
<span class="n">I</span><span class="w"> </span><span class="n">tensorflow</span><span class="o">/</span><span class="n">stream_executor</span><span class="o">/</span><span class="n">dso_loader</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">105</span><span class="p">]</span><span class="w"> </span><span class="n">successfully</span><span class="w"> </span><span class="n">opened</span><span class="w"> </span><span class="n">CUDA</span><span class="w"> </span><span class="n">library</span><span class="w"> </span><span class="n">libcufft</span><span class="o">.</span><span class="n">so</span><span class="w"> </span><span class="n">locally</span>
<span class="n">I</span><span class="w"> </span><span class="n">tensorflow</span><span class="o">/</span><span class="n">stream_executor</span><span class="o">/</span><span class="n">dso_loader</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">105</span><span class="p">]</span><span class="w"> </span><span class="n">successfully</span><span class="w"> </span><span class="n">opened</span><span class="w"> </span><span class="n">CUDA</span><span class="w"> </span><span class="n">library</span><span class="w"> </span><span class="n">libcuda</span><span class="o">.</span><span class="n">so</span><span class="o">.</span><span class="mi">1</span><span class="w"> </span><span class="n">locally</span>
<span class="n">I</span><span class="w"> </span><span class="n">tensorflow</span><span class="o">/</span><span class="n">stream_executor</span><span class="o">/</span><span class="n">dso_loader</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">105</span><span class="p">]</span><span class="w"> </span><span class="n">successfully</span><span class="w"> </span><span class="n">opened</span><span class="w"> </span><span class="n">CUDA</span><span class="w"> </span><span class="n">library</span><span class="w"> </span><span class="n">libcurand</span><span class="o">.</span><span class="n">so</span><span class="w"> </span><span class="n">locally</span>
</pre></div>

<p>Is this output enough to check if tensorflow is using gpu ?</p>
<p><br><br></p>
<h2>Answer</h2>
<p>No, I don't think "open CUDA library" is enough to tell, because different
nodes of the graph may be on different devices.</p>
<p>When using tensorflow2:</p>
<div class="code"><pre class="code literal-block">print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
</pre></div>

<p>For tensorflow1, to find out which device is used, you can enable log device
placement like this:</p>
<div class="code"><pre class="code literal-block">sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
</pre></div>

<p>Check your console for this type of output.</p>
<p><img alt="" src="../../images/RtRiB.png"></p>
<p><br></p>
<h3>Suggest</h3>
<p>Apart from using <code>sess =
tf.Session(config=tf.ConfigProto(log_device_placement=True))</code> which is
outlined in other answers as well as in the official TensorFlow documentation,
you can try to assign a computation to the gpu and see whether you have an
error.</p>
<div class="code"><pre class="code literal-block"><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">'/gpu:0'</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">],</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">'a'</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">],</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">'b'</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="nb">print</span> <span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>
</pre></div>

<p>Here</p>
<ul>
<li>"/cpu:0": The CPU of your machine.</li>
<li>"/gpu:0": The GPU of your machine, if you have one.</li>
</ul>
<p>If you have a gpu and can use it, you will see the result. Otherwise you will
see an error with a long stacktrace. In the end you will have something like
this:</p>
<blockquote>
<p>Cannot assign a device to node 'MatMul': Could not satisfy explicit device
specification '/device:GPU:0' because no devices matching that specification
are registered in this process</p>
</blockquote>
<hr>
<p>Recently a few helpful functions appeared in TF:</p>
<ul>
<li>tf.test.is_gpu_available tells if the gpu is available</li>
<li>tf.test.gpu_device_name returns the name of the gpu device</li>
</ul>
<p>You can also check for available devices in the session:</p>
<div class="code"><pre class="code literal-block">with tf.Session() as sess:
  devices = sess.list_devices()
</pre></div>

<p><code>devices</code> will return you something like</p>
<div class="code"><pre class="code literal-block">[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 4670268618893924978),
 _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6127825144471676437),
 _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 16148453971365832732),
 _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 10003582050679337480),
 _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 5678397037036584928)
</pre></div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/gpu/" rel="tag">gpu</a></li>
            <li><a class="tag p-category" href="../../categories/python/" rel="tag">python</a></li>
            <li><a class="tag p-category" href="../../categories/tensorflow/" rel="tag">tensorflow</a></li>
            <li><a class="tag p-category" href="../../categories/ubuntu/" rel="tag">ubuntu</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../how-to-use-hex-color-values/" rel="prev" title="How to use hex color values">Previous post</a>
            </li>
            <li class="next">
                <a href="../typescript-input-onchange-event-target-value/" rel="next" title="Typescript input onchange event.target.value">Next post</a>
            </li>
        </ul></nav></aside></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow中文网</a>  
            
        </footer>
</div>
</div>


            <script src="../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="../../assets/js/search.js"></script>
</body>
</html>
