<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Why do neural networks work so well? | StackOverflow Snapshot</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/posts/why-do-neural-networks-work-so-well/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Arya">
<link rel="prev" href="../structured-factored-and-atomic-representation/" title="Structured, factored and atomic representation?" type="text/html">
<link rel="next" href="../how-do-you-derive-the-time-complexity-of-alpha-beta-pruning/" title="How do you derive the time complexity of alpha-beta pruning?" type="text/html">
<meta property="og:site_name" content="StackOverflow Snapshot">
<meta property="og:title" content="Why do neural networks work so well?">
<meta property="og:url" content="https://ohstackoverflow.netlify.app/posts/why-do-neural-networks-work-so-well/">
<meta property="og:description" content="I understand all the computational steps of training a neural network with
gradient descent using forwardprop and backprop, but I'm trying to wrap my
head around why they work so much better than logi">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2023-02-28T03:30:22+08:00">
<meta property="article:tag" content="artificial-intelligence">
<meta property="article:tag" content="machine-learning">
<meta property="article:tag" content="neural-network">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../archive.html">Archive</a>
                </li>
<li>
<a href="../../categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Why do neural networks work so well?</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    <a class="u-url" href="../../authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:30:22+08:00" itemprop="datePublished" title="2023-02-28 03:30">2023-02-28 03:30</time></a>
            </p>
            

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>I understand all the computational steps of training a neural network with
gradient descent using forwardprop and backprop, but I'm trying to wrap my
head around why they work so much better than logistic regression.</p>
<p>For now all I can think of is:</p>
<p>A) the neural network can learn it's own parameters</p>
<p>B) there are many more weights than simple logistic regression thus allowing
for more complex hypotheses</p>
<p>Can someone explain why a neural network works so well in general? I am a
relative beginner.</p>
<p><br><br></p>
<h2>Answer</h2>
<p>Neural Networks can have a large number of free parameters (the weights and
biases between interconnected units) and this gives them the flexibility to
fit highly complex data (when trained correctly) that other models are too
simple to fit. This model complexity brings with it the problems of training
such a complex network and ensuring the resultant model generalises to the
examples it’s trained on (typically neural networks require large volumes of
training data, that other models don't).</p>
<p>Classically logistic regression has been limited to binary classification
using a linear classifier (although multi-class classification can easily be
achieved with one-vs-all, one-vs-one approaches etc. and there are kernalised
variants of logistic regression that allow for non-linear classification
tasks). In general therefore, logistic regression is typically applied to more
simple, linearly-separable classification tasks, where small amounts of
training data are available.</p>
<p>Models such as logistic regression and linear regression can be thought of as
simple multi-layer perceptrons (check out this site for one explanation of
how).</p>
<p>To conclude, it’s the model complexity that allows neural nets to solve more
complex classification tasks, and to have a broader application (particularly
when applied to raw data such as image pixel intensities etc.), but their
complexity means that large volumes of training data are required and training
them can be a difficult task.</p>
<p><br></p>
<h3>Suggest</h3>
<p>Recently Dr. Naftali Tishby's idea of Information Bottleneck to explain the
effectiveness of deep neural networks is making the rounds in the academic
circles. His video explaining the idea (link below) can be rather dense so
I'll try to give the distilled/general form of the core idea to help build
intuition</p>
<p>https://www.youtube.com/watch?v=XL07WEc2TRI</p>
<p>To ground your thinking, vizualize the MNIST task of classifying the digit in
the image. For this, I am only talking about simple fully-connected neural
networks (not Convolutional NN as is typically used for MNIST)</p>
<p>The input to a NN contains information about the output hidden inside of it.
Some function is needed to transform the input to the output form. Pretty
obvious. The key difference in thinking needed to build better intuition is to
think of the input as a signal with "information" in it (I won't go into
information theory here). Some of this information is relevant for the task at
hand (predicting the output). Think of the output as also a signal with a
certain amount of "information". The neural network tries to "successively
refine" and compress the input signal's information to match the desired
output signal. Think of each layer as cutting away at the unneccessary parts
of the input information, and keeping and/or transforming the output
information along the way through the network. The fully-connected neural
network will transform the input information into a form in the final hidden
layer, such that it is linearly separable by the output layer.</p>
<p>This is a very high-level and fundamental interpretation of the NN, and I hope
it will help you see it clearer. If there are parts you'd like me to clarify,
let me know.</p>
<p>There are other essential pieces in Dr.Tishby's work, such as how minibatch
noise helps training, and how the weights of a neural network layer can be
seen as doing a random walk within the constraints of the problem. These parts
are a little more detailed, and I'd recommend first toying with neural
networks and taking a course on Information Theory to help build your
understanding.</p>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/artificial-intelligence/" rel="tag">artificial-intelligence</a></li>
            <li><a class="tag p-category" href="../../categories/machine-learning/" rel="tag">machine-learning</a></li>
            <li><a class="tag p-category" href="../../categories/neural-network/" rel="tag">neural-network</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../structured-factored-and-atomic-representation/" rel="prev" title="Structured, factored and atomic representation?">Previous post</a>
            </li>
            <li class="next">
                <a href="../how-do-you-derive-the-time-complexity-of-alpha-beta-pruning/" rel="next" title="How do you derive the time complexity of alpha-beta pruning?">Next post</a>
            </li>
        </ul></nav></aside></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow-ZH</a>  
            
        </footer>
</div>
</div>


            <script src="../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="../../assets/js/search.js"></script>
</body>
</html>
