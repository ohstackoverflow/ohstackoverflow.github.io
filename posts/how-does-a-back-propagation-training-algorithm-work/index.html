<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>How does a back-propagation training algorithm work? | StackOverflow Snapshot</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="canonical" href="https://ohstackoverflow.netlify.app/posts/how-does-a-back-propagation-training-algorithm-work/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Arya">
<link rel="prev" href="../algorithm-for-matching-noisy-names/" title="Algorithm for matching 'noisy' names" type="text/html">
<link rel="next" href="../what-are-the-uses-of-recurrent-neural-networks-when-using-them-with-reinforcement-learning/" title="What are the uses of recurrent neural networks when using them with Reinforcement Learning?" type="text/html">
<meta property="og:site_name" content="StackOverflow Snapshot">
<meta property="og:title" content="How does a back-propagation training algorithm work?">
<meta property="og:url" content="https://ohstackoverflow.netlify.app/posts/how-does-a-back-propagation-training-algorithm-work/">
<meta property="og:description" content="I've been trying to learn how back-propagation works with neural networks, but
yet to find a good explanation from a less technical aspect.
How does back-propagation work? How does it learn from a tra">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2023-02-28T03:29:02+08:00">
<meta property="article:tag" content="artificial-intelligence">
<meta property="article:tag" content="backpropagation">
<meta property="article:tag" content="computer-science">
<meta property="article:tag" content="neural-network">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ohstackoverflow.netlify.app/">

                <span id="blog-title">StackOverflow Snapshot</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../archive.html">Archive</a>
                </li>
<li>
<a href="../../categories/">Tags</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<div style="display:table;min-height:5rem;min-width:27rem;">
					<div class="input-group" style="display: table-cell;vertical-align: middle;">
						<input id="words" type="text" class="form-control" style="max-width:22rem;" onkeydown="if(event.keyCode==13){btn.click()}"><span class="input-group-btn" style="float:left">
							<button id="btn" class="btn btn-default" type="button" data-toggle="modal" data-target="#myModal">
								<span class="glyphicon glyphicon-search">
							</span></button>
						</span>
					</div>
<!-- /input-group -->
				</div>

				
                
                
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><!-- 模态框（Modal） --><div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			<div class="modal-header">
				<button type="button" class="close" data-dismiss="modal" aria-hidden="true">×
				</button>
				<h4 class="modal-title" id="myModalLabel">
					查找结果
				</h4>
			</div>
			<div class="modal-body">
				<div id="search-count" style="min-height:4rem;">
				查找中，请稍后...
				</div>
				<div id="search-result">
				</div>

				
			</div>
			<div class="modal-footer">
				<button type="button" class="btn btn-default" data-dismiss="modal">
					关闭
				</button>
			</div>
		</div>
<!-- /.modal-content -->
	</div>
<!-- /.modal-dialog -->
</div>
<!-- /.modal -->

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">How does a back-propagation training algorithm work?</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    <a class="u-url" href="../../authors/arya/">Arya</a>
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2023-02-28T03:29:02+08:00" itemprop="datePublished" title="2023-02-28 03:29">2023-02-28 03:29</time></a>
            </p>
            

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>I've been trying to learn how back-propagation works with neural networks, but
yet to find a good explanation from a less technical aspect.</p>
<p>How does back-propagation work? How does it learn from a training dataset
provided? I will have to code this, but until then I need to gain a stronger
understanding of it.</p>
<p><br><br></p>
<h2>Answer</h2>
<p>Back-propagation works in a logic very similar to that of <em>feed-forward</em>. The
difference is the direction of data flow. In the feed-forward step, you have
the inputs and the output observed from it. You can propagate the values
forward to <em>train</em> the neurons ahead.</p>
<p>In the back-propagation step, you cannot know the errors occurred in every
neuron but the ones in the output layer. Calculating the errors of output
nodes is straightforward - you can take the difference between the output from
the neuron and the <em>actual output</em> for that instance in training set. The
neurons in the hidden layers must fix their errors from this. Thus you have to
pass the error values back to them. From these values, the hidden neurons can
update their weights and other parameters using the weighted sum of errors
from the layer ahead.</p>
<p>A step-by-step demo of feed-forward and back-propagation steps can be found
here.</p>
<hr>
<h4>Edit</h4>
<p>If you're a beginner to neural networks, you can begin learning from
<strong>Perceptron</strong> , then advance to NN, which actually is a multilayer
perceptron.</p>
<p><br></p>
<h3>Suggest</h3>
<h4>High-level description of the backpropagation algorithm</h4>
<p>Backpropagation is trying to do a <em>gradient descent</em> on the <em>error surface</em> of
the neural network, adjusting the weights with <em>dynamic programming</em>
techniques to keep the computations tractable.</p>
<p>I will try to explain, in high-level terms, all the just mentioned concepts.</p>
<h4>Error surface</h4>
<p>If you have a neural network with, say, N neurons in the output layer, that
means your output is really an N-dimensional vector, and that vector lives in
an N-dimensional space (or on an N-dimensional surface.) So does the "correct"
output that you're training against. So does the <strong>difference</strong> between your
"correct" answer and the actual output.</p>
<p>That difference, with suitable conditioning (especially some consideration of
absolute values) is the <strong>error vector</strong> , living on the error surface.</p>
<h4>Gradient descent</h4>
<p>With that concept, you can think of training the neural network as the process
of adjusting the weights of your neurons so that the error function is small,
ideally zero. Conceptually, you do this with calculus. If you only had one
output and one weight, this would be simple -- take a few derivatives, which
would tell you which "direction" to move, and make an adjustment in that
direction.</p>
<p>But you don't have one neuron, you have N of them, and substantially more
input weights.</p>
<p>The principle is the same, except instead of using calculus on lines looking
for slopes that you can picture in your head, the equations become vector
algebra expressions that you can't easily picture. The term <strong>gradient</strong> is
the multi-dimensional analogue to <em>slope</em> on a line, and <strong>descent</strong> means you
want to move <em>down</em> that error surface until the errors are small.</p>
<h4>Dynamic programming</h4>
<p>There's another problem, though -- if you have more than one layer, you can't
easily see the change of the weights in some non-output layer vs the actual
output.</p>
<p>Dynamic programming is a bookkeeping method to help track what's going on. At
the very highest level, if you naively try to do all this vector calculus, you
end up calculating some derivatives over and over again. The modern
backpropagation algorithm avoids some of that, and it so happens that you
update the output layer first, then the second to last layer, etc. Updates are
<strong>propagating backwards</strong> from the output, hence the name.</p>
<p>So, if you're lucky enough to have been exposed to gradient descent or vector
calculus before, then hopefully that clicked.</p>
<p>The full derivation of backpropagation can be condensed into about a page of
tight symbolic math, but it's hard to get the sense of the algorithm without a
high-level description. (It's downright intimidating, in my opinion.) If you
haven't got a good handle on vector calculus, then, sorry, the above probably
wasn't helpful. But to get backpropagation to actually work, it's not
necessary to understand the full derivation.</p>
<hr>
<p>I found the following paper (by Rojas) very helpul, when I was trying to
understand this material, even if it's a big PDF of one chapter of his book.</p>
<p>http://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf</p>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/artificial-intelligence/" rel="tag">artificial-intelligence</a></li>
            <li><a class="tag p-category" href="../../categories/backpropagation/" rel="tag">backpropagation</a></li>
            <li><a class="tag p-category" href="../../categories/computer-science/" rel="tag">computer-science</a></li>
            <li><a class="tag p-category" href="../../categories/neural-network/" rel="tag">neural-network</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../algorithm-for-matching-noisy-names/" rel="prev" title="Algorithm for matching 'noisy' names">Previous post</a>
            </li>
            <li class="next">
                <a href="../what-are-the-uses-of-recurrent-neural-networks-when-using-them-with-reinforcement-learning/" rel="next" title="What are the uses of recurrent neural networks when using them with Reinforcement Learning?">Next post</a>
            </li>
        </ul></nav></aside></article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2023         Go to StackOverflow Chinese Site  <a href="http://stackoverflow.ink">StackOverflow-ZH</a>  
            
        </footer>
</div>
</div>


            <script src="../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script src="../../assets/js/search.js"></script>
</body>
</html>
